5287478 4696050 4456514 
5291806
Performance evaluation of cluster-based target tracking protocols for wireless sensor networks###Target tracking is an important application type for wireless sensor networks (WSN). Recently, various approaches are proposed to maintain the accurate tracking of the targets as well as low energy consumption. Clustering is a fundamental technique to manage the scarce network resources. The message complexity of an application can be significantly decreased when it is redesigned on top of a clustered network. Clustering has provided an efficient infrastructure in many existing studies. The clusters can be constructed before the target enters the region which is called the static method or clusters are created by using received signal strength (RSS) from target which is called the dynamic method. In this paper we provide simulations of static and dynamic clustering algorithms against various mobility models and target speeds. The mobility models that we applied are random waypoint model, random direct model and Gauss Markov model. We provide metrics to measure the tracking performance of both approaches. We show that the dynamic clustering is favorable in terms of tracking accuracy whereas the energy consumption of static clustering is significantly smaller. We also show that the target moving with Gauss Markov model can be tracked more accurately than the other models.
5291807
PVC: A novel personal virtual cluster based on multi-core platform###As multi-core processors become increasingly the mainstream, people have likewise become more interested in how best to make use of the computing capacity of the CPU. Although many methods, running multi-thread application for example, have been adopted to increase the CPU utilization, most multi-core PC's and workstation's CPU cycles are idle, even during peak hours. So it is an efficient solution to help a personal user to build his own small non-dedicated cluster by collecting some idle PC in a LAN. In order to improve the utilization of the multi-core processor and shield the heterogeneity of different platform, virtual machine (VM) technology can be applied to partition the resource of each computer, changing a physical node into several homogeneous virtual nodes. This personal virtual cluster (PVC) can be created, managed, and released by a personal user, and run some computationally intensive parallel program such as application with MPI during some temporary time. In this paper we present a prototype of PVC with the popular, open-source, Xen virtualization system, and investigate the performance of the typical parallel programming paradigm MPI in PVC. The results of experiments show that the PVC is a helpful computing mode for a personal user in a LAN, and the application with MPI without much communication between different processes can achieve good performance in PVC.
5291804
Cross layer implementation of key establishment protocols in wireless sensor networks###Security in Wireless Sensor Networks (WSN) can be achieved by establishing shared keys among the sensor nodes having possibility to communicate in the field. However, to achieve the optimum network operation after the deployment, there may be no need to establish keys for all the possible links. Because, links to be established are defined by the network configuration and as long as the network connectivity requirement is satisfied number of links to be secured can be limited accordingly. In this sense, key establishment and network configuration performances are related to each other and this cross relation should be considered when implementing security for WSN. In this paper, we have investigated the cross layer relations and performance variations of the selected pre-distribution and public key based key establishment protocols with the network configuration protocol proposed in. We have proposed three methods as straight, reactive and proactive for selection of links to be secured and simulated the performance in terms of network connectivity, total energy cost and resilience to node capture attack. Simulation results indicate that total energy cost can be reduced by reducing the number of links to be secured without affecting the connectivity performance of the configuration protocol. Also we have figured out that the energy and resilience performance of the public key establishment is better than the key pre-distribution for the given network configuration parameters.
5291805
Channel import efficiency of the DonorList algorithm###Rapid advances of the handheld devices and the emergence of the demanding wireless applications require the cellular networks to support the demanding user needs more effectively. The cellular networks are expected to provide these services under a limited bandwidth. Efficient management of the wireless channels by effective channel allocation algorithms is crucial for the performance of any cellular system. To provide a better channel usage performance, dynamic channel allocation schemes have been proposed. Among these schemes, distributed dynamic channel allocation approaches usually have better performance results. The two important issues that must be carefully addressed in such algorithms are the efficient co-channel interference avoidance and messaging overhead reduction. One of these distributed channel allocation algorithms is the DonorList algorithm, which can effectively manage these two issues. In this paper the channel import efficiency of the DonorList algorithm is evaluated. The performance evaluation results obtained after extensive simulation studies, which were conducted under different traffic load conditions, show that the proposed algorithm provides improvement over the performance of other algorithms recently proposed in the literature.
5291802
Ant colony optimization for survivable virtual topology mapping in optical WDM networks###The high capacity of fibers used in optical networks, can be divided into many channels, using the WDM technology. Any damage to a fiber causes all the channels routed through this link to be broken, which may result in a serious amount of data loss. As a solution to this problem, the virtual layer can be mapped onto the physical topology, such that, a failure on any physical link does not disconnect the virtual topology. This is known as the survivable virtual topology mapping problem. In this study, our aim is to determine a suitable ant colony optimization algorithm to solve this problem. Our results show that ant colony heuristics perform remarkably well while producing high quality solutions in less than half a minute.
5291803
Predicting future object states using learned affordances###The notion of <i>affordances</i> that was proposed by J.J. Gibson, refers to the action possibilities offered to the organism by its environment. In a previous formalization, affordances are defined as general relations that pertain to the robot-environment interaction and they are represented as triples which consist of the initial percept of the environment, the behavior applied and the effect produced. In this paper, we focus on the object affordances and propose a developmental method that enables the robot to ground symbolic object-based operators in its own continuous sensory-motor experiences. The method allows the robot to learn the object affordance relations which can be used to predict the change in the percept of the object when a certain behavior is executed.
5291800
Ensemble detection: A new architecture for multisensor data fusion with ensemble learning for object detection###In this work, we propose a framework for multimodal data fusion at decision level under a multilayer hierarchical ensemble learning architecture. The architecture provides a generative discriminative model for probability density estimations and decreases the entropy of the data throughout the vector spaces. The architecture is implemented for human motion detection problem, where the motion analysis problem is formulated as a multi-class classification problem on audio-visual data. The vector space transformations are analyzed by the investigation of probability density and entropy transitions of data across the levels. The architecture provides an efficient sensor fusion framework for the robotics research, object classification, target detection and tracking applications.
5291801
Improving balance regulation in visuo-motor control for humanoid robots###We know various strategies toward teaching and controlling humanoid robots. Some refer to direct joint or tip control and others use more intuitive approach such as mimicking human motion in a certain task. This kind of robot control, where a robot is considered a tool, controlled by a human demonstrator, is called visuo-motion control. In this paper, we present an improved approach to overcome a problem of balance in visuo-motor control of humanoid robots.
5291808
SCALAR data replication performance in mobile ad hoc applications###Data replication is an indispensable service for mobile ad hoc networks and it helps to increase data availability, reduce communication overhead, achieve fault-tolerance and load balancing. Our scalable and reactive data replication framework SCALAR operates in a peer-to-peer manner where the network nodes construct a connected dominating set based on the topology graph, and use that virtual backbone for efficient data lookup and replication. In this study, we compare the performance of SCALAR with the well-known SAF and DAFN replication approaches on realistic application scenarios simulating university campus and shopping mall settings. We examine data accessibility, protocol overhead, effects of data request probability and percentage of popular data items in the system through extensive runs conducted on SWANS simulator. SCALAR is shown to outperform the other approaches in terms of data accessibility and overhead, and demonstrated as an efficient and scalable replication solution for mobile ad hoc applications.
5291809
A resilient key predistribution scheme for multiphase wireless sensor networks###In wireless sensor networks, sensor nodes eventually die due to battery depletion. Wireless sensor networks (WSNs) in which new nodes are periodically redeployed with certain intervals, called generations, to replace the dead nodes are called multi-phase wireless sensor networks. In the literature, there are several key predistribution schemes proposed for secure operation of WSNs. However, these schemes are designed for single phase networks which are not resilient against continuous node capture attacks; even under temporary attacks on the network, the harm caused by the attacker does not heal in time. However, the periodic deployments in multi-phase sensor networks could be utilized to improve the resiliency of the WSNs by deploying nodes with fresh keys. In the literature, there is limited work done in this area. In this paper, we propose a key predistribution scheme for multi-phase wireless sensor networks which is highly resilient under node capture attacks. In our scheme, called RGM (random generation material) key predistribution scheme, each generation of deployment has its own random keying material and pairwise keys are established between node pairs of particular generations. These keys are specific to these generations. Therefore, a captured node cannot be abused to obtain keys of other generations. We compare the performance of our RGM scheme with a well-known multi-phase key predistribution scheme and showed that RGM achieves up to three-fold more resiliency. Even under heavy attacks, our scheme's resiliency performance is 50% better in steady state.
5291888
A novel approach for off-line Arabic writer identification based on stroke feature combination###This paper presents a novel approach for off-line text-independent Arabic writer identification. The approach operates in four steps: 1) handwritten text is segmented into strokes after an image thinning step; 2)length, height/width ratio and curvature stroke features are extracted; 3) five feature vectors are computed: stroke length/ratio probability distribution function (PDF), stroke length/ratio horizontal and vertical cross-correlation, stroke length/curvature PDF, stroke length/curvature horizontal and vertical cross-correlation, and stroke length/curvature and length/ratio cross-correlation; 4) classification is carried out using different metrics and the Borda count ranking algorithm. A first experimental evaluation performed on 40 writers from the IFN/ENIT database produced a promising identification rate of 92.5% for Top1 and 100% for Top5.
5291889
Stereo line matching based on the combination of geometric and intensity data###This paper presents a line matching method for the reconstruction of 3D line segment, based on geometric and intensity information. Line matching is a difficult task in the area of stereo vision, since it relies on incomplete line data. A set of line attributes is used to match a pair of 2D lines. Line attributes consist of intensity information and geometric information including length, orientation, and endpoint position. For the implementation of the proposed matching method, we employ multi-threshold technique, which provides flexible and efficient matching capability. The experimental results show that the rate of the correct match comes up with 96.12%, which is very hard to be achieved in the complex scene used in the experiment.
5291886
Polynomial approach in a secret image sharing using quadratic residue###Secret image sharing is a technique to share the secret among n participants. Each participant has meaningless, noise like shares. The secret image is revealed if any k of the shares are gathered. This scheme uses polynomial based (k, n) secret sharing approach proposed by Shamir in 1979. In 2007, Chen et al. proposed a new secret image sharing method that uses quadratic residues. Their scheme restricts the value of k, only (2, 2) scheme is realized with their algorithm. This prevents use of their method for any values of (k, n). In this work, we applied Shamir's polynomial approach with quadratic residues to generalize Chen's method. Proposed method is valid for any values of (k, n) as can be seen in the experimental results. Two methods (both lossy and lossless) are implemented, and their results are compared with respect to shares' size. As a result, lossless method is preferable for k values larger than two. Otherwise, lossy method is recommended.
5291887
Active authentication by mouse movements###We present an application for authenticating users with mouse movements. We extract the features from nine paths between seven squares displayed consecutively on the screen. Although the paths among the squares are fixed, user can not get familiar to the paths by the selection algorithm of next path. This prevents similar results from different users. We experimented on improving the success rate, by selecting the optimal time interval between mouse movement points and refining features to obtain valid data. Experiment's results presented a false acceptance rate (FAR) of 5.9% and a false reject rate (FRR) of 5.9%.
5291884
Movies genres classifier using neural network###In this paper we have designed a neural network based movie genres classifier. The Movie classifier characterizes the movie clips into different movie genres. The characterization is based on low level audio-visual features. We have extracted the computable audio-visual features from the movie clips which are inspired by the techniques and film grammars used by many filmmakers to endow specific characteristics to a genre. The extracted visual features are shot length, motion, color dominance and lighting key and the extracted audio features are based on time domain, pitch, frequency domain, energy and MFCC. Movie classifier is designed using feed forward neural network with back propagation learning algorithm. We have demonstrated the effectiveness of the classifier for characterizing the movie clips into action, horror, comedy, music and drama genres.
5291885
Consistency checking for automatic software generation###This paper presents a new formal approach to checking consistency of a set of axioms that define a function. Our approach is an alternative to the original knuth-bendix (KB) completion procedure which can only process equational axioms. Additional steps are required to be implemented in KB in order to use it with conditional equations which are heavily used as function specifications. Since the size of the code to be trusted is important in systems like Code-carrying theory and Proof-carrying code, it is also important to have a short implementation. We show that our alternative idea is simpler and easy to implement with a small amount of code. In addition, our approach does not require one to define a well-founded ordering relation as in knuth-bendix procedure making it more practical to use it with less effort.
5291882
An outlier detection algorithm based on object-oriented metrics thresholds###Detection of outliers in software measurement datasets is a critical issue that affects the performance of software fault prediction models built based on these datasets. Two necessary components of fault prediction models, software metrics and fault data, are collected from the software projects developed with object-oriented programming paradigm. We proposed an outlier detection algorithm based on these kinds of metrics thresholds. We used Random Forests machine learning classifier on two software measurement datasets collected from jEdit open-source text editor project and experiments revealed that our outlier detection approach improves the performance of fault predictors based on Random Forests classifier.
5291883
Using a SAT solver to generate checking sequences###Methods for software testing based on finite state machines (FSMs) have been researched since the early 60's. Many of these methods are about generating a checking sequence from a given FSM which is an input sequence that determines whether an implementation of the FSM is faulty or correct. In this paper, we consider one of these methods, which constructs a checking sequence by reducing the problem of generating a checking sequence to finding a Chinese rural postman tour on a graph induced by the FSM; we re-formulate the constraints used in this method as a set of Boolean formulas; and use a SAT solver to generate a checking sequence of minimal length.
5291880
Testing requirements for mobile applications###Wireless networks have contributed to the technological advance that popularized the use of mobile devices, and fostered the development of applications targeted to these devices. However, issues such as mobility and communication intermittency, as well as processing, storage and battery constraints demand changes in the traditional software testing process. This paper aims to present testing requirements that are specific to applications developed for mobile devices. We also show that these requirements can improve productivity and efficacy in the testing process of an application.
5291881
A web-based data source for metabolomics###With the development of improved and cost-effective technologies, it is now possible to detect thousands of metabolites in biofluids or specific organs, and reliably quantify their amounts. Metabolomics focuses on studying the concentrations of metabolites in a cell or a tissue. In this paper, we describe a prototype Web-based metabolomics data analysis system, PathCase<sup>MAW</sup> (pathcase metabolomics analysis workbench), which features (1) A Web-accessible metabolic pathway database that supports online browsing and querying, and is novel in that it includes tissue and subcompartment information for pathways, and models transport processes, (2) Tissue-aware visualization support for viewing processes, pathways, or groups of pathways in PathCase<sup>MAW</sup> database, and (3) An online metabolomics data analysis tool, called automated consequence prediction tool, which allows users to upload their own observed/measured metabolite level changes, and computationally invalidates or M(aybe)-validates those biological mechanisms that produce the observed metabolite changes.
5291873
Domain specific phase by phase effort estimation in software projects###Software cost and effort estimation models mostly focus on predicting the overall project cost rather than the cost of phases separately. In addition to that, no previous research has a domain specific view on phase by phase effort estimation. In this research, domain specific data analysis is conducted to discover the difference in phase distribution profiles in different application domains. Furthermore, a regression model is built in order to see whether a domain specific approach provides improvement in effort estimation accuracy. Our experimental results showed when domain specific data and phase by phase approach is used the prediction accuracy increases up to 74%.
5291872
Integrating business process and user interface models using a model-driven approach###Business services are complex entities that encompass descriptions about different aspects including business processes and user interfaces. Typically, the modeling of business services results on several correlated models. On the one hand, there is the need to keep these models apart in order to attain the levels of abstractions needed to model different aspects of a service. On the other hand, it is important to maintain the consistency and integrity of models. In this paper, we show how to maintain the consistency and integrity of models and present a use case for the integration of user interface design and business process models.
5291871
Error density metrics for business process model###In this paper, metrics for business process model (BPM), are proposed, which are capable to measure the usability and effectiveness of BPMs. The proposed model is adapting error density metrics to BPMs by considering the similarities between the conceptual characteristics of BPMs and software products. We applied seven software metrics for evaluating quality of business processes/ process models. Results show that our metrics help the organization to improve their process, as weighted measurements are indicators for unexpected situations/behaviour for business processes.
5291870
Making functional similarity count for more reliable effort prediction models###This paper evaluates the impact of functional similarities on the relation between functional size and software development effort. The estimation of the required effort cannot be achieved within reasonable limits at all times. We aimed to address this problem by considering functional similarities. Functional similarity is calculated for COSMIC measurement size results as adjusted functional sizes. The case study includes six cases. We discussed the challenges and the improvement opportunities of the method.
5291877
HMM-based sliding video text recognition for Turkish broadcast news###In this paper, we develop an HMM-based sliding video text recognizer and present our results on Turkish broadcast news for the hearing impaired. We use well known speech recognition techniques to model and recognize sliding video text characters using a minimal amount of labeled data. Baseline system without any language modeling gives a word error rate of 2.2% on 138 minutes of test data. We then provide an analysis of character errors and employ a character-based language model to correct most of them. Finally we decrease the amount of training data to a quarter, split the test data into halves and investigate semi-supervised training. Word error rates after semi-supervised training are significantly lower than to those after baseline training. We see 40% relative reduction in word error rate (1.5 rarr 0.9) over the test set.
5291876
A novel noise robust and low bit rate speech coding algorithm###In this work, a new noise robust and variable length frame based speech modeling method is introduced. This method consists of three major steps which includes noise removal algorithm, coding and encoding algorithms, respectively. Coding and encoding parts are developed based on SYMPES (systematic procedure for predefined envelope and signature sequence sets). These sets have been developed in two types which represent voiced and unvoiced parts of the speech signals separately in order to obtain more efficient coding strategy and higher compression ratio while preserving the perceptual quality of the speech signals. As an extension of our previous works our new framework is not only consider the coding of the clean speech signals but also noisy speech signals. The new noise robust module suppresses the noise and delivers the clean speech signal to the newly designed modeling part. The modeling part promises higher compression ratios by switching to the more appropriate type of predefined sets take into account the voiced and unvoiced frames.
5291875
Towards an interdisciplinary methodology for service-oriented system engineering###Service concept is evolving as a broad idea recurrently discussed in recent years as it originates from many disciplines such as marketing, operations and computer science. Despite this interdisciplinary nature of service concept, existing methodologies for building service-oriented systems provide limited support to include a complete coverage of various engineering phases for a unified development and are therefore not directly applicable to service-oriented architectures. In particular, the design of coordination mechanisms between service providers and customers as well as the design of a common vocabulary in an inter-organizational setting is not adequately addressed. To face these shortcomings, in this position paper, we present an interdisciplinary approach integrating recently proposed methodologies into one coherent engineering methodology. The methodology aims to build a system that extends the basic find-bind-execute paradigm of Web services on an electronic market platform.
5291874
On educating globally distributed software development &#x2014; A case study###Globally distributed software development is widely used in industry, for example, to stay competitive in the market. To achieve competitiveness one needs cost reduction, delivery of best quality, use of latest high-tech skills, as well as reliability and creativity. The key criteria for all these factors is well educated people. Yet, software engineering education hardly focuses the needs of distributed software development. Consequently, the graduated students rarely posses necessary skills and hands-on experience required for such projects; e.g. communication skills, project and process management as well as knowledge management. This paper reports on a collaborative education project between the University of Karlsruhe, Germany, Research Centre for Information Technology (FZI) in Karlsruhe, Germany, and Polytechnic State University of St. Petersburg, Russia. The main goal of that project was to convey necessary skills to our students. The second goal was to investigate how different models of communication impact the project results.
5291879
Investigating the effect of data partitioning for GMM supervector based speaker verification###GMM supervectors are among the most popular feature sets used in SVM-based text-independent speaker verification systems. Most of the studies use only a single supervector to represent speaker characteristics, against a set of background samples. An alternative would be to divide the total training duration into smaller pieces to increase the number of supervectors for training the minority (speaker) class. Similarly, total test duration could also be partitioned, letting the final verification be made by majority voting over decisions on smaller durations. We explore the performance of speaker verification systems in terms of EER and minDCF by breaking down the input sequence into durations of 4 minutes, 1 minute and 10 seconds. We try different training/test data amounts to investigate the generalizability of this approach. Working on the CSLU speaker recognition dataset, we show that the lowest error rates are obtained when the training supervector representative duration is set equal to that of the test samples.
5291878
Semantic argument frequency-based multi-document summarization###Semantic role labeling (SRL) aims to identify the constituents of a sentence, together with their roles with respect to the sentence predicates. In this paper, we introduce and assess the idea of using SRL on generic multi-document summarization (MDS). We score sentences according to their inclusion of frequent semantic phrases and form the summary using the top-scored sentences. We compare this method with a term-based sentence scoring approach to investigate the effects of using semantic units instead of single words for sentence scoring. We also integrate our scoring metric as an auxiliary feature to a cutting edge summarizer with the intention of examining its effects on the performance. The experiments using datasets from the Document Understanding Conference (DUC) 2004 show that the SRL-based summarization outperforms the term-based approach as well as most of the DUC participants.
5291860
Ensembled gabor nearest neighbor classifier for face recognition###In last decades, Gabor feature based face representation presented promising results in face recognition applications due to its robustness against illumination and facial expression changes. The power of Gabor lays in its properties like the computation of local structure corresponding to different spatial frequency (scale), spatial localization, orientations and inessentiality of manual annotations. This work, an ensemble based Gabor nearest neighbor classifier (EGNNC), extends the Gabor nearest neighbor classifier (GNNC), which extracts important discriminative features utilizing both the Gabor filter and nearest neighbor discriminant analysis (NNDA). EGNNC is an ensemble classifier combining multiple NNDA based component classifiers which are designed using different segments of the reduced Gabor features. EGNNC has better use of the discriminability implied in reduced Gabor features by avoiding 3S (small sample size) problem in contrast to GNNC, since GNNC operates on the whole Gabor feature space and reduces the dimension by one component NNDA. In experimental analysis EGGNC outperformed its ancestor GNNC in terms of recognition rate and accuracy for the two test datasets which makes the proposed ensemble approach applicable in face recognition.
5291861
Text classification in the Turkish marketing domain for context sensitive ad distribution###In this paper, we construct and compare several feature extraction approaches in order to find a better solution for classification of Turkish Web documents in the marketing domain. We produce our feature extraction techniques using characteristics of the Turkish language, structures of Web documents and online content in the marketing domain. We form datasets in different feature spaces and we apply several support vector machine (SVM) configurations on these datasets. We conduct our study considering the performance needs of practical context sensitive systems. Our results show that linear kernel classifiers achieve the best performance in terms of accuracy and speed on text documents expressed as keyword root features.
5291862
A microformat based approach for crawling and locating services in the eGovernment domain###As a result of the soaring amount of solutions in the eGovernment domain, there is an increasing number of services available for citizens. This phenomena can introduce some level of confusion for citizens that may encounter difficulties to locate the desired service. Thus, mechanisms to search and locate a particular service must be provided. In this task, the use of semantic technologies is expected to play a paramount role. This papers aims at presenting a two-fold mechanism to fight back this issue. The proposed solution will be able to make semantic annotations on services and to crawl eGovernment Web pages in search for services. Finally, these services will be put at the disposal of citizen by means of a semantic search engine. All details regarding the semantic support and the technical details are fully discussed in the paper that also lays some conclusions.
5291863
Track me! a web based location tracking and analysis system for smart phone users###Mobility information of cell phone users is very important for wide range of applications, including context-based search and advertising, early warning systems, city-wide sensing applications such as air pollution exposure estimation and traffic planning. With the inclusion of new technologies in the cell phone hardware such as built-in GPS and 802.11 supports, mobility information are easily captured, managed and forwarded to a remote system via opportunistic connections over Internet. However, it is very difficult to use these low level location data for practical applications due to lack of sufficient information including high level location and temporal data. In order to solve this problem, we propose a Web based mobility analysis system which collects location data from cell phone users via opportunistic Internet connections and convert these low level location data to high level mobility information as well as adding a temporal dimension. In our experiments, we have illustrated the benefits of our systems on the reality mining data set which contains 350 K hours of real cell tower connection data.
5291864
An approach to state space reduction for systems with dynamic process creation###Automated verification of dynamic multi-threaded computing systems can be adversely affected by problems relating to dynamic process creation. We therefore investigate - in a general setting of labelled transition systems - a way of reducing the state spaces of multi-threaded systems. At the heart of our method is a state equivalence, which may produce a finite representation of an infinite state system while still allowing to validate the relevant behavioural properties. We demonstrate the feasibility of the method through experiments involving the checking of the proposed state equivalence.
5291865
A taxonomy based semantic similarity of documents using the cosine measure###In this paper, we present a new method for calculating semantic similarities between documents. This method is based on cosine similarity calculation between concept vectors of documents obtained from a taxonomy of words that captures IS-A relations. The calculation of semantic similarities between documents is a very time consuming task, since it is necessary first to calculate semantic similarities between each pair of words that appear on different documents. In this paper, we present a new method to calculate semantic similarities between documents which results in faster computational time. Both a taxonomy based semantic similarity and cosine similarity are employed. First, the concept vectors of documents are obtained by extending the terms in the document vectors with their corresponding IS-A concepts. Cosine similarity is then calculated between those concept vectors of documents. Thus, the overall similarity between documents is a combination of cosine similarity and semantic similarity. The proposed semantic similarity is tested in document clustering problem. The experimental results show that our method achieves a good performance.
5291866
The effect of granularity level on software defect prediction###Application of defect predictors in software development helps the managers to allocate their resources such as time and effort more efficiently and cost effectively to test certain sections of the code. In this research, we have used naive Bayes classifier (NBC) to construct our defect prediction framework. Our proposed framework uses the hierarchical structure information about the source code of the software product, to perform defect prediction at a functional method level and source file level. We have applied our model on SoftLAB and Eclipse datasets. We have measured the performance of our proposed model and applied cost benefit analysis. Our results reveal that source file level defect prediction improves the verification effort, while decreasing the defect prediction performance in all datasets.
5291867
Performance evaluation of disk scheduling algorithms in the presence of bad sectors###Disk scheduling algorithms have long been a topic of study in computer science. Many researchers studied the performance of the disk scheduling algorithms. However, perhaps due to the difficulty of implementation, those early works focused solely on exploring the basic ideas and comparing the performance of these algorithms. No one studied the effect of bad sectors in their performance. In this paper, the performance of the disk scheduling algorithms in the presence of bad sectors is studied. The mapping of bad sectors to spare sectors is considered. We use simulation to do the performance evaluation. Simulation results show that all disk scheduling algorithms are same when there is a high percentage of bad sectors and when the spare sectors are distributed at the end of the disk. It is shown also that with a high percentage of bad sectors, the disk scheduling algorithms perform better if the distribution of spare sectors is at the end of the disk. With a low percentage of bad sectors, the scheduling algorithms perform better if the distribution of spare sectors is within the entire disk. It is shown also that all scheduling algorithms are more sensitive to bad sectors in heavily loaded systems.
5291868
A cognitive requirement specification model###Eliciting/gathering information from the customers in requirement phase is the most crucial task in the development of the software development process, because this phase builds the base for the success or failure of any software product. Requirements specification process highly depends on the knowledge and mental abilities of the customers. In this paper, we are proposing a cognitive requirement specification model based on the cognitive classification of customers.
5291869
Negotiation based advance reservation priority grid scheduler with a penal clause for execution failures###The utility computing in a grid demands much more adaptability and dynamism when certain levels of commitments are to be complied with. Users with distinct priorities are categorized on the basis of the types of organizations or applications they belong to and can submit multiple jobs with varying specific needs. The interests of consumers and the resource service providers must be equally watched upon, with focus on commercials too. At the same time, the quality of the service must be ascertained to a committed level, thus enforcing an agreement. The authors propose an algorithm for a negotiation based scheduler that dynamically analyses and assesses the incoming jobs in terms of priorities and requirements, reserves them to resources after negotiations and match-making between the resource providers and the users. The jobs thus reserved are allocated resources for future. The performance evaluation of the scheduler for various parameters was done through simulations. The results were found to be optimal after incorporating advance reservation with dynamic priority control over job selection, involving the impact of situations confronting resource failures introducing economic policies and penalties.
5291909
A token based control channel protocol for cognitive radio networks###In cognitive radio networks, the main purpose of medium access layer is to make optimum decisions for efficient usage of spectrum which is scarcely used by primary users. In addition to primary users (PUs), secondary users (SUs) have to share the spectrum with each other by exchanging information over a control channel. Therefore, control channel has vital importance on MAC layer. When there is a significant number of SUs in the network, this control channel becomes saturated because of the increasing number of channel requests and control information. In this paper, we propose a token ring based solution for the control channel to limit the response delay; thus prevent excess waiting time which happens with carrier sensing methods even though there are available licensed channels. The protocol proposed also improves the quality of the spectrum decision process, because the necessary global information about the state of channels is carried inside the token. The protocol was compared with a CSMA/CA protocol and gained better results in response delay and utilization.
5291908
A dynamic spectrum decision scheme for heterogeneous cognitive radio networks###Spectrum sensing and spectrum management are the main challenging functions that cognitive radio (CR) networks have to perform. In this paper, we focus specifically on the spectrum decision problem. This problem is worsened in the presence of users with different demands and spectrum channels with different properties in a heterogeneous network. For accurate and proper spectrum management, we propose a spectrum decision algorithm for spectrum brokers, which takes user type and spectrum channel properties into consideration. This approach increases both the number of users that can get proper spectrum bands and the throughput of the system. The algorithm also includes a "patience" option. Users that choose the "patience" option agree to wait for a predetermined amount of time until their connection is established. Meanwhile, a better spectrum may become available. This option increases the efficiency of spectrum usage too.
5291907
Joint routing and multi level data compression for lifetime optimization in wireless sensor networks###In this paper we present a framework to investigate the effects of tunable data compression on sensor network lifetime. In our framework we integrate tunable data compression with flow balancing to extend the overall system lifetime beyond the lifetime achievable by flow balancing and single level mandatory data compression. Analysis of energy balancing through flow balancing in conjunction with tunable compression, mandatory compression, and no compression strategies reveal that the lifetime achievable with energy balancing with optimal tunable data compression results in significantly longer network lifetimes than the other strategies.
5291906
The romberg-like parallel numerical integration on a cluster system###In this paper, an approximation of a double integral over a triangular region using a Romberg-like buffered extrapolation technique is considered. A master-worker algorithm organization was developed and implemented using the message passing interface (MPI) paradigm which aims at computing the function f(x,y) at the nodes of the triangle by the worker processes in parallel. The algorithm is implemented on a Sun Cluster consisting of 8xX2200 Sun Fire dual core processors using Sun's OpenMPI message passing environment. Here, the Sun Studio Performance Analyzer is utilized to display the runtime behavior of the processes and CPU's involved in the computations. A simple model is developed to predict the speed-up factor.
5291905
Construction of examination timetables based on ordering heuristics###In this paper, we combine graph coloring heuristics, namely largest degree and saturation degree with the concept of a heuristic modifier under the framework of squeaky wheel optimization for solving a set of examination timetabling problems. Both components interact adaptively to determine the best ordering of examinations to be processed at each iteration. A variety of approaches using different combinations of graph coloring heuristics and heuristic modifiers are investigated on a benchmark of examination timetabling problems. Experimental results show that our approaches can produce high quality solutions that are comparable to the ones obtained from previously proposed constructive methods. We conclude that our approach is simple, yet effective.
5291904
Comparison of balancing techniques for multimedia IR over imbalanced datasets###A promising method to improve the performance of information retrieval systems is to approach retrieval tasks as a supervised classification problem. Previous user interactions, e.g. gathered from a thorough log file analysis, can be used to train classifiers which aim to inference relevance of retrieved documents based on user interactions. A problem in this approach is, however, the large imbalance ratio between relevant and non-relevant documents in the collection. In standard test collection as used in academic evaluation frameworks such as TREC, non-relevant documents outnumber relevant documents by far. In this work, we address this imbalance problem in the multimedia domain. We focus on the logs of two multimedia user studies which are highly imbalanced. We compare a naiinodotve solution of randomly deleting documents belonging to the majority class with various balancing algorithms coming from different fields: data classification and text classification. Our experiments indicate that all algorithms improve the classification performance of just deleting at random from the dominant class.
5291903
Frequent itemsets hiding: A performance evaluation framework###Sensitive knowledge hiding is an essential requirement to prevent disclosure of any sensitive knowledge holding in shared databases. The security of a database may be risked when it is made public as is: because the data mining tools are so sophisticated that the sensitive knowledge can easily be surfaced by receivers. This gives rise to a sanitization process which transforms the original database into another database, the released one, which does not hold the sensitive knowledge but can substitute the original otherwise. In case the sensitive knowledge is of the form frequent itemsets, the resulting concrete problem is called frequent itemsets hiding. A number of algorithms, exploiting different approaches and techniques, for frequent itemsets hiding problem is proposed in the literature. Since finding optimal solutions is NP-Hard, algorithms resort to certain heuristics having different levels of sophistication, complexity, efficiency and effectiveness. This paper presents an evaluation framework which implements recent algorithms belonging to different approaches and a set of metrics to gauge the performance and problem difficulties. The current work also presents an experimental study and its results where four algorithms and seven datasets are involved. Our results indicate that data distortion levels and runtime requirements are quite high, especially for difficult problem instances. Our conclusion is that there are new rooms for more sophisticated and tuneable (w.r.t. effectiveness/efficiency tradeoff) algorithms.
5291902
Stylistic document retrieval for Turkish###In information retrieval (IR) systems, there are a query and a collection of documents compared with this query and ranked according to a particular similarity measure. Since texts with the same content can be written by different authors, the writing styles of the documents change as well accordingly. This observation brings the idea of investigating text by means of style. In this paper, we analyze text documents in terms of stylistic features of the written text and measure effectiveness of these features in an IR system. Our main focus is on Turkish text documents. Although there are many studies about broadening IR systems with style based enhancement, there is no similar application for Turkish which performs retrieval depending purely on style.
5291901
Accurately clustering moving objects with adaptive history filtering###This paper addresses the problem of detecting and tracking clusters of moving objects in spatio-temporal data sets. Spatio-temporal data sets contain data objects that move in space over time. Traditional data clustering algorithms work well on static data sets that contain well separated clusters. Traditional techniques breakdown when they are applied to spatio-temporal data sets. They are not capable of tracking clusters when the moving objects intersect the space occupied by objects from another cluster. This work aims to improve the accuracy of traditional data clustering algorithms on spatio-temporal data sets. Many clustering algorithms create clusters based on the distance between the objects. We extend this distance measure to be a function of the position history of the objects. We show through a series of experiments that the use of the history based distance measures greatly improves the accuracy of existing data clustering algorithms on spatio-temporal data sets. In random data sets we achieve up to a 90% improvement in cluster accuracy. To evaluate the clustering algorithms we created 100 spatio-temporal data sets. We also defined a set of metrics that are used to evaluate the performance of the clustering algorithms on the spatio-temporal data sets.
5291900
Bootstrap Bayesian analysis with applications to gene-environment interaction###We propose a novel statistical model and inferential algorithm for gene environment interaction. Our methodology was motivated by and applied to identity by descent (IBD) sharing for sibling pairs affected by schizophrenia. Our analysis confirms some of the previous findings on the same data set, e.g. the estimated location of the disease gene and the existence of the interaction between the location of disease gene and environment. Our analysis also provides new insights by better accounting for overall variability in the data. We show that taking into account sampling variability may increase the length of posterior credible intervals for the true location of the disease gene by as much as 140%. Moreover, the posterior distribution is shown to be non-Gaussian, which more closely matches the data.
5291859
Improved face recognition system using probability distribution functions extracted from wavelet subbands###In this paper, we propose a high performance face recognition system based on the probability distribution functions (PDF) extracted from discrete wavelet transform in different colour channels. The PDFs of the equalized and segmented faces in different subband images obtained from discrete wavelet transform (DWT) are used as statistical feature vectors for the recognition of faces by minimizing the Kull-back- Leibler divergence (KLD) between the PDF of a given face and the PDFs of faces in the database. Majority voting (MV) and feature vector fusion (FVF) methods have been employed to improve the recognition performance by combining feature vectors in HSI and YCbCr colour spaces of LL, LH, HL, and HH subband images. The system has been tested on the FERET and the head pose (HP) face databases. The results have been compared with conventional PCA, and three state-of-art face recognition techniques, namely, adaptive local binary pattern (LBP) PDF based face recognition, nonnegative matrix factorization (NMF), and supervised incremental NMF (INMF).
5291858
Utilization of dimensionality reduction in stacked generalization architecture###Stacked generalization (SG) is a hierarchical architecture, which combines classifiers in order to boost the performance of overall system by integrating the individual classifiers. Two-layered version of SG has been utilized in many image analysis researches and shown to be an effective tool for enhancing the performance measure of the individual classifiers in a number of various cases. However, due to its layered architecture and integration approach, it inherits some problems such as curse of dimensionality, i.e. necessity for more samples, and higher computational time for classification. In this work, the effect of two different dimensionality reduction methods, namely linear and non-linear, between layers of SG are analyzed to attack the curse of dimensionality problem. In the experimental part, the comparisons of these approaches with respect to each other and SG-without-dimensionality-reduction are presented. According to test results, it is concluded that dimensionality reduction has a positive effect on the performance of SG, i.e. both linear and nonlinear dimensionality reduction approaches performs better than SG architecture, and nonlinear dimensionality reduction achieves better classification accuracy than linear dimensionality reduction.
5291855
A buffer zone computation algorithm for corridor rendering in GIS###This work defines a corridor rendering algorithm with variable leg buffer distances and the algorithm also supports geographic world model. A corridor is defined by a path and two distances for each leg to make a buffered zone around the path. Rendering of a corridor is a challenging task in GIS applications. Corridor is extensively used on mission computer displays on command and control platforms and civilian air control centers. Line buffering and offset curve approximations are the special case for the corridor in which there is a constant buffer distance for each leg whereas a corridor may have different leg distances. There are various works about line buffering and offset curve approximations but no algorithm is found in literature for corridor problem.
5291854
Distance computation using OBB-trees###Distance computation between geometric models plays an important role in a variety of areas such as robotics and computer graphics. In this paper we present a hierarchical method for computing the distance between two triangular mesh objects. Our method is based on direct utilization of OBB-trees. The only alternative method which uses OBB-trees for distance calculation is the swept-sphere-trees (SST) method where OBBs are indirectly used for construction. Our method employs an alignment scheme proposed for OBB-OBB intersection tests in the distance calculation context. This alignment allows us to reduce the OBB-OBB distance problem to computing the distance between two AABBs. In this paper we also propose an evaluation method, where we measure the parameters affecting the computation time as a function of the distance between object pairs. This allows an effective comparison of the bounding volumes under consideration. Using this scheme we compare the performance of the proposed method with the SST-method which is widely used for distance computation.
5291857
Plant image retrieval using color and texture features###An application of content-based image retrieval is proposed for identifying plants, along with a preliminary implementation. The system takes a plant image as input and finds the matching plant from a plant image database and is intended to provide users a simple method to locate information about their house plants. Max-flow min-cut technique is used as the image segmentation method to extract the general structure of the plant. Various color and texture features extracted from the segmented plant region are used in matching images to the database. Results show that for 60% of the queries, the correct plant image is retrieved among the top-10 results, using a small database of 188 images.
5291856
Halftoning soft cores for low-cost digital displays###This paper presents hardware design of a family of soft cores that improve image quality on low cost digital displaying devices with limited color palette. The two-element half-toning method proposed for gray-scale images has been extended for color images. It also supports real-time video streams. Experimental results on Xilinx Spartan 3E1800 FPGAs show that frame rate for 512 times 512 video streams is more than 150 fps.
5291851
Set-based approach in mining sequential patterns###In this paper, we describe a set-based approach for mining association rules and finding frequent sequential patterns in customer transactional databases. The set-based approach is a direct improvement of the original association rule mining algorithms proposed by R. Agrawal and R. Skrikant. Our approach relaxes the constraints described in Apriori (All/Some), and improves the performance while being more user-oriented and self-adaptive than the probabilistic knowledge representation. We compare the performance of the improved algorithms with results from an experimental study. The approach can be extended to more set-based mathematical models for further data analysis in order to discover hidden knowledge and patterns with the improved workflow and set-based representation.
5291850
Exploiting scientific workflows for large-scale gene expression data analysis###Microarrays are state technologies of the art for the measurement of expression of thousands of genes in a single experiment. The treatment of these data are typically performed with a wide range of tools, but the understanding of complex biological system by means of gene expression usually requires integrating different types of data from multiple sources and different services and tools. Many efforts are being developed on the new area of scientific workflows in order to create a technology that links both data and tools to create workflows that can easily be used by researchers. Currently technologies in this area aren't mature yet, making arduous the use of these technologies by the researcher. In this paper we present an architecture that helps the researchers to make large-scale gene expression data analysis with cutting edge technologies. The main underlying idea is to automate and rearrange the activities involved in gene expression data analysis, in order to freeing the user of superfluous technological details and tedious and error-prone tasks.
5291853
Editing heightfield using history management and 3D widgets###In virtual environments, terrain is generally modeled by heightfield, a 2D structure. To be able to create desired terrain geometry, software editors for this specific task have been developed. The graphics hardware, data structures and rendering techniques are developing fast to open up new possibilities to the user and terrain editor functionalities are following such improvements (such as real-time lighting updates during editing operations and multi-texture blending). Yet, current terrain editors mostly fail to give the user feedback about their actions and also fail to help the users understand and undo the editing operations on the terrain. The aim of this study is to investigate the 3d-widget based visualization of possible editing (sculpturing) actions on terrain and to help user undo previous operations.
5291852
Generating motion graphs from clusters of individual poses###In this paper, we propose a methodology to generate novel motion clips from existing motion capture data. We present an automated procedure that performs clustering on the data in order to generate a robust and powerful motion graph. We discard the temporal connection between frames during clustering phase, which lets us spot hubs that motions tend to visit more often and use these poses as nodes of our motion graph. This approach greatly reduces computational times and complexity of the final graph structure while maintaining the representational power.
5291918
Joint utilization of appearance and geometry for determining correspondences###A novel approach, which is based on combining the competence of interest point detectors to capture primitives and the capability of geometric constraints to discriminate between spatial configurations of these primitives is presented. In the proposed approach, the geometric constraints are enforced by means of barycentric coordinates, a mathematical tool that has been utilized in the relevant literature as a neighborhood constraint. In the context of our research, however, these coordinates are utilized as a geometric description of a group of interest points. Using this description, local appearance descriptor based potential matches are intended to be filtered and evaluated according to their geometric consistency. This method is applicable to one-to-one image matching in its current form, and to classification tasks after extensions for appearance generalization.
5291919
A new method for computer generated holography of 3D objects###In this manuscript, a new method for calculating the artificial holograms of three dimensional objects is described. This approximate but fast method is compared with previously defined methods in terms of accuracy and complexity. In addition, an example application of the method to vibration mode shape analysis is presented.
5291914
Using continuous integration and automated test techniques for a robust C4ISR system###We have used CI (continuous integration) and various software testing techniques to achieve a robust C4ISR (command, control, communications, computers, intelligence, surveillance, and reconnaissance) multi-platform system. Because of rapid changes in the C4ISR domain and in the software technology, frequent critical design adjustments and in turn vast code modifications or additions become inevitable. Defect fixes might also incur code changes. These unavoidable code modifications may put a big risk in the reliability of a mission critical system. Also, in order to stay competitive in the C4ISR market, a company must make recurring releases without sacrificing quality. We have designed and implemented an XML driven automated test framework that enabled us developing numerous high quality tests rapidly. While using CI with automated software test techniques, we have aimed at speeding up the delivery of high quality and robust software by decreasing integration procedure, which is one of the main bottleneck points in the industry. This work describes how we have used CI and software test techniques in a large-scaled, multi-platform, multi-language, distributed C4ISR project and what the benefits of such a system are.
5291915
Merging model driven and ontology driven system development approaches pervasive computing perspective###In this paper we present a view point on ldquointelligentrdquo application development for pervasive computing environments. We first point out that today's traditional ldquointelligentrdquo computing is built on strong and hard-coded logical assumptions and computational procedures which are pre-defined by developers, that is, what we call as simulated intelligence within the course of this paper. Such assumptions and procedures are based on enumerations of possible contexts of use which is predefined mappings between situations in contextual space to rational behaviors in behavior space. However pervasive computing applications extend the scope of application's context space and behavior space towards infinity which hardens development of ldquointelligentrdquo systems having a certain degree of rationality. Therefore, approaches merging human intelligence and computing ldquointelligencerdquo are required to be employed. We further advocate that pervasive computing era increases the complexity of application development because of the extended context space, hence software development approaches based on higher abstractions need to be employed where model driven approaches and ontology driven approaches are promising. We propose a basic methodology which merges model driven and ontology driven development approaches. Resulting methodology employs use of formalized conceptual models to be employed both at run-time and development time in terms of reasoning and automatic code generation respectively. We finally point out that such application development paradigm based on pervasive computing perspective will enable users to program their own environment (i.e. smart spaces) in the future.
5291916
Shape detection in images exploiting sparsity###Detection of different kinds of shapes, i.e. lines, circles, hyperbolas etc., in varying kinds of images arises in diverse areas such as signal and image processing, computer vision or remote sensing. The generalized Hough transform is a traditional approach to detect a specific shape in an image by transforming the problem into a parameter space representation. In this paper we use the observation that the number of shapes in an image is much smaller than the number of all possible shapes. This means the shapes are sparse in the parameter domain. Rather than forming the parameter space from the image as in the HT, we take a reverse approach and ask ldquowhich combination of parameter space cells represent my data best?rdquo. This leads us to generate a dictionary of shapes and use additional information about sparsity of shapes within a basis pursuit framework. The results indicate enhanced shape detection performance, increased resolution, joint detection of different shapes in an image and robustness to noise. In addition to this, combining the sparsity of shapes with the compressive sensing ideas shows that it is possible to directly find the shapes in an image from small number of random projections of the image without first reconstructing the image itself.
5291917
Software product line development: A review on practical issues and challenges###Software product line concept has been a key area of concern due to its impacts on reducing the costs and increasing the productivity by means of reuse maximization. The literature has a plethora of success stories about the software product lines. On the other hand, success in product line development requires specific techniques and methodologies at every step of the software development process. Moreover, a defect introduced in any step might have abnormally negative impact due to diffusion, if special care is not devoted. In this paper, with a different point of view, potential risks and hazards in case of `careless product line development' are discussed rather than the benefits of the product line.
5291910
A robust localization framework to handle noisy measurements in wireless sensor networks###We construct a robust localization framework to handle noisy measurements in wireless sensor networks. Traditionally many approaches employ the distance information gathered from ranging devices of the sensor nodes to achieve localization. However the measurements of these devices may contain noise both as hardware noise and as environmental noise due to the employment conditions of the network. It is necessary to provide a general framework that handles such a noise in data and yet still be applicable within several localization algorithms. In order to handle noise in distance measurements, our framework utilizes convex constraints and confidence intervals of a random variable. At the end of the localization process nodes are assigned to a set of feasible regions with corresponding probabilities. The accuracy of the localization can be adjusted and the framework can easily be embedded to work within previously suggested localization algorithms.
5291911
Replacement policies for super-peer in unstructured P2P network###For the last few years, there has been a large volume of research on peer-to-peer (P2P) systems, resulting in many hybrid P2P models. In the hybrid model, all P2P overlay traffic will be routed via the SP. In this model, the SP selection and replacement are important to accomplishing high scalability and reliable service in P2P network. Main purpose of this paper is to provide highly available SP (super-peer) by reducing the replacement of SPs in the network. In this paper, we present a variety of replacement policies and discuss replacement of SPs in a P2P system. We manage SPs by dividing the selected group and the candidate SP group and use a lazy replacement concept to distinguish SP's status between unavailable and available. Finally, we choose new SPs using proposed replacement policies. Through experiments, we can provide the reliable SP by reducing the number of replacements using a timeout value and presented replacement policy. Also, we can improve the resilience of the system by improving the availability of SPs selected by OPs (ordinary-peer). Finally, we present the points to be considered for analysis of past behavior of peers and discuss which factors affect the performance of the system.
5291912
Real-time cutting simulation based on stiffness-warped FEM###Finite element method (FEM) is a widely used method in the simulation of virtual surgery since it can model elasticity behavior of soft tissues accurately. Linear FEM uses constant stiffness matrix that achieves fast and stable simulation but objects increase unnaturally in volume under large rotational deformation. Nonlinear FEM, on the other hand, models large deformation correctly but it is computationally complex. ldquostiffness warpingrdquo method is suitable for real-time virtual surgery applications because it updates rotational parts of the large deformations rapidly and stably. It also preserves the volume. Cutting is the main activity in virtual surgery and various methods have been presented to conduct this activity. This paper presents an improved version of the prior cutting methods used in virtual surgery. The test results indicate that the proposed surgery simulation method based on stiffness-warped FEM could perfectly be utilized in real-time virtual surgery applications.
5291913
A display processing software application framework for mission computer systems###This paper describes a framework for display processing applications in C4ISR systems and discusses Havelsan's approach. Mission computers in C4ISR systems need a framework for both mission processing and display processing applications to enhance productivity and robustness. Each framework has its own requirements and challenges. Our goal is to provide a general display processing framework for all kinds of mission computer applications.
5291848
Generic text summarization for Turkish###In this paper, we propose a generic text summarization method that generates summaries of Turkish texts by ranking sentences according to their scores calculated using their surface level features and extracting the highest ranked ones from the original documents. In order to extract sentences which form a summary with an extensive coverage of main content of the text and less redundancy, we use the features such as term frequency, key phrase, centrality, title similarity and position of the sentence in the original text. Sentence rank is computed using a score function that uses its feature values and the weights of the features. The best feature weights are learned using machine learning techniques with the help of human constructed summaries. Performance evaluation is conducted by comparing summarization outputs with manual summaries generated by 25 independent human evaluators. This paper presents one of the first Turkish summarization systems, and its results are promising.
5291849
Word sense disambiguation for Turkish###Word sense disambiguation (WSD) is the core and one of the hardest problems of many natural language processing tasks. WSD is considered as an AI-complete problem. Although there are many approaches trying to solve this problem, many of them are not adequate to solve WSD problem for Turkish. Dealing with sense ambiguity for Turkish also requires dealing with stemming ambiguity as well as polysemy, homonymy and categorical ambiguity. In this study, largely known Lesk and simplified Lesk methods are modified and adapted to Turkish. The main aim of this project is to minimize the word sense ambiguity for Turkish and this is performed by eliminating the incorrect senses as much as possible by applying proposed methods.
5291842
Prediction of protein-protein interaction relevance of articles using references###Classifying documents as protein-protein interaction (PPI) relevant or not is the first step towards extracting meaningful PPI data from article content. Currently, this classification step is handled manually by expert curators. A number of text-mining methods have been proposed to tackle this problem, using abstracts without references. We propose that article references contain important information that can be used to enhance these previous techniques. We trained an SVM classifier solely based on reference links extracted from Biocreative II data to test the effect of references. Our approach includes a feature selection method based on reference count imbalance between positive and negative examples. Classification results on Biocreative II test and Biocreative II.5 training datasets show that even simple referential information extracted from papers can be effective for predicting protein interaction.
5291843
MotifHider: A knowledge hiding approach to sequence masking###In a typical de novo motif discovery process, it is quite common that many of the motif candidates output from motif discovery programs are either already known motifs or motif-like decoy/repeat patterns. To prevent the false discovery and also to increase the chance of authentic novel motif discovery, some motif discovery programs employ a pre-processing stage in order to mask certain repeat positions in the input sequences. There are a few approaches to sequence masking aimed at avoiding the false discovery. This paper introduces a novel approach and a tool, called MotifHider, to sequence masking problem. MotifHider exploits sensitive knowledge hiding principles from database sharing. By hiding certain patterns, it provides successive motif discovery programs to avoid false discovery and rediscovery. At the same time, it avoids overly distortion of the input sequences so as to retain most of the authentic motifs.
5291840
NVR-BIP: Nuclear vector replacement using binary integer programming for NMR structure-based assignments###Nuclear magnetic resonance (NMR) spectroscopy is an important experimental technique that allows one to study protein structure in solution and to identify important sites in a protein. An important bottleneck in NMR protein structure determination is the assignment of NMR peaks to the corresponding nuclei. Structure-based assignment (SBA) aims to solve this problem with the help of a template protein which is homologous to the target and has applications in the study of structure-activity relationship, protein-protein and protein-ligand interactions. We formulate SBA as a linear assignment problem with additional Nuclear Overhauser Effect (NOE) constraints, which can be solved within Nuclear Vector Replacement's (NVR) framework. Our approach uses NVR's scoring function and data types, and also gives the option of using CH and NH RDCs, instead of NH RDCs which NVR requires. We test our technique on NVR's data set as well as on two new proteins. Our results are comparable to NVR's assignment accuracy on NVR's test set, but higher on novel proteins. Our approach allows partial assignments. It is also complete and can return the optimum as well as near-optimum assignments. Furthermore, it allows us to analyze the information content of each data type and is easily extendable to accept new forms of input data, such as additional RDCs.
5291841
Toward a semantic approach to improve data exchange in an industrial information system###Our research takes place inside a large public transport company, made up of a main group and about fifty subsidiary companies. In this paper, we discuss the problems associated with the interoperability and information exchange between the company's repositories. Within this context, a prototype system for the automatic detection and propagation of data modifications within the applications and projects repositories is presented. Our goal is to propose a new model for information exchange inside the enterprise. By focusing on data ambiguity, we propose a solution for interoperability by using constraints to solve those ambiguities. This is a first step toward a semantic approach, improving data exchanges by understanding the meaning of the information.
5291846
Dimensional fact model extension via predicate calculus###The dimensional fact model is a conceptual model that allows to design the multidimensional schema of a data warehouse. Its methodology is based on the (re)modelling of the schema of a relational database; such a schema is represented by a tree, and the (re)modelling consists of the traditional operations on graphs (as prune, graft, adding child, change parent, ...). The tree is generated automatically by an algorithm, starting from the logical schema of a relational database. Nowadays, this algorithm is not able to manage many-to-many relationships, and then, it stops the tree creation when it meets this kind of relationships. In this work, we propose a novel methodology, able to overcome this algorithmic limit, via the declarative approach of the logical programming.
5291847
Calculating the VC-dimension of decision trees###We propose an exhaustive search algorithm that calculates the VC-dimension of univariate decision trees with binary features. The VC-dimension of the univariate decision tree with binary features depends on (i) the VC-dimension values of the left and right subtrees, (ii) the number of inputs, and (iii) the number of nodes in the tree. From a training set of example trees whose VC-dimensions are calculated by exhaustive search, we fit a general regressor to estimate the VC-dimension of any binary tree. These VC-dimension estimates are then used to get VC-generalization bounds for complexity control using SRM in decision trees, i.e., pruning. Our simulation results shows that SRM-pruning using the estimated VC-dimensions finds trees that are as accurate as those pruned using cross-validation.
5291844
Mining quantitative class-association rules for software size estimation###Associative models are usually applied in knowledge discovery problems in order to find patterns in large databases containing mainly nominal data. This work is focused on two different aspects, the predictive use of association rules and the management of quantitative attributes. The aim is to induce class association rules that allow predicting software size from attributes obtained in early stages of the project. In this application area, most of the attributes are continuous; therefore, they should be discretized before generating the rules. Discretization is a data mining preprocessing task having a special importance in association rule mining since it has a significant influence on the quality and the predictive precision of the induced rules. In this paper, a multivariate supervised discretization method is proposed, which takes into account the predictive purpose of the association rules.
5291845
Extracting similar sub-graphs across PPI networks###Singling out conserved modules (corresponding to connected sub-graphs) throughout protein-protein interaction networks of different organisms is a main issue in bioinformatics because of its potential applications in biology. This paper presents a method to discover highly matching sub-graphs in such networks. Sub-graph extraction is carried out by taking into account, on the one side, both protein sequence and network structure similarities and, on the other side, both quantitative and reliability information possibly available about interactions. The method is conceived as a generalization of a known technique, able to discover functional orthologs in interaction networks. Some preliminary experimental results obtained with both synthetic and real data are also accounted for in the paper.
5291929
Image annotation with semi-supervised clustering###Methods developed for image annotation usually make use of region clustering algorithms. Visual codebooks are generated from the region clusters of low level features. These codebooks are then, matched with the words of the text document related to the image, in various ways. In this paper, we supervise the clustering process by using three types of side information. The first one is the topic probability information obtained from the text document associated with the image. The second is the orientation and the third one is the color information around each interest point. The side information provides a set of constraints in a semi-supervised k-means region clustering algorithm. Consequently, in clustering of regions not only low level features, but also this extra information is used. Experimental results show that image annotation with semi-supervision of side information is more successful compared to the one that uses low level features alone. Moreover, a speedup is obtained in the modified k-means algorithm because of the constraints. The proposed algorithm is implemented in a high performance parallel computation environment.
5291928
Video shot boundary detection by graph-theoretic dominant sets approach###We present a video shot boundary detection algorithm based on the novel graph theoretic concept, namely dominant sets. Dominant sets are defined as a set of the nodes in a graph, mostly similar to each other and dissimilar to the others. In order to achieve this goal, candidate shot boundaries are determined by using simply pixel-wise differences between consequent frames. For each candidate position, a testing sequence is constructed by considering 4 frames before the candidate position and 2 frames after the candidate position. Proposed method works on a weighted undirected graph, where the graphs are established by using the frames in the testing sequence. Each frame in the sequence corresponds to a node in the graph, whereas edge weights between the nodes are calculated by using pairwise similarities of frames. By utilizing the complete information of the graph, its dominant set is detected. The simulation results indicate that the proposed algorithm can be a promising approach for abrupt shot boundary detection.
5291921
Dynamic feature weights with relevance feedback in content-based image retrieval###In this paper, we present a novel relevance feedback method for content-based image retrieval systems based on dynamic feature weights. The proposed method utilizes intra-cluster and inter-cluster information for representing the descriptive and discriminative properties of the features according to the labeled images by the user. Afterwards, feature weights are updated dynamically according to the user's preferences for improving retrieval results. The proposed method has been thoroughly evaluated and selected results are illustrated in the paper. It is shown that, satisfactory improvements can be achieved with small number of iterations and labeled samples. Furthermore, it is a low-complex and flexible method that can be used on various databases and content-based image retrieval applications.
5291920
Recognizing faces in news photographs on the web###We propose a graph based method in order to recognize the faces that appear on the web using a small training set. First, relevant pictures of the desired people are collected by querying the name in a text based search engine in order to construct the data set. Then, detected faces in these photographs are represented using SIFT features extracted from facial features. The similarities of faces are represented in a graph which is then used in random walk with restart algorithm to provide links between faces. Those links are used for recognition by using two different methods.
5291923
Feature extraction and discriminating feature selection for 3D face recognition###This paper presents a 3D face recognition method. In this method, 3D discrete cosine transform (DCT) is used to extract features. Before the feature extraction, faces are aligned with respect to nose tip and then registered two times: according to average nose and average face. Then the coefficients of 3D transformation are calculated. The most discriminating 3D transform coefficients are selected as the feature vector where the ratio of between-class variance and within-class variance is used for discriminant coefficient selection. The results show that the most energetic features, low frequency components, are not the most discriminating features. The method was also modified based on 3D discrete Fourier transform (DFT) for feature selection as regarding real and complex DFT coefficients as independent features. Discriminating features were matched by using the nearest neighbor classifier. Recognition experiments were realized on 3D RMA face database. The proposed method yields a recognition rate above 99% for 3D DCT based features.
5291922
A new histogram modification based robust image data hiding technique###The main objective of this presented research work is to implement a steganography application simply based on a new image histogram modification approach. An image histogram is a type of histogram which acts as a graphical representation of the tonal distribution in a digital image. The stego images, obtained using the proposed data hiding method, are inherently robust against main geometrical attacks such as rotation, scattered tiles and warping. The application of proposed method has comparatively higher secret data embedding capacity than its counterparts, as well as providing relatively enhanced PSNR results. Not only has it improved the data embedding capacity from 300-360 bits to 665-747 bits but also the PSNR from 53.10-55.18 dB to 56.01-62.75 dB for the well-known Lena, Baboon and Pepper images, compared to one of its important counterparts.
5291925
Feature selection for person-independent 3D facial expression recognition using NSGA-II###In this paper, the problem of person-independent facial expression recognition from 3D facial features is investigated. We propose a methodology for the selection of features that uses a multi-objective genetic algorithm where the number of features is optimized to improve classification accuracy. The facial feature selection aims to derive a set of features from the original expression images, which minimizes the within-class separability and maximizes the between-class separability. We used non-dominated sorted genetic algorithm II (NSGA II) which is one of the latest genetic algorithms developed for resolving problems of multi-objective aspects with more accuracy and higher convergence speed. The proposed methodology is evaluated using 3D facial expression database BU-3DFE. Facial expressions such as anger, sadness, surprise, joy, disgust, fear and neutral are successfully recognized with an average recognition rate of 88.18%.
5291924
Interactive MPEG-4 videos: An impress extension for MPEG-4 conversion and a streaming architecture for e-learning###MPEG-4 gives the opportunity to describe the scenes of a motion picture directly through XMT-A and XMT-O: two ldquohigh level languagesrdquo that are XML based. Moreover MPEG-4 allows (and defines) the opportunities for video streaming. This makes this standard appealing for the conversion of Impress or PowerPoint presentations and also for other e-learning applications because it offers the possibility of publishing the converted presentations on streaming servers (as for example the Darwin server). In this paper we exploit those possibilities by implementing an Impress extension that translates an electronic presentation to XMT-O, so to be finally ldquocompiledrdquo for MPEG-4 conversion, and a streaming architecture for e-learning.
5291927
Template-based level set segmentation using anatomical information###We present a preliminary evaluation of an automated segmentation method of the quadriceps muscles from MR images of the thigh. The method is being developed to assist research into morphological properties of the quadriceps muscles as biomarkers of osteoarthritis (OA) incidence and progression. Our method uses an anatomically anchored, template-based initialization of the level set-based segmentation approach. A template image is selected using the Kullback-Leibler divergence measure based on the muscle and fat content of the thigh images. Contours of the quadriceps muscles of the chosen template are then semi-automatically registered to the image to be segmented using an affine transformation. These registered contours are used as initializations for the multi-phase level-set segmentation of the image, which is pre-processed to reduce arterial flow artifacts, the bias field, and intramuscular fat/connective tissue. Thirteen studies from eleven different subjects were analyzed. The performance was compared against manual segmentations using the Zijdenbos similarity index (ZSI). The ZSI means and standard deviations were: rectus femoris, 0.73 plusmn 0.13; vastus intermedius, 0.78 plusmn 0.09; vastus lateralis, 0.81 plusmn 0.14; vastus medialis, 0.85 plusmn 0.10.
5291926
An image-processing based automated bacteria colony counter###This paper presents an image processing based automated counting system to detect the number of bacteria colonies that develop in Petri dishes of microbiology laboratories. The visible colonies represent the initial number of bacteria present in the aqueous environment. The counting system contains shape based segmentation and classification algorithms. Colonies are considered as (possibly overlapping with some amount of amorphous deviations from) discs and classified as a cluster of bacteria with respect to their compactness ratio. The system is implemented using Matlab, and tested using ground truth data provided from Anadolu University, Dept. of Environmental Engineering microbiology laboratory. Results are presented.
5291931
Single stripe projection based range scanning of shiny objects under ambient light###Range scanners are used in various applications in industry. Therefore, various range scanners, based on different working principles are developed. Among these, scanners using stripe based triangulation are the most promising ones. Unfortunately, these scanners have problems in obtaining the range data of shiny objects (having highlights) under ambient light. In this study, we develop a stripe projection based range scanner to solve this problem. Our main contribution is on detecting the stripe using a color invariant in a robust manner. We developed a projector based scanner using this color invariant. Our scanner has also the advantage of flexibly changing stripe color for different colored objects. We test the performance of our novel stripe detection method under various controlled experiments. We also test the overall performance of our scanner in extracting the range data of several objects having highlights. Extensive testings indicate the success of our range scanner, especially for scanning objects having highlights under ambient light.
5291839
An evolutionary genetic algorithm for optimization of distributed database queries###High performance low cost PC hardware, and high speed LAN/WAN technologies make distributed database(DDB) systems an attractive research area. Since Dynamic programming is not feasible for optimizing queries in a DDB, we propose a GA based query optimizer and compare its performance to random and optimal algorithms. We analyzed a set of possible GA parameters and determined that two-point truncate technique using GA gives the best results. New mutation and crossover operators have also been defined and experimentally analyzed. We performed experiments on a synthetic database with replicated relations, but no horizontal or vertical fragmentation. Network links are assumed to be gigabit Ethernet. Comparisons with optimal results found by exhaustive search show that our new GA formulation performs only 20% off the optimal results and we have achieved a 50% improvement over a previous GA based algorithm.
5291838
PopulusLog: People information database###Information about individuals on publicly available Web sites stands as a valuable, yet unorganized, data source. Turning such an enormous data source into a ldquodatabaserdquo is highly desirable as it has the potential to lead to novel ways of using the available information to the largest extent. In this paper, we present PopulusLog, a novel Web data mining system. PopulusLog is a pioneering example of next generation search engines which produces and provides access to non-intuitive knowledge on the Web. It involves a framework for tools that collect, extract, mine, query, browse, and visualize information about anonymous people.
5291837
Query smearing: Improving classification accuracy and coverage of search results using logs###High dimensional concept spaces have various applications in Web search including personalized search, related page computation, diversity preservation, user interest inference, similarity computation, and advertisement targetting. Clustering and classification methods are common means to map documents and users into concept spaces. In most classification algorithms, precision (accuracy) and recall (coverage) tend to be competing aspects. In this paper, we introduce query smearing, an algorithm that can significantly improve both the accuracy and coverage of an existing classifier by leveraging information contained in fully anonymized search engine logs. Starting with a potentially incomplete seed classification, it expands the classification information to cover various items in search engine logs using a weighted majority voting scheme. The technique is similar to semi-supervised learning approaches and may be classified as one, but we have notable differences from most such examples. In particular, initial labels are not fully trusted for accuracy or completeness (hence, after the first stage, they can be thrown away), and additional relationships between classified items are used extensively to guide the process. Empirical evaluation shows that our algorithm performs well under the following assumptions: (i) the search engine log contains a sufficiently large number of query transactions, (ii) most results of most queries are relevant and on-topic, and (iii) sufficient fraction of search results are classified in the seed classification, and those classifications are reasonably accurate (but not necessarily complete).
5291836
Employing named entities for semantic retrieval of news videos in Turkish###Named entities are known to be important means for semantic annotation of news texts. Considerable work has been carried out for semantic indexing of both textual news and news videos especially in English through the employment of named entities extracted from textual news or transcriptions of the news videos. In this paper, we present our semantic retrieval architecture for news videos in Turkish based on prior semantic annotation of the videos with the corresponding named entities in the news transcription texts. We employ a rule-based named entity recognizer for Turkish which makes use of handcrafted sets of lexical resources and pattern bases. We compiled a small corpus of Turkish news videos and the named entity recognizer in its current form achieves a success rate of about 75% on this corpus. A retrieval interface is implemented to access the video corpus through the Boolean queries formed with the extracted named entities. The interface currently does not involve any ranking procedure, displaying all the videos, the transcription texts of which satisfy the Boolean query posed through the interface, sorted by their broadcast date. The presented study is significant for its being the first study to perform automatic semantic video annotation on a genuine news video corpus in Turkish and demonstrating the utilization of the annotations through a retrieval interface.
5291835
Efficient k-word proximity search###In this paper, we propose the ldquoaddedrdquo use of proximity search to a Web search query for narrowing down the set of documents returned as answers to a keyword based search query. This approach adds value to Web search query results by allowing users to better express what they are looking for. Most of the current search engines provide limited proximity search behaviour such as allowing only two query terms. While there are many algorithms for k-word near proximity search, there is no work for k-word ordered proximity search. This paper presents (a) a new algorithm for k-word ordered proximity search in a document, which runs in O(nlogk) time per document where n is the number of words (terms) in the document, and k is the number of query terms in a query, (b) enhancements to ranking techniques related to proximity search, and (c) a suggestion involving frequent combinations of query terms in order to help users locate the desired documents more accurately.
5291834
Exploring queriability of encrypted and compressed XML data###When large XML documents are shared, documents compression and encryption become simultaneously important for efficient and secure access. Existing approaches for compressing and encrypting large XML documents provide a non-queriable intermediate document representation, creating a need to decompress and decrypt the whole document before any access, which simply wastes computing resources (such as memory, time, and power) especially in mobile environment. To overcome this limitation, this paper proposes a new approach for compressing and encrypting large XML documents while maintaining queriability over the intermediate document representation. The proposed approach separates document structure from its contents using a variation of the Ctree XML indexing approach (known as the Ctree<sup>+</sup>), then applies context-free lossless encryption and compression techniques over the resulting Ctree<sup>+</sup> intermediate representations. Experiments results show that the proposed approach dramatically enhances queries response times when compared with existing approaches. However, the proposed approach provides lesser compression ratios when compared with context-based compression techniques.
5291833
Performance study of a wireless mobile ad hoc network with orientation-dependent inter-node communication links###A Petri-net-based simulation model of a wireless mobile ad hoc network is studied. The model includes a parameterized mobility model and a novel scheme of orientation-dependent internode communication links with random states. In simulation, three fundamental performance metrics-packet delivery rate, average number of hops and relative network traffic - are investigated under different combinations of model parameters.
5291832
MIPS extension for a TCAM based parallel architecture for fast IP lookup###This paper discusses the feasibility of the use of minimum independent prefix set (MIPS) algorithm in a ternary content addressable memory (TCAM) based parallel architecture proposed for high speed packet switching. MIPS algorithm is proposed earlier to transform a forwarding table into one that contains the minimum independent prefix set. In this paper we propose MIPS algorithm to be integrated into a TCAM based parallel IP lookup engine. We first analyze the table compression performance of the MIPS algorithm and demonstrate that it may lead to routing table expansion rather than compression in general. For certain cases on the other hand, MIPS algorithm performs well and is suitable to be used in TCAM based parallel IP lookup engine. We demonstrate that with the adoption of the MIPS algorithm, the overall performance, in terms of throughput, of the IP lookup engine can be enhanced considerably.
5291831
Preimages of hash functions through rainbow tables###In this paper, we introduce a natural way of how to find preimages on a hash function by using a rainbow table even if the hash function utilizes the Merkle-Daringmgard (MD) strengthening as a padding procedure. To overcome the MD strengthening, we identify the column functions as representatives of certain set of preimages, unlike conventional usage of rainbow tables or Hellman tables to invert one-way functions. The workload of the precomputation to prepare a table is as much as the workload of brute force as usual. Then, one can find a preimage of a given arbitrary digest value in 2<sup>2n/3</sup> steps by using 2<sup>2n/3</sup> memory where n is both the digest size and the length of the chaining value. The notion of the attack is extended to certain improved variants of the MD construction. We generalized the attack when the digest size is not equal to the length of chaining value. We verified the results experimentally as well.
5291830
Channel assignment problem in cellular networks: A reactive tabu search approach###The channel assignment problem (CAP) in cellular networks is concerned with the allocation and reuse of the frequency spectrum to the base stations in such a way that both the interference constraints and the traffic requirements of the cells are met. In this paper, we apply a reactive tabu search based method to solve the CAP and compare this approach with the classical tabu search as well as genetic algorithm based approaches in the literature.
5291828
Feature selection for collective classification###When in addition to node contents and labels, relations (links) between nodes and some unlabeled nodes are available, collective classification algorithms can be used. Collective classification algorithms, like ICA (iterative classification algorithm), determine labels for the unlabeled nodes based on the contents and/or labels of the neighboring nodes. Feature selection algorithms have been shown to improve classification accuracy for traditional machine learning algorithms. In this paper, we use a recent and successful feature selection algorithm, mRMR (minimum redundancy maximum relevance, Ding and Peng, 2003), on content features. On two scientific paper citation data sets, Cora and Citeseer, when only content information is used, we know that the selected features may result in almost as good performance as all the features. When feature selection is performed both on content and link information, even better classification accuracies are obtained. Feature selection considerably reduces the training time for both content only and ICA algorithms.
5291829
Reducing power consumption in wired networks###Over 500 million host computers, three billion PCs and mobile devices consume over a billion kilowatts of electricity. As part of this ldquosystemrdquo computer networks consume an increasing amount of energy, and help reduce energy expenditure from other sources through e-work, e-commerce and e-learning. Traditionally, network design seeks to minimise network cost and maximise quality of service (QoS). This paper examines some approaches for dynamically managing wired packet networks to minimise energy consumption while meeting users' QoS needs, by automatically turning link drivers and/or routers on/off in response to changes in network load.
5291824
A MLP based PD estimation model for SME credits###This study presents a multilayer perceptron (MLP) cascaded with a logistic regression (LR) model for the probability of default (PD) estimation of real-life small and medium enterprises (SMEs). The MLP does a preprocessing for LR which is used to compute PD value. The obtained raw PD values are calibrated according to the real portfolio default average. In experiments, a Turkish SME database is used. The results indicate that the cascaded MLP-LR model provides better classification accuracy and outperforms commonly used logistic regression.
5291825
Turkish keyphrase extraction using multi-criterion ranking###Keyphrases have been extensively used for indexing and searching in databases and information retrieval systems. In addition, they provide useful information about semantic content of a document. In this paper, we propose an algorithm for automating Turkish keyphrase extraction. Several features of candidate phrases are exploited and form the extraction task as a problem of finding optimal set of candidate phrases. We use multi-criterion ranking to tackle this problem.
5291826
Multi-robot pursuit with visibility constraints###We address the problem of pursuing a moving target from multiple angles using multiple mobile robots. The pursuer uses a modified form of pursuit curve in order to reach the multiple aim points which are at a pre-determined distance and angle from the target. This is different from existing works that track a target to localize it, follow a target by keeping it in the current frame of view generally from behind the target, or pursue a target in order to establish a line of sight. The focus in our work is on an efficient pursuit of the target such that the pursuers move to a position in order to obtain certain task specific information or perform actions that require specific positioning with respect to the target. Examples include following a target and taking left, right, and frontal images of it for accurate identification, or robots approaching a vehicle for docking, etc. In this paper, we define the multi-angle pursuit problem for multiple robots and present a Pareto optimal solution for task allocation.
5291827
Improvement on corpus-based word similarity using vector space models###This paper presents a new approach for finding semantically similar words from large text collection using window based context methods. Previous studies on this problem mainly concentrate on finding new methods which are new combination of distance-weight measurement methods or new context methods. The main difference of our approach is that we focus on reprocessing of existing methods' outputs to update the representation of related word vectors, which are used for measuring semantic distance between words, to further improve the results. This new approach can be easily applied to many of the existing word similarity methods using the vector space model for representing contexts. We claim that our method improves the performance of some of the existing similarity measuring methods.
5291820
Joint visual attention modeling for naturally interacting robotic agents###This paper elaborates on mechanisms for establishing visual joint attention for the design of robotic agents that learn through natural interfaces, following a developmental trajectory not unlike infants. We describe first the evolution of cognitive skills in infants and then the adaptation of cognitive development patterns in robotic design. A comprehensive outlook for cognitively inspired robotic design schemes pertaining to joint attention is presented for the last decade, with particular emphasis on practical implementation issues. A novel cognitively inspired joint attention fixation mechanism is defined for robotic agents.
5291821
Multi-relational concept discovery with aggregation###Concept discovery aims at finding the rules that best describe the given target predicate (i.e., the concept). Aggregation information such as average, count, max, etc. are descriptive for the domains that an aggregated value takes part in the definition of the concept. Therefore, a concept discovery system needs aggregation capability in order to construct high quality rules (with high accuracy and coverage) for such domains. In this work, we describe a method for concept discovery with aggregation on an ILP-based concept discovery system, namely C<sup>2</sup>D-A. C<sup>2</sup>D-A extends C<sup>2</sup>D by considering all instances together and thus improves the generated rule's quality. Together with this extension, aggregation handling mechanism is modified accordingly, leading to more accurate aggregate values, as well.
5291822
Unsupervised learning of affordance relations on a humanoid robot###In this paper, we study how the concepts learned by a robot can be linked to verbal concepts that humans use in language. Specifically, we develop a simple tapping behaviour on the iCub humanoid robot simulator and allow the robot to interact with a set of objects of different types and sizes to learn affordance relations in its environment. The robot records its perception, obtained from a range camera, as a feature vector, before and after applying tapping on an object. We compute effect features by subtracting initial features from final features. We cluster the effect features using Kohonen self-organizing maps to generate a set of effect categories in an unsupervised fashion. We analyze the clusters using the types and sizes of objects that fall into the effect clusters, as well as the success/fail labels manually attached to the interactions. The hand labellings and the clusters formed by robot are found to match. We conjecture that this leads to the interpretation that the robot and humans share the same ldquoeffect conceptsrdquo which could be used in human-robot communication, for example as verbs. Furthermore, we use ReliefF feature extraction method to determine the initial features that are related to clustered effects and train a multi-class support vector machine (SVM) classifier to learn the mapping between the relevant initial features and the effect categories. The results show that, 1) despite the lack of supervision, the effect clusters tend to be homogeneous in terms of success/fail, 2) the relevant features consist mainly of shape, but not size, 3) the number of relevant features remains approximately constant with respect to the number of effect clusters formed, and 4) the SVM classifier can successfully learn the effect categories using the relevant features.
5291823
Role of preferred terminology in the classification of medical reports###We explore the role of the unified medical language system metathesaurus preferred terms in multi-label classification of medical reports. We use preferred terms to represent lexically-disparate but semantically-equivalent medical concepts. We experiment with two classifiers: one based upon Lucene and the other based upon BoosTexter. We find the performance of these systems fails to show consistent improvement when their input is enriched with preferred terminology. Our analysis shows that in our data, some reports can be correctly classified not only in the absence of preferred terms, but in some cases, in the absence of direct references to the diseases themselves.
5291819
Using semantic information for web usage mining based recommendation###Web usage mining has become popular in various business areas related with Web site development. In Web usage mining, commonly visited navigational paths are extracted in terms of Web page addresses from the Web server visit logs, and the patterns are used in various applications including recommendation. The semantic information of the Web page contents is generally not included in Web usage mining. In this work, a framework for integrating semantic information with Web usage mining is presented. The frequent navigational patterns are extracted in the form of ontology instances instead of Web page addresses and the result is used for generating Web page recommendations to the visitor. In addition, an evaluation mechanism is implemented in order to test the success of the recommendation. Test results show that more accurate recommendations can be obtained by including semantic information in the Web usage mining.
5291818
LDA-based keyword selection in text categorization###Text categorization is the task of automatically assigning unlabeled text documents to some predefined category labels by means of an induction algorithm. Since the data in text categorization are high-dimensional, feature selection is broadly used in text categorization systems for reducing the dimensionality. In the literature, there are some widely known metrics such as information gain and document frequency thresholding. Recently, a generative graphical model called latent dirichlet allocation (LDA) that can be used to model and discover the underlying topic structures of textual data, was proposed. In this paper, we use the hidden topic analysis of LDA for feature selection and compare it with the classical feature selection metrics in text categorization. For the experiments, we use SVM as the classifier and tf*idf weighting for weighting the terms. We observed that almost in all metrics, information gain performs best at all keyword numbers while the LDA-based metrics perform similar to chi-square and document frequency thresholding.
5291811
Active humanoid vision and object classification###In this paper we study object learning and recognition on a humanoid robot with foveated vision. The developed approach is view-based and can learn viewpoint-independent representations for object recognition. The training data is collected statistically and in an interactive way where a human instructor freely shows the object from a number of different viewpoints. The proposed system was fully implemented and runs in real-time, which is essential for meaningful interaction with a humanoid robot.
5291810
Forward kinematics of the 3RPR planar parallel manipulators using real coded genetic algorithms###This article examines genetic algorithms to solve the forward kinematics problem applied to planar parallel manipulators. Most of these manipulators can be modeled by the tripod 3-RPR. The conversion of an equation system solving problem into an optimization one is investigated. Parallel manipulator kinematics are formulated using the explicit inverse kinematics model (IKM). From the displacement based equation systems, the objective function is formulated specifically for the FKP using one squared error performance criteria. The proposed approach implements an elitist selection process where a new mutation operator for real-coded GA is analyzed. Experiments on a typical manipulator example shows the fast convergence of the proposed method.
5291813
Towards linking affordances with mirror/canonical neurons###The notion of affordances is often directly linked with studies on mirror and canonical neurons as well as studies that try to relate language. In our previous work, we had developed a formalization for the learning, perception and use of affordances in autonomous robots. In this paper, we will present ideas on how this formalization can be used as a base to link affordances with mirror/canonical neuron studies. Specifically, we will review the studies on mirror and canonical neurons as well as studies on modeling them and relating them to affordances, and will propose how our own affordances formalization can be extended to link affordances with mirror and canonical neurons.
5291812
Does attention capacity correlate with the effects of chord function on phoneme monitoring?###The present study assessed the relationship between attention capacity and effects of chord function on phoneme monitoring. Attention capacity was measured with performance on Stroop task and choice reaction time task. The results showed a significant correlation between Stroop performance and chord function effects on phoneme monitoring, in that, participants with high Stroop performance were better at blocking the interference of chordal processing on their attention capacity. In addition, participants with high focused attentional processing capacity discriminated target syllables faster, since choice reaction time score positively correlated with average correct reaction time for both ldquodirdquo and ldquodurdquo.
5291815
Using 3D contours and their relations for cognitive vision and robotics###In this work, we make use of 3D visual contours carrying geometric as well as appearance information. Between these contours, we define 3D relations that encode structural information relevant to object-level operations such as similarity assessment and grasping. We show that this relational space can also be used as input features for learning which we exemplify for the grasping of unknown objects. Our representation is motivated by the human visual system in two respects. First, we make use of a visual descriptor that is motivated by hyper-columns in V1. Secondly, the contours can be seen as one stage in a visual hierarchy bridging between local symbolic descriptors to higher level stages of processing such as object coding and grasping.
5291814
Effect of wave height to connectivity and coverage in sea surface wireless sensor networks###Sea surface sensor networks with underwater sensing units and oversea communication units, formed either by bottom-anchored or floating sensor nodes, have many scientific, commercial, military, and industrial applications. In these networks, waves constitute significant obstacles for radio communication over the sea surface. We have developed a shallow water wave height model, and analyzed the effect of wave heights to communication and network connectivity under various sea states, by numerous simulations, in order to determine the number of wireless sensor nodes to provide necessary connectivity and coverage for a region. As the waves get higher, the number of nodes required to provide appropriate coverage redundancy increases. Using our wave model, the number of nodes required to provide application-specific redundancy can be determined.
5291817
A path-quality-aware peer-to-peer file sharing protocol for mobile ad-hoc networks: Wi-share###Peer-to-peer networks are rather well-studied and currently there are numerous systems based on peer-to-peer principles running on the Internet. On the other hand peer-to-peer networks for mobile ad-hoc networks have attracted attention only in the recent years. In this paper, we propose a novel peer-to-peer file sharing system particularly designed for mobile ad-hoc networks. The proposed system, namely Wi-Share, has both network and application layer aspects enabling efficient search and download of the shared files. Wi-Share uses reactive routing for the search operation combined with source discovery and uses the routing tables constructed during the search operation for the download operation. In order to increase the overall efficiency of the file sharing in the network, Wi-Share applies techniques to reduce the required traffic and to increase efficient parallelism of the download operation. These techniques include filtering search results, preferring the higher quality routing paths, using partitioned download scheme and allowing the nodes that have joined to the network recently to contribute to the ongoing downloads. Wi-Share is implemented to work on mobile computers and the results of several experiments are also presented in the paper.
5291816
A secure key establishment protocol for zigbee wireless sensor networks###ZigBee is a wireless sensor network standard that defines network and application layers on top of IEEE 802.15.4's physical and medium access control layers. In the latest version of ZigBee, enhancements are prescribed for the security sublayer but we show in this paper that problems persist. In particular we show that the end-to-end application key establishment protocol is flawed and we propose a secure protocol instead. We do so by using formal verification techniques based on static program analysis and process algebras. We present a way of using formal methods in wireless network security, and propose a secure key establishment protocol for ZigBee networks.
5291891
Novel traffic lights signaling technique based on lane occupancy rates###In a conventional traffic lights controller, the lights either change at constant cycle times or at times proportional to the length of each leg of the intersection. Such approaches clearly are not perfect for optimizing traffic flow. Waiting times proportional to lane length may work well for a single-lane road but when roads with multiple lanes are considered the solution would not be optimal. The authors believe that an adaptive signaling based on fullness of each leg of the intersection would be a better approach. This paper presents the segmentation of foreground objects from frames of the surveillance video using an adaptive K-Gaussian mixture model and describes an approach for determining the lane occupancy rates for the north leg of the intersections. To give an accurate fullness measure the cast shadows that might be present in the segmented foregrounds are removed using a combined probability map called the shadow confidence score. Simulation results are provided for two standard and one custom recorded sequence.
5291890
Recognizing partially occluded irises using subpattern-based approaches###In this study, iris recognition in the presence of partial occlusions is investigated using holistic and subpattern-based approaches. Principal Component Analysis (PCA) and subspace Linear Discriminant Analysis (ssLDA) methods are used as feature extractors to recognize iris images. In order to eliminate the effect of illumination changes, histogram equalization and mean-and-variance normalization techniques are used. The recognition performance of the holistic approaches is compared with the performance of subpattern-based approaches spPCA, mPCA and subpattern-based ssLDA approaches in order to demonstrate the performance differences and similarities between these two types of approaches in the presence of partial occlusions. Various experiments are carried out on CASIA, UPOL and UBIRIS databases to demonstrate the effect of occlusions on iris recognition with holistic and subpattern-based approaches.
5291893
Adaptive and fixed eigenspace methods with a novel fitness measure for video based face recognition###In this paper a new system for identifying faces from video sequences using adaptive and fixed eigenspace approaches with a novel fitness measure is proposed. During the recognition process, each image in the gallery set is assigned a fitness value. The fitness value is updated for each frame and at the end of the probe video; the person corresponding to the gallery image with the highest fitness value is declared to be the identified person in the probe video. Eigenspace is used, for generating the feature vectors from the gallery set images. Two approaches have been introduced, where in the first approach; the eigenspace is updated after each frame is processed. The eigenspace update is performed by updating the fitness values and discarding the gallery images with the lowest respective fitness values. In the second method, a fixed eigenspace is generated from the initial gallery set and the fitness value for each gallery image is updated through the processing of the frames in the probe video. Again, in the end the gallery face image with the highest fitness value is declared to be the identified person. The BANCA video face database was adapted for performance testing. Both of the methods showed very competitive recognition rates in different scenarios.
5291892
Automatic generation for web services conversations adapters###Web services interact with each other over the Web to accomplish different business objectives, such interactions are known as services conversations, which are specified via the exchanged messages containing the operations to be invoked. Service consumers search the Web for services that fulfil their requirements. Unfortunately, due to services heterogeneities, services consumers usually cannot find the perfect match for their requests, hence they manually create adapters to mediate between chosen services and their requests. Creating such adapters manually becomes an obstacle for achieving high business agility, and definitely increases development efforts and costs. To overcome this problem, this paper proposes an approach for automatically generating conversations adapters. The proposed approach is ontology-based, where the adopted ontology provides the conversion semantics between application domain concepts via a graph data structure known as the concepts substitutability graph. The proposed approach uses such conversion semantics to determine the mappings between different conversation messages, and to generate the suitable concepts convertors, which are later used to build the required adapter. We believe the proposed approach helps in improving business agility and responsiveness, and of course minimizes efforts needed for developing SOA applications.
5291895
Co-occurrence based statistical approach for face recognition###This paper introduces a new face recognition method based on the gray-level co-occurrence matrix (GLCM). Both distributions of the intensities and information about relative position of neighbourhood pixels are carried by GLCM. Two methods have been used to extract feature vectors from the GLCM for face classification. The first, method extracts the well-known Haralick features to form the feature vector, where the second method directly uses GLCM by converting the matrix into a vector which can be used as a feature vector for the classification process. The results demonstrate that using the GLCM directly as the feature vector in the recognition process outperforms the feature vector containing the statistical Haralick features. Additionally, the proposed GLCM based face recognition system outperforms the well-known techniques such as principal component analysis and linear discriminant analysis.
5291894
XLambda: A functional programming language with XML syntax###We describe XLambda, a functional language with XML syntax, and its processor which is implemented fully and completely in XSLT. XLambda has all the basic features of a functional language, such as defining named functions, operations on numbers, passing functions as parameters, constructing arbitrary data structures etc. What sets XLambda apart from the rest of the functional languages is not its feature set though, but rather its syntax and processor which is implemented as an XSLT stylesheet. Since most Web browsers have XSLT processors already built in, XLambda has the potential to be used as a scripting language in Browsers, much in the same way as JavaScript, among other possibilities.
5291897
Investigation of mashups for managers###Main focus of this paper is on investigating mashups for managers in organizations. The question to answer is whether mashups have value for managers, to what extent, and in what ways. The first part of the paper is dedicated to reviewing of what managers do in their work. The second part is to determine what mashups are and what they do. The overlapped functions that managers and mashups share will be highlighted. In order to evaluate mashup effectiveness in management in real world, a conducted experiment will be provided as proof of concept. Therefore, a further study is dedicated to experimenting mashups in the School of Postgraduate Studies (SPS) Admission Unit in Universiti Teknologi Malaysia (UTM). Corresponding managers are involved in assessing mashups usefulness for their management processes. Finally the study will propose mashups for managers in the light of previous studies on mashups themselves and results from the case study.
5291896
Turkish &#x2014; English cross language information retrieval using LSI###This paper describes a study of Turkish-English cross language information retrieval (CLIR) system. One of the biggest issues with CLIR studies is to access to bi-lingual parallel corpus. So, the first step of this study was to construct a parallel Turkish-English corpus. We have constructed a corpus that has 1801 parallel documents. The corpus has been divided in to two parts, first one for training the system and second one for testing the system. Latent semantic indexing (LSI) techniques applied to the training set to obtain the language relations. After the training, we have performed set of tests (queries) to measure the effectiveness of LSI based retrieval on Turkish-English parallel corpus. Our experimental results show that, LSI based CLIR outperforms the non-LSI based retrieval where their retrieval successes are %69 and %26 respectively.
5291899
Prediction of protein-protein interactions in yeast using SVMs with genomics/proteomics information and feature selection###In current Proteomics research, prediction of protein-protein interactions (PPIs) is one of the main goals, since PPIs explain most of the cellular biological processes. In the present work, we propose a method for prediction of protein-protein interactions in yeast. Our proposal is based on the well-known classification paradigm called support vector machines and a well-known feature selection method (Relief) using genomics/proteomics information. In order to obtain higher values of specificity and sensitivity in predicting PPIs, we use a high reliable set of positive and negative examples from which to extract a set of proteomic/genomic features. We also introduce a similarity measure for pairs of proteins to calculate additional features from well-known databases, that allow us to improve the prediction capability of our approach. After applying a feature selection method, we construct SVM classifiers that obtain a low error rate in the prediction for each pair of proteins. Finally, we analyse and compare the prediction quality of the method proposed with other high-confidence datasets from other works.
5291898
Stableness in large join query optimization###In relational database model, the use of exhaustive search methods in the large join query optimization is prohibitive because of the exponential increase of search space. An alternative widely discussed is the use of randomized search techniques. Several previous researches have been showed that the use of randomized sampling in query optimization permits to find, in average, near optimal plans in polynomial time. However, due to their random components, the quality of yielded plans for the same query may vary a lot, making the response time of a submitted query unpredictable. On the other hand, the use of heuristic optimization may increase stability of response time. This characteristic is essential in environments where response time must be predicted. In this paper, we will compare a randomized algorithm and a heuristic algorithm applied to large join query optimization. We used an open source DBMS as experimental framework and we compared the quality and stability of these algorithms.
4717925
TurKeyX: Turkish keyphrase extractor###Keyphrases are useful information extracted from documents. They reflect the main ideas of the text. Therefore knowing the list of keyphrases can save substantial amount of time which can be lost during searching for a document about a particular topic. Unfortunately, there are many documents which do not include a list of keyphrases. Thus automatic extraction of keyphrases becomes an important task. In this paper, a method for Turkish keyphrase extraction is explained.
4717924
From simple to large scale Matching: A hybrid approach###The current schema matching techniques do not stand up to data and schema scaling up. We present existing matching approaches at large scale called holistic and pair-wise. On the basis of the state of the art techniques we suggest the elaboration of a hybrid approach that combines these known techniques to deal with a large scale matching.
4717927
Analysis of delay factors for voice over WiMAX###This paper presents the results of our computer simulation for delays experienced by voice packets over WiMAX 802.16e protocol running over an enterprise packet network connected to public internet. In the simulation we analyzed the effect of packet size, core network link speed, Internet service provider link speed, wireless network link speed, wireless distance, base station range, and distance in internet parameters that effect the amount of delay in a WiMAX embedded IP network. In this study, all the delay parameters are fixed to their default values and current parameter values are changed so that the delay is varied between minimum and maximum values in uniformly distributed 100 steps and the delay is plotted in each step. Total fixed delays can be considered as minimum delay and variable delay corresponds to the maximum delay that can occur under the given conditions. All other fixed and variable delay parameters such as switching delays, packetization delays, contention delays and the delays caused by buffers etc. are also taken into account. The results indicate that among these delays WiMAX base station range and the number of base stations play the most decisive role.
4717926
Automatic age classification with LBP###Estimating the age exactly and then producing the younger and older images of the person is important in security systems design. In this paper local binary patterns are used to classify the age from facial images. The local binary patterns (LBP) are fundamental properties of local image texture and the occurrence histogram of these patterns is an effective texture feature for face description. In the study we classify the FERET images according to their ages with 10 years intervals. The faces are divided into small regions from which the LBP histograms are extracted and concatenated into a feature vector to be used as an efficient face descriptor. For every new face presented to the system, spatial LBP histograms are produced and used to classify the image into one of the age classes. In the classification phase, minimum distance, nearest neighbor and k-nearest neighbor classifiers are used. The experimental results have shown that system performance is 80% for age estimation.
4717921
Shot boundary detection in H.264###This study focuses on improving the available scene change detection methods developed for bit streams compressed with the latest coding standard H.264/AVC. Two algorithms that form the basis of our study are introduced and compared. Then, a method based on the combination of the two methods is proposed together with some improvements. Experimental studies show the proposed method is superior to others.
4717920
Distributed intrusion detection using mobile agents against DDoS attacks###Along with the increasing wide applications of computer and network technologies, the security problems of information systems are becoming more complicated. Most computer systems have some kind of a security flaw that may allow outsiders or legitimate users to gain unauthorized access to sensitive information. Among the network exploits, Distributed Denial of Service (DDoS) attack is a large-scale, coordinated attack on the availability of services of a victim system, launched indirectly through many compromised computers on the Internet. Intrusion detection systems (IDS) are network security tools that process local audit data or monitor network traffic to search for specific patterns or certain deviations from expected behavior which indicate malicious activities against the protected network. In our study, we proposed and tested four different distributed intrusion detection methods to detect DDoS attacks in the MIT DARPA LLDOS 1.0 dataset. Currently, all of our methods use the alarms generated by Snort, a signature-based network IDS. We used mobile agents in three of the methods on the Jade (Java Agent Development Framework) platform in order to reduce network bandwidth usage by moving data analysis computations to the location of the intrusion data. Based on reliability, network load and mean detection time values of each, one of the methods is shown to be better than the others.
4717923
Experiments of running parallel auctions in an agent-based auction service###In this paper we discuss experimental results obtained with a prototype implementation of an agent-based service for generic auctions that is currently under development. We configured the service to allow parallel execution of multiple English auctions with buyer and seller agents bidding from different machines. The experimental results to assess the performance of the service include latency and throughput of the service as functions of the number of participants and the number of simultaneously active auctions.
4717922
Smart dynamic memory allocator for embedded systems###Dynamic memory (DM) allocation is one of the most crucial components of modern software engineering. It offers a greatest flexibility to the software systemspsila design; nevertheless, developers of real-time systems often avoid using dynamic memory allocation due to its problems like unbounded or long bounded response time and memory fragmentation. However, the modern complex applications like multimedia streaming and network applications made the dynamic memory allocation mandatory for applicationspsila design. The major challenges of memory allocator are minimizing fragmentation, providing a good response time, and maintaining a good locality among the memory blocks. This paper introduces a new smart dynamic memory allocator particularly for embedded systems that have limited memory and processing power. It aimed at addressing the major challenges of dynamic memory allocator. The smart allocator predicts the short-lived objects and allocates those objects on one side of the heap memory and remaining objects on other side of the heap memory for effective utilization of memory footprint. The allocator is implemented with enhanced multilevel segregated mechanism using lookup tables and hierarchical bitmaps that ensure very good response time and reliable timing performance. The proposed algorithm has shown excellent experimental results, with respect to both response time and memory footprint usage, as compared to the well known algorithms. In addition, this paper presents a memory intensive synthetic (MIS) work load, which can model the allocation behavior of most of the real applications.
4717929
Investigating software design pattern behavior in multiprocessor systems: A case study on observer###With the emergence of multicore processors, parallel software is beginning to be used in the domain of application software in addition to high performance computing software. The use of parallel processing hardware with current software engineering techniques and principles, like object oriented programming, will surely produce new challenges in area and change the way we look at quality criterions. In this work, experiments on bringing a software design pattern to a parallel environment are explained regarding different aspects of parallelization like data consistency, scalability and workload distribution. All the experiments are performed by keeping the parallelization at object level in order to reason about discussions that can be made on object oriented software design for multicore systems. Discussions include adaptation of current software to multicore platforms, important points when designing software for multicore systems and possible research on quality criterions of parallel software. As a result of our initial studies we have seen that it is promising to conduct research on object-level parallelism in multicore systems.
4717928
An approach to improve information sharing in a large Information System###Actual information systems (IS) of big companies are very complex and sometimes difficult to control even if new techniques and new integrated software appears to give solutions. To get a complete, fresh and relevant information, close to the IS users needs, is a very difficult task, sometime impossible. We try to suggest a non-intrusive solution, facilitating this task and transparent to the users, relaying on some concepts like ldquoreferential repositoryrdquo, ldquoa priorirdquo connector and semantic descriptions. Our approach aim to improve the IS organization by combining these concepts and technical information integration solutions.
4717947
The effects of terrain types on 3D coverage under heterogeneous deployment strategies in Wireless Sensor Networks###This paper focuses on the 3D coverage problem in wireless sensor networks employing real-world terrain models. Firstly, we examine the sensing models used in WSN and propose novel deployment strategies. Then, we study the coverage behavior of these heterogeneous deployment strategies under three 3D real-world terrain models. Our purpose is to find answers to questions like which deployment strategy gives the best 3D coverage for a specified 3D terrain and how many sensors are needed to maintain coverage at a specified percentage. Furthermore, on these 3D real-world terrain models we also study the effects of sleeping on coverage. Simulation results demonstrate that for a given terrain type an appropriate deployment strategy can be selected to obtain required 3D coverage level.
4717946
An authentication protocol for hierarchy-based wireless sensor networks###Hierarchy-based wireless sensor networks (WSNs) achieve excellent performance with reserving energy consumption and decreasing system delay. However, distinguished hierarchy-based research such as LEACH (low-energy adaptive clustering hierarchy) does not consider a security issue for ensuring the protection of the network. Due to the peculiar character of LEACH such as round base operation and header centrality topology, adopting security in LEACH is a complicated task. In this paper, we present an authentication protocol for hierarchy-based WSNs to protect network from inside attackers. This protocol uses pre-distribution unique symmetric key with authentication management database (AMDB) and live message to monitor the network to detect the entrance of compromise node using illegal keys.
4717945
Semblance based disseminated software watermarking algorithm###Software piracy is a direct threat to the revenue of software vendors, requiring the need to employ effective and efficient techniques for detecting and preventing software piracy. One of the most promising attempts to protect intellectual property rights includes software watermarking. In this paper, we present a new technique for software watermarking which we call semblance based disseminated software watermarking algorithm (SDSW). Our technique embeds imitative instructions in object code using a defined dasiastring to instructionpsila mapping. These statements do not affect the overall execution semantics of program, however are hard to identify and extract for having resemblance with actual program code. This characteristic leads to the identification of the legitimate buyer, responsible for distribution of pirated copies to penalize legally. The effectiveness of proposed technique has been evaluated against published techniques implemented in Sandmark a watermarking application.
4717944
Combined effects of mobility, congestion and contention on network performance for IEEE 802.15.4 based networks###Besides being very application specific the design and performance evaluation of wireless sensor networks (WSN) is also very dependent on another inherent feature of these networks - the mobility of the sensor nodes. A major challenge is to provide reliable and energy-efficient operation taking into consideration different mobility models. On the other hand, due to the multi-hop nature of many WSN local contention can lead to network-wide congestion and reduce both the efficiency and the lifetime of the network. In this paper we study the interdependence between end-to-end congestion and local contention taking into consideration different mobility scenarios. The work is based on the slotted CSMA/CA medium access control method adopted in IEEE 802.15.4 protocol specification for LR-WPAN and covers beacon-enabled mode. Random waypoint mobility model (RWP) and ad hoc on demand (AODV) routing protocol are used. Beacon-enabled mode is chosen because it includes regulating the activity period of each single node. The aim is to investigate the effects of changing local contention regulating parameters (activity periods, number of MAC layer retransmissions and transmission buffer size) on the overall network congestion in different mobility scenarios.
4717943
Robotic wireless network connection of civilians for emergency response operations###Mobile robots equipped with wireless devices can prove very useful during emergency response operations. We envision such robots that locate trapped civilians and initiate an ad hoc network connection between them and the rescuers, so that the latter can better assess the situation and plan the rescue operation accordingly. We present a centralised formulation for the novel problem of optimally allocating robots so that they connect as many civilians as possible, while maintaining their multi-hop connection with a static wireless sink. This formulation stems from a combination of characteristics typically found in assignment and network flow optimisation problems. We have also developed a distributed heuristic with which the robots start from the location of the sink and move autonomously trying to connect the civilians while maintaining connectivity. We evaluate our distributed heuristic using a building evacuation simulator and compare it with the centralised approach.
4717942
Virtualization and allocation of network service resources using graph embedding###Recent developments in optical communications have led to the creation of large scale optical networks allowing users to run distributed applications with high QoS requirements. The virtualization and the efficient allocation of network resources and application services is one key element of these new applications management. In this paper, we study this problem from a graph embedding point of view. After defining and motivating the main problems we focus on, we give results on NP-completeness and inapproximability of these problems. Then, we propose an online heuristic approach to solve them and we evaluate its performances by simulation.
4717941
Scale and pose invariant real-time face detection and tracking###This paper extends the face detection framework proposed by Viola and Jones to handle all upright face poses between full left and full right profiles and also describes its combination with Camshift based face tracking system. The whole 180deg degree range was divided into 5 sub-ranges and their corresponding view based detectors are constructed separately. Face detection framework is a combination of real Adaboost algorithm, integral image and cascading classifiers. Faces are trained for five different poses (left, left+45deg, front, right+45deg and right) and face detectors are obtained for all poses. For tracking part of the system, Camshift algorithm is used because of its speed and easy implementation on video frames. To avoid impact of image analysispsilas computations on real-time application, multithreading methods are implemented by means of two processes. The combination of all these techniques yields good results for the face detection and tracking system.
4717940
Distributed key selection for group applications in ad hoc networks###Efficient key management is an indispensable element for supporting security in ad hoc networks. However, several key pre-distribution approaches based on a trusted third party have limitations caused by their centralized principles. Distributed key selection mechanisms are offered to deal with the limitations of the centralized counterparts in ad hoc networks. In this study, we consider distributed key selection for ad hoc networked group applications. We provide extensions to a recent key establishment approach named SeeDKS (Seed-based Distributed Key Selection). The approach is generalized by offering a novel exclusion property testing required for secure communications. Secure group management algorithms for SeeDKS are also proposed. Quantitative evaluation and superiority of the generalized algorithm over distributed key selection are presented with experimental results.
4717949
Fast Correlation Based Filter (FCBF) with a different search strategy###In this paper we describe an extension of the information theoretical FCBF (Fast Correlation Based Feature Selection) algorithm. The extension, called FCBF#, enables FCBF to select any given size of feature subset and it selects features in a different order than the FCBF. We find out that the extended FCBF algorithm results in more accurate classifiers.
4717948
Facial feature tracking and expression recognition for sign language###Expressions carry vital information in sign language. In this study, we have implemented a multi-resolution active shape model (MR-ASM) tracker, which tracks 116 facial landmarks on videos. Since the expressions involve significant amount of head rotation, we employ multiple ASM models to deal with different poses. The tracked landmark points are used to extract motion features which are used by a support vector machine (SVM) based classifier. We obtained above 90% classification accuracy in a data set containing 7 expressions.
4717970
Efficient secure storage of privacy enhanced video surveillance data in intelligent video surveillance systems###The trade-off between privacy and security was a subject of intensive discussion. Giving up privacy does not necessarily result in greater security, and greater security does not necessarily require a loss of privacy. In this work, three different novel approaches are presented to store personal data obtained from intelligent video surveillance systems in an efficient and secure way. The storage process of this data poses a number of challenges e.g. storage optimization, query performance, security management, access control and performance management. Each of the proposed technique is evaluated and analyzed with respect to the above-mentioned challenges. Field test results showed that each of the technique can be successfully applied to hide/conceal privacy related data with a maximum storage as well as retrieval performance.
4717971
Co-training with adaptive Bayesian classifier combination###In a classification problem, when there are multiple feature views and unlabeled examples, co-training can be used to train two separate classifiers, label the unlabeled data points iteratively and then combine the resulting classifiers. Especially when the number of labeled examples is small due to expense or difficulty of obtaining labels, co-training can improve classifier performance. For binary classification problems, mostly, the product rule has been used to combine classifier outputs. In this paper, we propose an adaptive Bayesian classifier combination method which selects either the Bayesian or the product combination method based on the belief values. We compare our adaptive Bayesian method with Bayesian, product and maximum classifier combination methods for the multi-class pollen image classification problem. Two different feature sets, Haralickpsilas texture features and features obtained using local linear transforms are used for co-training. Experimental results show that adaptive Bayesian combination with co-training performs better than the other three methods.
4717932
Credit risk analysis using Hidden Markov Model###This study investigates the performance of Hidden Markov Model (HMM) for credit risk analysis in terms of classification and probability of default (PD) modeling. The PD modeling assigns default bankruptcy probabilities to credit customers instead of strictly classifying them as good (solvent) and bad (insolvent) borrowers. In the first part, the classification ability of HMM is compared to that of Logistic Regression (LR) and k-Nearest Neighbors (k-NN). In the second part, the PD modeling performance of HMM is analyzed and compared to that of popular LR algorithm for PD modeling. This study aims to build appropriate algorithms to make HMM an effective way of credit risk analysis as well as conventional methods. Results of the experiments show that HMM is a powerful and robust method for credit risk analysis and can be utilized by financial institutions.
4717933
Clustering poses of motion capture data using limb centroids###With increasing popularity of using motion capture hardware for motion synthesis, studies that exploit large motion databases for faster indexing and retrieval become more important. Here, an automated procedure to analyze motion databases and cluster poses according to various feature vectors is presented. We propose the use of limb centroids as an alternative feature vector. Limb centroids have a smaller size compared to standard alternatives, thus time complexity is reduced while satisfactory clusters are achieved when used with our iterative clustering procedure. In addition, along with two different metrics to evaluate formed clusters, we also propose pose clouds for visualization and perceptual analysis.
4717930
Cognitive aspects of error finding on a simulation conceptual modeling notation###The aim of the study is to investigate and compare experimentally the error finding strategies of a notation-familiar group with degrees in computer science related fields and a domain-familiar group on a simulation conceptual modeling representation based on UML. The use of eye movement and verbal protocols together with performance data underline the differences such as error finding and reasoning between two groups. The experiment with 20 participants also reveals that the diagrammatic complexity and the degree of causal chaining are the properties of diagrams that affect understanding, reasoning and problem solving with conceptual modeling representations. In a follow-up study with 24 university students, it is seen that these properties are independent of gender. The study also emphasizes the combination of different data collection modalities, namely eye movements, verbal protocol and performance data to be effective in uncovering individual differences in human-computer interaction studies in the domain of software engineering.
4717931
Optimization of power consumption using trespassers&#x2019; favorite path and variable sensing range integrated sleep schedule in surveillance wireless sensor networks###Coverage problems related to the wireless sensor networks are widely studied during the last few years. Generally, the coverage of the whole area is taken into account in those studies, giving the entire region the same importance. On the other hand, in the real life, some of the covered regions can be more important compared to other parts. In border surveillance applications, some paths are favored by intruders and it is important to keep these areas under constant surveillance. But keeping these points under constant surveillance means more energy consumption for these sensors and shorter lifetime. In order to solve these contradicting objectives, we introduce the minimum power maximum coverage sensor (MPMC) heuristic. Using MPMC, sensors adjust their ranges and construct clusters during operation to decrease the power consumption with minimum performance loss in coverage and detection. We also present simulation results which indicate the distinctive energy-coverage balance achieved by the proposed heuristic.
4717936
Ontological video annotation and querying system for soccer games###This paper describes a video annotation and querying system which is capable of semi-automatic annotation of videos from text. The extracted metadata is aligned with the corresponding video segments. This allows users to query videos according to their semantic content. We have chosen soccer domain to demonstrate the use of the system. The soccer videos are very suitable for our framework, since it is easy to find Web-cast match reports for soccer games. The annotated videos are stored in MPEG-7 format in an object-oriented database. The keyword-based indexing allows fast retrieval of video segments. The system accepts match reports in Turkish which makes querying in Turkish possible.
4717937
Predicting Uniaxial Compressive Strengths of Brecciated Rock Specimens using neural networks and different learning models###Calculation of the uniaxial compressive strength (UCS) of Breccia rock specimens (BRS) is required for the correct determination of material strengths of marble specimens. However, this procedure is expensive and difficult since destructive laboratory tests (DLT) are needed to be done. Therefore, the results of non-destructive laboratory tests (NDLT) combined with different features that are extracted by using image processing techniques can be used instead of DLT to predict UCS of BRS. The goal of this study is to predict the results of DLT by using the results of NDLT, extracted features and artificial neural networks (ANN). Unfortunately, having enough number of specimens for training of ANN is often impossible since the preparation of the standard BRS is extraordinarily difficult. Hence, it is very important to use a learning methodology that prevents deficient evaluation practices. Therefore, different well-known learning methodologies are tested to train the ANN. Then, their effects on error estimation for our small size sample set of BRS are evaluated. The results of simulations show the importance of learning strategies for accurate evaluation of an ANN with a low error rate in prediction of UCS of BRS.
4717934
Prior information based segmentation: A 3D level set surface matching approach###This paper presents a level set based segmentation method with shape priors. The shape priors guide the level set deformations so that the contour extraction process is affected not only from the local image properties, but also from the expert knowledge in the form of manual contours. The method does not need an explicit training phase and it does not complicate the level set functional because level set deformations and incorporation of prior information are done separately. The system uses manual expert contours to produce new level set surfaces which are warped into the surface from the level set process. The prior information is incorporated into the level sets by re-initializing these warped surfaces as new level set surfaces. The resulting system is validated by running experiments on synthetic data and real MR and ultrasound cardiac images.
4717935
Metadata extraction from text in soccer domain###Event detection is a crucial part for soccer video searching and querying. The event detection could be done by video content itself or from a structured or semi structured text files gathered from sports Web sites. In this paper, we present an approach of metadata extraction from match reports for soccer domain. The UEFA Cup and UEFA Champions League Match Reports are downloaded from the web site of UEFA by a Web-crawler. Using regular expressions we annotate these match reports and then extract events from annotated match reports. Extracted events are saved in an MPEG-7 file. We present an interface that is used to query the events in the MPEG-7 match corpus. If an associated match video is available, the video portions that correspond to the found events could be played.
4717938
Focused crawler for finding professional events based on user interests###In this paper, a focused crawler is designed to gather information about conferences and meetings on a specific area. Our crawler starts visiting links from a set of related Web sites that are specified by a human expert then, it classifies pages as relevant and not relevant by using vector space model. During the crawling process, metadata information (e.g., title, date, venue, url, etc.) are extracted and stored in a relational DBMS. Through a Web-based interface, users search the database to find an event that matches their preferences.
4717939
Comparative study of cognitive complexity measures###Complexity metrics are used to predict critical information about reliability and maintainability of software systems. Cognitive complexity measure based on cognitive informatics, plays an important role in understanding the fundamental characteristics of software, therefore directly affects the understandability and maintainability of software systems. In this paper, we compared available cognitive complexity measures and evaluated cognitive weight complexity measure in terms of Weyukerpsilas properties.
4717954
A spatially controlled histogram equalization for image enhancement###Image enhancement mostly means handling with the contrast of the pixels. However, conventional histogram equalization (HE) methods works based on the global histogram information only. Hence they often make an overall enhancement of the whole image without taking any intensive care of the spatial relationship among the pixels. This often leads to a number of annoying artifacts. In this paper we propose to incorporate spatial information in histogram equalization process to apply the appropriate amount of contrast enhancement for a visually pleasing output. This method outperforms other present HE approaches by enhancing the contrast well without any severe side affects such as washed out appearance (caused by over-enhancing some of the features and under-enhancing some others), checkerboard effects etc.
4717955
Short time traffic speed prediction using data from a number of different sensor locations###In this study we predict traffic speed on Istanbul roads using RTMS (remote traffic microwave sensor) speed measurements obtained from the Istanbul Municipality Web site from 327 different sensor locations. We do speed predictions 5 minutes to an hour ahead and use SVM (support vector machine) and kNN (k nearest neighbor) methods for speed prediction. First of all, for speed prediction at a certain sensor location, we compute the most important past speed measurements for better accuracy using feature selection methods. We also find out which other sensors could be used to predict the speed at a certain sensor location and show that especially for nearby/correlated sensors, it is possible to get better results using related sensor measurements in addition to the sensor being predicted. We also show that only using the correlated sensors, it is possible to get good accuracy. This result could be very useful when a sensor breaks down or needs to be calibrated. In all our experiments, we find out that SVM produces better results than kNN.
4717956
Several facilities of class diagram generation from two-hemisphere model in the framework of MDA###In Object Management Group model driven architecture models are the primary artifacts during software development, which are presented at the different levels of abstraction from programming details. Class diagram is the most often used model for visual representation of static aspects of software system, but still the formal generation of class diagram from problem domain is under investigation. The paper describes principles of development of class diagram at the conceptual level by using problem domain knowledge presented in the form of two-hemisphere model. Problem domain is presented as two interrelated models of the most important aspects of a system, namely, business process and concept models. Abilities of class diagram generation from two-hemisphere model are presented as a collection of graph transformations and are demonstrated by practical example for abstract problem domain.
4717957
Impact of peer-to-peer communication on real-time performance of file replication algorithms###A variety of file replication algorithms are proposed for Data Grids in the literature. Most of these algorithms ignore real-time Grid applications which are emerging in many disciplines of science and engineering today. Thus, it is important to study and improve the real-time performance of these file replication algorithms. Based on this motivation, in this study, performance of four replication algorithms, two from the literature and two new algorithms, are evaluated. For this evaluation, a process oriented and discrete-event driven simulator is developed. A detailed set of simulation studies are conducted using the simulator and the results obtained are presented to elaborate on the real-time performance of these replication algorithms.
4717950
Extracting vehicle density from background estimation using Kalman filter###This study aims to extract traffic density through traffic monitoring camera images. To this end, it uses Kalman filter based background estimation, which can efficiently adapt to environmental factors such as light change. The difference between the incoming image and the calculated background was subjected to the proposed filters and the vehicles in the foreground were marked. The binary image representing the background and the foreground was subjected to geometric correction to equalize the effects of the vehicles near and distant to the camera, and then, the road ratio of the vehicles was computed by proportioning the foreground to the entire road area. All these procedures were applied to the road areas manually marked beforehand. The developed method was experimented on four different points recorded by traffic surveillance cameras operated by the Traffic Control Office of Istanbul Metropolitan Municipality.
4717951
Fast non-linear video synopsis###This paper presents a very efficient method for extracting the non-linear summary of a video sequence by synthesizing new summary frames from a number of original frames. The non-linear summary of a video sequence is fundamentally different from the classical video summarizing techniques which discard full frames. The high efficiency of the method is due to the employment of dynamic programming to find the space-time surfaces in the video volume that have lesser motion information. We project the video volume onto a plane orthogonal to one of its axes and find a minimum cost path on the time derivative of the projected image. Back-projecting the minimum cost path down to the video volume gives the space time surface to be discarded. Applying this process several times results in the summarized video. One of the major contributions of our method is that it can work on real-time video sequences with a small memory requirement. We demonstrated our method on several examples which are available at http://vision.gyte.edu.tr/projects. php?id=5.
4717952
Modeling of inhomogeneous intensityd istribution of X-ray source in radiographic images###X-ray bone images are used in the areas such as bone age assessment, bone mass assessment and examination of bone fractures. The computer aided analysis leads to operator independent, subjective and fast results.
4717953
Considering spatio-temporal outliers in function approximation###When trying to fit a population into a model (e.g. classification or regression) outliers tend to decrease the performance of the model. Thus, in most of the research outliers are detected and removed from the population to overcome this decrease. This common judgment about outliers may increase the modeling accuracy of inliers; however knowledge about outliers is discarded by this way. If outliers occur from natural phenomena (i.e. when outliers are not noise) their identification may accomplish even more important knowledge than the inliers. In other words, outliers should also be considered as a member of the population while trying to fit the members of a population into a model. This paper attacks to this idea in spatio-temporal domain. Firstly, spatio-temporal outliers are identified, secondly this information is used explicitly while fitting the population into a regression model via support vector regression to investigate whether there will be increase in the prediction accuracy of the model or not. Experiments are carried out through a transactional data set on forest fires. It is shown that regression performance increases by considering spatio-temporal outliers.
4717958
On-demand server synchronization algorithms for session guarantees###Session guarantees define required properties of the system regarding consistency of replicas in a distributed system from a single, migrating clientpsilas point of view. This paper presents propositions of two new server synchronization algorithms. These algorithms solve two of the main problems of some earlier protocols: lack of serverpsilas history pruning and synchronous history update sending.
4717959
Priority disciplines - a diffusion approach###The article presents a diffusion approximation model applied to investigate the behaviour of priority queues. Diffusion approximation allows us to include in queueing models fairly general assumptions. First of all it gives us a tool to consider in a natural way transient states of queues, which is vary rare in classical queueing models. Then we may consider input streams with general interarrival time distributions and servers with general service time distributions. Single server models may be easily incorporated into the network of queues. Here, we apply the diffusion approximation formalism to study transient and steady-state behavior of G/G/1/N priority preemptive models. The models can be easily converted to nonpreemptive queueing discipline. Also the introduction of self-similar traffic is possible. The models may be useful in performance evaluation of mechanisms to differentiate the quality of service e.g. in WiMAX, metro networks, etc.
4717961
Contour compression using trapezoid method###An efficient algorithm for the contour data approximation and compression is presented and discussed in this paper. The contour inputs are represented in Cartesian coordinates. The contour input is processed and some of the vertices are chosen as the vertices of the contour polygon. We also discuss the main idea of the analysed approach and we will compare it with the Ramer and Triangle methods of contour approximation. The single step parallel contour extraction (SSPCE) method will be used in this work. For comparison, we are going to use the mean square error and signal-to-noise ratio criterions. Computational time of analysed method is estimated depending on a number of numerical operations. This paper shows that the simplicity and the small number of operations are the main advantages of the proposed algorithm.
4717960
Space efficient caching of query results in search engines###Web search engines serve millions of query requests per day. Caching query results is one of the most crucial mechanisms to cope with such a demanding load. In this paper, we propose an efficient storage model to cache document identifiers of query results. Essentially, we first cluster queries that have common result documents. Next, for each cluster, we attempt to store those common document identifiers in a more compact manner. Experimental results reveal that the proposed storage model achieves space reduction of up to 4%. The proposed model is envisioned to improve the cache hit rate and system throughput as it allows storing more query results within a particular cache space, in return to a negligible increase in the cost of preparing the final query result page.
4717963
Availability analysis and Connection provisioning in overlapping shared segment protection for optical networks###In this paper, we propose an availability analysis method for overlapping shared segment protection. We validate the proposed analysis method for different traffic demand matrices and different failure rate values. We show that the proposed analysis method achieves estimating the availability of a connection with a negligible error ratio. We then present two connection provisioning schemes, namely availability constrained generalized segment protection (AC-GSP), and share ability driven availability constrained generalized segment protection (SDAC-GSP) that are availability-aware adaptation of a conventional segment selection algorithm, namely the generalized segment protection (GSP). We evaluate the performance of these proposed schemes, and we show that as an enhancement to GSP, SDAC-GSP enhances the performance of AC-GSP in terms of the availability per connection, blocking probability and availability satisfaction ratio. However, it utilizes more wavelengths in comparison to AC-GSP since the more segments protects the links of the working path, the higher availability.
4717962
Specification and verification of agent interactions in matchmaking processes using FSP and FLTL###In this note we show how our framework for formal specification and verification using FSP process algebra and FLTL temporal logic can be practically applied to check properties of agent systems that contain matchmakers.
4717965
Goal oriented edge detection###In many vision applications, there is a great demand for an edge which can produce edge maps with very different characteristics in nature, so that one of these edge maps may meet the requirements of the problem under consideration. Unfortunately it is not evident how to choose the desired or the optimum edge maps from these solutions that the edge detector offers. The proposed solutions are usually too general that cannot be easily adapted to the application needs by tuning edge detection parameters. One edge detector that we have studied in this study is Generalized Edge Detector which is capable of producing edges with very different characteristics. Although the edge maps based on this representation are reasonable, no one set of scale parameters alone yields a solution close to the desired edges. In this study, we have developed powerful edge operates and have used them under a goal-based edge detection framework. Proposed framework is a two-stage process. First, user marks some pixels in the database as edge and non-edge pixels. Then feature vectors comprised of filter responses to G-Filters at different scales are extracted at these marked pixels. Edge detection problem is imposed as two-class classification problem. Classifier itself is not adequate to extract desired edges for the application under consideration. In the second stage continuous edges are treated as one contour. Then contours are matched with the contours in the training set. Only matched contours are kept and the other contours are eliminated. The purpose of the first stage is to keep only prominent edges and remove irrelevant edges with respect to the application. The classifier decides which discontinuity is prominent or irrelevant. Experimental studies on real license plate images show that the proposed edge detector can successfully detects edges only on license plate regions.
4717964
Segmentation of S1&#x2013;S2 sounds in phonocardiogram records using wavelet energies###Locating S1 and S2 sounds in order to diagnose diseases by classifying through the determination of systole and diastole phases is of great significance. The method suggested in this study is available for the segmentation of S1-S2 sounds in heart sounds (HSs) acquired in real-time and it also renders the classifying possible, thereby ensuring a correct diagnosis. In this study, multi-band wavelet energy (WTE) method was proposed for the segmentation of S1-S2 sounds in 16 different HS types. After normalizing the heart sound, the signal is filtered by using wavelet transform, after than S1-S2 sounds are segmented by using multi-band wavelet energy method. For comparison, besides the method which is proposed, two different methods are investigated. One of them is segmentation of S1-S2 sounds using multi-band wavelet Shannon energy (WSE) method and the other is segmentation of S1-S2 sounds using homomorphic filtering (HMF) method. The highest performances are achieved by the proposed WTE method; 91% and 89% segmentation accuracies are obtained for SI and S2 sounds, respectively. The methods' robustness to noise was also analysed.
4717967
Gene ontology prediction using compression based distances and alignment scores on both amino acid sequence and secondary structure###Normalized compression distance (NCD) is a compression based pairwise distance measure. NCD has been shown to perform well in different domains, such as music, biological sequence and text classification. In this study, we use NCD distance together with Smith-Waterman (SW) alignment scores of protein sequences for gene ontology prediction. We find out that, using secondary structure in addition to the amino acid sequence increases the prediction performance when using NCD or SW alignment scores alone. The best contribution ratio of secondary structure for SW alignment scores is 0.25, while it is 0.50 for NCD scores. We also investigate using both NCD and SW together with the amino acid and secondary structure. We find out that this combination results in better prediction than NCD alone, but worse prediction than SW alone.
4717966
Connection provisioning constrained to fault localization in all-optical networks###Fault localization is becoming a critical issue in all-optical networks. The Limited Perimeter Vector Matching (LVM) protocol is a novel fault localization protocol for localizing single-link failures in all-optical networks. In this paper, we study the fault localization optimization problem by applying the LVM protocol where the traffic demands (or lightpath requests) are known a priori. Given the traffic demands, the fault localization optimization problem aims to optimize the traffic distribution so that the fault localization probability in terms of the number of localized links can be maximized. Moreover, the solution to the problem can also provide the maximum number of wavelengths needed on each link to obtain the maximum fault localization probability.
4717969
Ad-hoc connections of miscellaneous sensors in a CCTV system###In this paper the need of sensors other than analog and digital cameras in a CCTV environment are discussed as well as the advantages these security relevant sensors can offer. After evaluating various use cases the currently available technologies in this area will be analyzed including Bluetooth, Zigbee and Wireless USB. The main focus will be placed on the security aspects, the behavior in case of failure and power management. On the basis of the evaluation a network topology will be proposed which is resilient to failure and reorganizing with the installation of new sensors.
4717968
Image frame fusion using 3D anisotropic diffusion###In this paper, a modified 3D anisotropic diffusion method is proposed to improve the multi-frame image fusion performance. Multi frame image sequence is considered to be composed of aligned and warped images. The goal of this approach is to obtain a restored image from the aligned and warped image sequence, where alignment error and Gaussian noise are reduced. The proposed method consists of medium band stack filter and tree-structured 3D diffusion filter.
4717899
Stroke matching for off-line signature verification based on bounding rectangles###One of the key tasks in computer vision is the identification and matching of coherent shapes in different images. In signature verification these shapes are represented by the strokes of the signature. Strokes may fall apart, close up, change or simply disappear among different signatures of the same signer. This paper proposes an algorithm to tackle the stroke matching problem using an area based method for finding and linking corresponding strokes and dynamic time warping to perform intra stroke matching. Using the above method the two dimensional image comparison problem can be reduced to a one dimensional case. The method was tested on the corpus of the 2004 Signature Verification Competition; experimental results are presented and evaluated. A generalized form of the algorithm could well be applied on other areas of computer vision.
4717898
3D facial expression recognition with geometrically localized facial features###This paper describes a pose invariant three-dimensional (3D) facial expression recognition method using distance vectors retrieved from 3D distributions of facial feature points to classify universal facial expressions. Probabilistic Neural Network architecture is employed as a classifier to recognize the facial expressions from a distance vector obtained from 3D facial feature locations. Facial expressions such as anger, sadness, surprise, joy, disgust, fear and neutral are successfully recognized with an average recognition rate of 87.8%.
4717895
Generalized ID-based blind signatures from bilinear pairings###Blind signature schemes provide the feature that a user is able to get a signature without giving the actual message to the signer. Recently a number of ID-based blind signatures have been proposed. In this paper, we introduce the concept of generalized ID-based blind signatures based on ElGamal signature variants. We obtain several new ID-based blind signatures from this generalized scheme which have not been explored before and some of them turn out to be more efficient than previously proposed schemes.
4717894
Energy Aware Dynamic Server Selection and Task Allocation###Battery energy limitation is one of the main challenges in the mobile ad hoc networks. Several hardware and software based techniques have been proposed in this field. Energy aware task scheduling is one of the software methods where the scheduling policy aims at optimizing the energy. Most of the previous work have achieved significant energy savings for individual mobile nodes but did not consider overall network lifetime and scalability. In this paper, we propose an Energy Aware Dynamic Server Selection and Task Allocation technique (EADSSTA) for prolonging the network lifetime. In our model, the network lifetime is divided into many rounds. At the beginning of each round new servers are located. Based on the proposed technique, selected servers are used in the remote allocation such that minimum residual energy is maximized at the end of each round. The results of our simulation showed that our proposed scheme makes a significant improvement in the network lifetime while simultaneously minimizing the energy consumption and time delay for each task.
4717897
Person name extraction from Turkish financial news text using local grammar-based approach###Local grammar approach relies on constructing polylexical units having frozen characteristics. It has recently been shown to be superior to other named entity extraction approaches including the probabilistic, the symbolic, and the hybrid approach in terms of being able to work with untagged corpora and has successfully been applied to English, Portuguese, Korean, French and Chinese texts. In this paper, we evaluated local grammar-based approach on Turkish financial texts. We have found that although the method is successful in finding person names, the construction of frozen expressions for person name extraction is rather difficult, which can be attributed to that of Turkish word formations.
4717896
A control plane for prioritized real-time communications in wireless token ring networks###Providing real-time guarantees in wireless networks is a challenging research problem. Token ring networks are more suitable for real-time communications due to the fact that the response time is highly deterministic and also the upper bound of the latency in these networks is predictable. This paper proposes a centralized control plane incorporating the timed token protocol in the MAC layer for providing hard real-time guarantees in wireless token ring networks which implements three important functions, namely the admission control procedure, the station eviction procedure and a traffic differentiation mechanism. In this approach a dynamic ring structure is built, where high priority stations have more chance of admittance and stations with low priority can be removed from the ring. Simulation results show that the proposed control plane ensures higher priority traffic more bandwidth than lower priority traffic and guarantees that deadline constraints of hard real-time traffic are satisfied.
4717891
PIRS: An information retrieval system based on the vector space model###In this work we designed an information retrieval system ldquoPIRSrdquo - precision information retrieval system - based on the modified vector space model introducing a new query weighting formula and similarity function. These modifications of the classical vector space model aimed to improve the average precision level of the system. Two well known IR parameters, precision and recall were used to compute the performance of system.
4717890
Integrating agents into data-centric naval combat management systems###The number of systems and software developed by using agent oriented technologies has increased dramatically within the last decade. In parallel with this increase, lots of agent based tools, techniques, methods and platforms have been proposed. Some of the agent platforms were designed for generic purposes, while the others were tailored for specific needs. Although there are many existing platforms, we still need novel agent platforms supporting the new promising application domains. Naval combat management systems is one of these domains due to its complex and dynamic nature. In the Information Age, intelligent systems become an inevitable necessity for combat management systems while the network-centric warfare supersedes the conventional platform-centric concepts. In this paper, we will discuss how agent technology can be integrated with data-centric naval combat management systems. The proposed solutions are based on the Data Distribution Service (DDS) infrastructure. DDS is an Object Management Group (OMG) middleware standard using the publish-subscribe paradigm and quality of service parameters for data-centric asynchronous communication.
4717893
A new steganography method using image layers###In this study we propose a new steganography method based image layers. The proposed method divides the host image into blocks and embeds the corresponding secret data bits into each block using the layers generated by the binary representation of pixel values. Given a secret bit sequence, it performs a search on the rows and columns of the layers for finding the most similar row or column. The location of row/column and its differences from the secret data is then marked by modifying minimum number of bits in the least significant bits of the blocks. In the experiments, randomly selected secret messages are embedded into 100 different images. The performance of the proposed method against steganalysis techniques is evaluated using the measure of distorted pixels. The method outperforms the other steganography techniques in terms of image quality.
4717892
Next generation filtering: Offline filtering enhanced proxy architecture for web content filtering###Most of available web filters especially parental controls work inline meaning that all outgoing and incoming packets are passed through a filter driver. This approach widely used in parental control applications because they mostly use blacklist, whitelist approach and defense of the applications to bypass the filter easily. Online content filtering along with its own benefits has a big flaw; filtering process affects data transfer throughput. Also this method suffers from being poor to analyze HTTP/HTTPS traffic like html reconstruction, HTML encoding, object analysis, text classifying, image analysis, analyzing HTTPS. Online content filtering is not effective if network traffic is encrypted and cannot be decrypted by the online content filter or more generally, if the traffic is in a form that the online content filter cannot interpret correctly. One of the other disadvantages identified in this work was: packet filter doesnpsilat analyze the contents of a packet; it decides whether to pass it based on the packets addressing information. Accelerating content filtering by only examining Web content is major method used. This architecture proposed in this work use offline filtering and proxy as a synergistic approach. By this way filtering process and data transfer acts independently. This work presents a simple, but effective framework to accelerate the filtering process by examining only part of the Web content. The framework can make the filtering decision, either to block or to pass the Web content. The proposed framework can be used as a gateway, as an agent based content filter or a parental control framework..
4717877
A stability approach to improve MANET-internet connection###Nowadays, there is an important effort in the research community to connect mobile devices in ad hoc networks with the Internet. In order to allow the connectivity to the Internet, a gateway that supports the wired single-hop and wireless multi-hop approaches is necessary. The wireless node, as laptops or personal digital assistants, must select the best gateway from the information that these elements send in the modified router advertisement (MRA) messages. The frequency of MRA messages generation by the Internet gateway clearly affects the network performance. In this paper, we propose an adaptive Gateway discovery scheme that dynamically adjusts the frequency of MRA messages and improves the conventional proactive schemes. The proposed scheme makes use of the mobility and connectivity properties of the network to tune the control system that sends the MRA messages.
4717876
A novel approach about cohesion measurement for classes###Cohesion refers to the degree of the relationships among the members in a class. A class is cohesive when its members are highly correlated. Several metrics have been proposed in the literature in order to capture class cohesion in terms of connections among members. They generally count the number of attributes used by methods or the number of methods pairs that share attributes. They constitute a restrictive way for capturing the cohesion. Because they do not consider some characteristics of classes like that special methods, disjoint interaction patterns and connectivity among class members. In this study, a new criterion, which focuses on interactions and groups between class members with considering density of connections among members and incorporates the special methods to cohesion capturing process, is presented, and a new notion about determination of class cohesion is proposed.
4717875
Traffic sign recognition using Scale Invariant Feature Transform and color classification###In this paper, we propose a traffic sign detection and recognition technique by augmenting the scale invariant feature transform (SIFT) with new features related to the color of local regions. SIFT finds local invariant features in a given image and matches these features to the features of images that exist in the training set. Recognition is performed by finding out the training image that gives the maximum number of matches. In this study, performance of SIFT in traffic sign detection and recognition issue is investigated. Afterwards, new features which increase the performance are added. Those are color inspection by using proposed color classification method and inspecting the orientations of SIFT features. These features check the accuracy of matches which are found by SIFT. Color classification method finds out true colors of the pixels by applying some classification rules. It is observed that adding color and orientation inspections raises the recognition performance of SIFT significantly. Obtained results are very good and satisfying even for the images containing traffic signs which are rotated, have undergone affine transformations, have been damaged, occluded, overshadowed, had alteration in color, pictured in different weather conditions and different illumination conditions.
4717874
Feature selection for multi-SVM classifiers in facial expression classification###Facial expressions are non-verbal signs that play an important role between interpersonal communications and also they are the most effective way to describe human emotions. The correct and fast extraction/recognition of the facial expressions is an ongoing research area for computer vision. In this study, the effects of feature selection on the classification of seven different facial expressions (anger, disgust, fear, joy, neutral, sadness, and surprise) are analyzed. To this end, the features for each expression were extracted using Gabor filters from the facial images and selected the best ones using two different feature categories. In the first category, the features that they do exist in one class but do not exist in all other classes have been selected. In the second category, the features that they represent the one class have been selected. These selected features were employed for expression classification using support vector machines (SVMs). Comparative results are given in terms of types for selecting features, feature selection algorithms and the approaches used for the multi-classification. The best results were obtained using RFE algorithm with first type of feature selection and using one-vs-rest approach for training. It was also demonstrated that the feature selection has a positive effect to increase the classification performance comparing when the results were observed with no feature selection.
4717873
Palmprint recognition using kernel PCA of Gabor features###This paper presents a new method for automatic palmprint recognition based on kernel PCA method by integrating the Gabor wavelet representation of palm images. Gabor wavelets are first applied to derive desirable palmprint features. The Gabor transformed palm images exhibit strong characteristics of spatial locality, scale, and orientation selectivity. These images can produce salient features that are most suitable for palmprint recognition. The kernel PCA method then nonlinearly maps the Gabor-wavelet image into a high-dimensional feature space. The proposed algorithm has been successfully tested on two different public data sets from the PolyU palmprint databases for which the samples were collected in two different sessions.
4717872
Combining adaptive tests###Adaptive test cases are often used when a specification permits several possible correct outputs from an implementation under test for a given input. In this paper, we propose an algorithm to apply such adaptive test cases which is optimal in terms of the number of inputs and is more efficient than previously published algorithm for executing the tests. Our solution comes at a cost, a pre-processing step that must be executed once. The solution presented here is particularly interesting in situations where the same set of adaptive test cases will be applied a large number of times.
4717871
Evolving aggregation behavior for robot swarms: A cost analysis for distinct fitness functions###Evolving behaviors for swarm robotic systems offers interesting emerged strategies which may be complex and unpredictable by an explicit behavioral controller design. However, even in the evolutionary case, there are critical choices regarding the design of the evolutionary algorithm that a roboticist should take into account to achieve desired goal with a reasonable efficiency. Among these design choices, adopting an appropriate fitness function is a crucial task, since it directly affects the resulting evolved strategy of a robot group. For the evolution of a single goal, different fitness functions can be used and their efficiencies can be compared. In this study, we chose complete aggregation as the desired goal for a robot swarm and compared the performances and costs of two distinct fitness functions in a simulated environment. Whilst the performance analysis consists of testing the average success rates, the cost analysis measures average time and distance taken by robots up to the successful formation. The results showed that for small communication ranges there is a trade-off between performance and cost in the fitness function selection; and hybrid control models can be utilized to overcome this issue to some extent.
4717870
Pattern recognition in cognitive communication###A cognitive communication method based on recognition of continuous signal patterns respecting to digital data streams is proposed in this study. Digital signals to be transferred are first converted to optimally chosen patterns from a pre-defined glossary with respect to bandwidth requirement and medium conditions. Receivers recognize the patterns from the chosen frequency band of the spectrum via neural network. The received patterns are matched to respecting data sequence within the common glossary to recover the information. The purpose of this study is increasing the spectrum efficiency via manageable Signal to Noise Ratio. Pattern based real-time encoding and neural network based decoding technique is used here. It is shown that the transmitted information by multiple sources can be decoded by the receivers in the same time interval, frequency band and location by using this method.
4717972
Multi sensor technologies augmenting video surveillance: Security and data fusion aspects###Based on the success of intelligent video surveillance systems, especially in the field of security, scalability and performance, the corresponding architecture and concept is extended to involve different types of sensors in addition to visual sensors. In this work an analytical study of the whole spectrum of sensor technologies and their characteristics and performance recommendation is presented. Subsequently the multi-sensor approach is discussed with the goal of augmentation of intelligent video surveillance systems. Utilizing multi-sensor technologies will require massive data fusion and bring new challenges not only in fusion but also security.
4717879
Iris recognition system using combined histogram statistics###This paper proposes a novel iris recognition system based on matching colour pixel statistics. The proposed system uses colour histograms as pixel statistic feature vectors for recognition of irises by cross correlation between the histogram of a given iris and the histograms of irises in the database. Majority voting (MV) has been applied to achieve the final recognition. Iris images taken from the UPOL are pre-processed and segmented. The proposed system gives a100% recognition rate on 64 iris images. The reliability of the proposed system is demonstrated by providing FRR and FAR curves with 0.00% EER.
4717878
Image equalization based on singular value decomposition###In this paper, a novel image equalization technique which is based on singular value decomposition (SVD) is proposed. The singular value matrix represents the intensity information of the given image and any change on the singular values change the intensity of the input image. The proposed technique converts the image into the SVD domain and after normalizing the singular value matrix it reconstructs the image in the spatial domain by using the updated singular value matrix. The technique is called the singular value equalization (SVE) and compared with the standard grayscale histogram equalization (GHE) method. The visual and quantitative results suggest that the proposed SVE method clearly outperforms the GHE method.
4717888
Time synchronization algorithms based on Timing-sync Protocol in Wireless Sensor Networks###Wireless sensor networks (WSN) are large scale networks of sensors running on wireless environment. For an application running on a WSN, gathered data by the sensors are time critical in most of the cases. However, almost all the nodes suffer from a problem named clock drift. This problem causes clock difference among nodes as time goes because the processors do not run exactly at the same speed. There are many proposed solutions to remedy this problem. TPSN (timing-sync protocol for sensor networks) is one of the effective protocols proposed to synchronize sensor networks. In this paper, we propose enhancements over TPSN to synchronize nodes in a wireless sensor network more effectively with a lower message complexity and higher precision.
4717889
Efficient integer programming formulations for optimum sink location and routing in wireless sensor networks###In this work we consider routing and sink location problems in sensor networks and propose two new mixed integer programming formulations to determine optimal sink locations and data flow routes. The models basically differ in the formulations of their objective functions. We assume that the sensor field consists of a finite set of points, and sensors, which are located on a subset, cover the points of the field completely. Experimental results indicate that these new formulations are very efficient and optimal solutions can be computed easily even for large networks. We also propose Lagrangean relaxation methods to solve the formulations approximately.
4717882
Image annotation by semi-supervised clustering constrained by SIFT orientation information###Methods developed for image annotation usually make use of region clustering algorithms. Visual codebooks are generated from the region clusters of low level features. These codebooks are then, matched with the words of the text document related to the image, in various ways. In this paper, we supervise the clustering process by using the orientation information assigned to each interest point of Scale-invariant feature transform (SIFT) features to generate a visual codebook. The orientation information provides a set of constraints in a semi-supervised k-means region clustering algorithm. Consequently, in clustering of regions not only SIFT features are normalized along the dominant orientation, but also orientation information itself is used. Experimental results show that image annotation with added orientation information by semi-supervised clustering is more successful compared to the one that uses SIFT features alone. The proposed algorithm is implemented in a parallel computation environment.
4717883
HANOLISTIC: A Hierarchical automatic Image Annotation System Using Holistic Approach###Automatic image annotation is the process of assigning keywords to digital images depending on the content information. In one sense, it is a mapping from the visual content information to the semantic context information. In this study, we propose a novel approach for automatic image annotation problem, where the annotation is formulated as a multivariate mapping from a set of independent descriptor spaces, representing a whole image, to a set of words, representing class labels. For this purpose, a hierarchical annotation architecture, named as HANOLISTIC (Hierarchical Image Annotation System Using Holistic Approach), is defined with two layers. The first layer, called level-0 consists of annotators each of which is fed by a set of distinct descriptor, extracted from the whole image. This enables us to represent the image at each annotator by a different visual property of a descriptor. Since, we use the whole image, the problematic segmentation process is avoided. Training of each annotator is accomplished by a supervised learning paradigm, where each word is represented by a class label. Note that, this approach is slightly different then the classical training approaches, where each data has a unique label. In the proposed system, since each image has one or more annotating words, we assume that an image belongs to more than one class. The output of the level-0 annotators indicate the membership values of the words in the vocabulary, to belong an image. These membership values from each annotator is, then, aggregated at the second layer to obtain meta-level annotator. Finally, a set of words from the vocabulary is selected based on the ranking of the output of meta-level. The hierarchical annotation system proposed in this study outperforms state of the art annotation systems based on segmental and holistic approaches.
4717880
Optimization of felf-organized flocking of a robot swarm via evolutionary strategies###This work presents the evolutionary optimization of a self-organized flocking controller for a swarm of mobile robots. The robotic platform utilized is Kobot, a robot developed specifically for swarm robotic studies. In previous work, a successful self-organized flocking algorithm was proposed, which depends on two basic behaviors of heading alignment and proximal control against nearby robots and obstacles. The controller is subject to many parameters, which include the relative weights of the two behaviors, and the maximum speed, the proportionality constant, and the maximum angular velocity of an individual robot. The possible dependence of the parameters on each other render a grid optimization infeasible. In this work, we present the partial results of an optimization of these parameters via evolutionary strategies by using a set of explicit metrics.
4717881
The cost-constrained web replica placement design in computer networks - an approximate approach###Paper deals with the resource replication problem which often must be solved in wide area network design process. It is important especially for big wide area networks, in which huge amount of data is exchanged between users and servers. Then replication of resources (for example servers) may prevent decreasing the quality of service in the network. Connecting replica to a node of the network is connected with some connecting cost. In practice, the connecting costs are often limited. Then, in the paper, an approximate algorithm for simultaneous serverpsilas replication, capacity and flow assignment problem with limited cost of connecting replica to the nodes of WAN is presented. Important and useful part of the paper are the results of computational experiments performed with the proposed algorithm. Important properties of the considered problem have been described. They may be helpful for planning and optimizing of WAN. Analysis of quality of the approximate solutions is also reported in the paper.
4717886
Identification of coreferential chains in video texts for semantic annotation of news videos###People can benefit from todaypsilas video archives of huge sizes only through appropriate and effective ways of querying the video data. In order to query the video data through high-level semantic entities such as objects, events, and relations, these entities should be properly extracted and the corresponding video shots should be annotated accordingly. Video texts, which comprise the caption texts on the frames as well as transcription texts obtained through automatic speech recognition techniques, are valuable sources of information for semantic modeling of the videos. In this paper, we present an approach for the extraction of semantic objects from videos by utilizing lexical resources along with the identification of coreference chains in the corresponding video texts. Coreference is a phenomenon in natural language texts where a number of entities in the text refer to the same real world entity. Therefore, while the domain-specific lexical resources aid in the determination of salient entities in the video text, the identification of coreference chains prevents the superfluous extraction of the same underlying entities due to their different surface forms in the video texts. The proposed approach is significant for its being the first attempt to address the importance of coreference phenomenon in video texts for precise entity extraction during the semantic modeling of news videos with a hands-on application. The approach has been evaluated on Turkish political news texts from the METU Turkish corpus and a number of evaluation problems faced such as sparseness of annotated evaluation data for Turkish are also pointed out together with further research directions to pursue.
4717887
A heuristic approach to military transition problem###Dynamic path planning problem, which is defined by finding the shortest distance movement from a starting point to a target destination in an environment that contains dynamic moveable obstacles, is a problem that is encountered in different forms in many fields. In military field, the safest transition of military units in shortest time is one of these forms. Transition problem is a very general problem. It can be used for vehicle routing in traffic, in military applications, in robotics, for determining the route between two points in urban transportation or for routing data packets in a network.
4717884
A simple and improvable method for face region extraction###A new face region extraction method is proposed in this study. The leading property of this method is its application simplicity. Performance success of 65% is achieved with this method and in order to improve this method for images which include multi-faces, we aim to incorporate Zernike moments as a future work.
4717885
PCA-based face recognition from video using super-resolution###In this paper, we propose a method to recognize faces from a set of consecutive video frames instead of a single image using super-resolution (SR). The SR process uses multiple frames acquired from video and combines information coming from them into a single image in higher resolution. As expected, a single low resolution image would contain less amount of information, than the same image taken from a video sequence with multiple other images with temporal changes from consecutive frames. the proposed method uses SR to generate a super resolved video sequences from a low resolution video sequences and uses frames acquired from the high resolution video sequences to train and test the performance of the principal component analysis based face recognition system. Entropy and MSE were used to check the performance of the system and the results showed the robustness of SR as a preprocessing step for recognition.
4717910
Discrete evolutionary transform based robust image watermarking###Watermarking techniques are used as a solution to copyright protection of digital media files. In this work, a new and robust watermarking method that is based on spatio-frequency (SF) representations is presented. We use the discrete evolutionary transform calculated by the Gabor expansion to represent an image in the SF domain. A watermark is embedded onto selected cells in the joint SF domain. Hence by combining the advantages of spatial and spectral domain watermarking methods, a robust and perceptual method is presented.
4717911
An analytical model of inter-node communication towards performance prediction of multi-node systems###Our goal is to predict the performance of multi-node systems consisting of identical processing nodes based on single node profiles. We clarified how much inter-node communication is caused on n-node systems because the performance significantly depends on the communication amount. However, the amount when business applications such as databases are running is difficult to predict for the following reasons: first, applications are closed and/or complex. Second, workloads vary from system to system. Third, the number of nodes affects the amount. We analytically model the amount under a simple cache coherence mechanism. Our model predicted, with a high degree of accuracy, the number of transfers of cached copies obtained by simulation. Even though our model explains the transfers only under a simple mechanism, the model should explain a fundamental part of those under more complex ones.
4717864
SIFT based geometric distortion correction method###It is very important to cope with geometric distortions especially for robust watermarking systems since these attacks break the synchronization between the watermark and detector. In general, robustness to geometric attacks is considered as a different concept as rotation, scale and translation (RST) invariant watermarking. However, it is inseparable part of a copyright protection system. There are different approaches to overcome this problem. Most of them are good in individual rotation, scaling and translation attacks but it is not the case for combination of them. In this paper, we propose and discuss the performance of a scale invariant feature transform (SIFT) based geometric distortion correction method against rotation, scaling, translation, cropping, flipping and so on. As it restores the approximate original, it can be used in watermarking as a synchronization step.
4717865
Quality-of-service-aware multicast routing in heterogeneous networks with ad hoc extensions###A growing number of users communicate on the move with each other utilising wireless network technologies. The heterogeneity level of networks is increasing with various wired and wireless parts as well as access technologies. Mobile ad hoc communications can fill the connectivity gaps in such networks. However, the increasing amount of multimedia content shared over the wireless medium makes quality of service (QoS) and resource efficiency essential requirements for mobile ad hoc networks. The growing number of group-oriented applications also requires efficient utilisation of network resources. The mesh-evolving ad hoc QoS multicast (MAQM) routing protocol proposes a solution to these problems. It achieves multicast efficiency by tracking resource availability in each nodepsilas neighbourhood and monitoring the QoS status continuously. Nodes decide on joining multicast sessions based on the sustainability of QoS. MAQM also evolves the initial multicast tree into a mesh to improve robustness. This article describes the modules of MAQM and presents its performance evaluation with regard to session-and packet-level QoS criteria. The results show that MAQM significantly improves multicast session efficiency.
4717866
A new approach for Threat Evaluation and Weapon Assignment problem, hybrid learning with multi-agent coordination###The use of intelligent agents is one of the popular topics in defense industry. Agent usage would be beneficial for defense industry especially in decision making phase of the domain procedures. In Threat Evaluation Weapon Assignment System (TEWAS), we tried to develop learning agents working in coordination for the decision process of command and control systems. This paper describes all details of TEWAS Project in the scope of machine learning techniques.
4717908
Towards a new summarization approach for search engine results: An application for Turkish###With the drastic increase of available information sources on the Internet, people with different backgrounds share the same problem: locating useful information for their actual needs. Search engines make this task easier only in certain ways; people still have to do the sifting process by themselves. At this point, automatic summarization can complement the task of search engines. In this paper, we consider a new summarization approach for Web information retrieval; i.e. structure-preserving and query-biased summarization. We evaluate this approach on Turkish Web documents using TREC-like topics defined for Turkish. The results of the task-based evaluation show that this approach has significant improvement over Google snippets and unstructured query-biased summaries in terms of f-measure using the relevance prediction approach.
4717860
Subspace representation of registration and reconstruction in multi-frame Super-Resolution###Multi-frame super-resolution reconstruction (SRR) problem has two bottlenecks; under constrained reconstruction and observation registration. Currently there is no practical solution for these problems, even in domain-specific cases. In this work we suggest solutions for both of these problems. SRR problem is transformed to a reduced space to render an over-constraint problem. Thus significant amount of computational saving and robustness is obtained. Moreover instead of estimating registration parameters we follow a more direct way and employ deformable models to register observations to the reference frame. By combining these two solutions we obtain faster, more flexible and superior SRR results.
4717861
Multiresolution techniques for rain rendering in virtual environments###Virtual environments often include rain and other atmospheric phenomena which are difficult to simulate in real time. In the field of virtual reality a number of solutions have been proposed which offer realistic but costly rain systems. Our proposal consists in developing a solution to facilitate the creation of rain scenes and to reduce the cost of previous particle systems, while maintaining the realistic appearance of rain . This objective is accomplished by performing a suitable management of the particle system. We include level-of-detail techniques in order to adapt the number of particles, their location and their size according to the position of the observer and distance to the particles. The framework we propose is completely integrated within the GPU, thus offering a solution which is fast and efficient.
4717862
Optimal discretization for high-entropy graphical passwords###In click-based graphical password schemes that allow arbitrary click locations on image, a click should be verified as correct if it is close within a predefined distance to the originally chosen location. This condition should hold even when for security reasons the password hash is stored in the system, not the password itself. To solve this problem, a robust discretization method has been proposed, recently. In this paper, we show that previous work on discretization does not give optimal results with respect to the entropy of the graphical passwords and propose a new discretization method to increase the password space. To improve the security further, we also present several methods that use multiple hash computations for password verification.
4717863
Perspectives of auto-correcting lens distortions in mosaic-based underwater navigation###When unmanned underwater vehicles (UUVs) perform missions near the ocean floor, optical sensors can be used to improve local navigation. Video mosaics allow to efficiently process the images acquired by the vehicle, and also to obtain position estimates. We discuss in this paper the role of lens distortions in this context, proving that degenerate mosaics have their origin not only in the selected motion model or in registration errors, but also in the cumulative effect of radial distortion residuals. Additionally, we present results on the accuracy of different feature-based approaches for self-correction of lens distortions that may guide the choice of appropriate techniques for correcting distortions.
4717903
Building an ontology for flexible power quality querying###Electrical power quality (PQ) is measured as a set of PQ parameters, such as frequency, flicker, and harmonics, obtained from electrical transmission and distribution systems. The domain of electrical PQ suffers from the lack of a common vocabulary to be shared by the involved parties. There exist several systems of varying complexities to measure PQ parameters, yet no ontology development effort for PQ data has been reported in the literature. In this paper, a novel and generic domain ontology for electrical PQ is presented. During the ontology development process, relevant standards as well as experiences gained during the development of two distinct large-scale systems for nationwide PQ monitoring have been used. Among these monitoring systems, one is a mobile measurement system which has collected sample PQ data of considerable size all over the Turkish electricity transmission system to detect problematic locations. The second system, which aims to perform on line PQ monitoring with permanent monitors, is the successor of the previous one and has recently started to collect data at several problematic points in the transmission system. The PQ ontology is especially important for its being the first attempt to create a common vocabulary for the domain and hence to make differing PQ monitoring systems interoperable. The proposed ontology is utilized to develop a natural language based interface for querying the PQ data collected through mobile measurement system, in a flexible way. The implementation details of the interface is presented with a query example on the PQ data. The future work will include the extension of this natural language based interface into a speech recognition engine to get spoken queries into the system and enhance the interface to handle queries in Turkish.
4717902
Fast video vector quantization###In this paper fast video vector quantization algorithm was developed utilizing high correlation between successive frames which can be used for motion compensation as a next step. Best matching code vector search speed is increased using previously encoded frame code vector index list, which carry the available information of that frame. By using interframe correlation, search speeds of tree structured vector quantization (TSVQ) was achieved Additionally the coding structure developed here is directly applicable to motion estimation (ME) and motion compensation. (MC) The main objective of this work is real time video encoding using high calculation power of server computer and at clients side requiring very low computation power, meanwhile requiring low electric energy. The algorithm developed here is very suitable especially for mobile equipments which can cary only limited electric power on it.
4717901
ComHRP : A hierarchical routing protocol in dynamic sensor networks###Sensor node has node ID for identity on sensor network. ID assignment is a principle technique for hierarchical routing. However, as conventional hierarchical routing methods do not consider end-nodes, those only can support a simple path search. Therefore, those are not adequate in dynamic sensor network environments. In this paper, we propose ComHRP (complement HRP) that inherits advantages of conventional methods. We also present address assignment and search techniques according to ComHRP. Network organizations can be simplified with ComHRP thereby mitigating the complexity of networks. Moreover, network reassignment, which is unpredictable by the nature of ubiquitous sensor networks, can be efficiently processed.
4717900
An evaluation of existing and new feature selection metrics in text categorization###Text categorization is widely used for organizing and manipulating the documents in the electronic medium. Since the data in text categorization field are high-dimensional, feature selection is crucial to make the task more efficient and precise. In this paper, we make an extensive evaluation of the feature selection metrics used in text categorization by using local and global policies. For the experiments, we use three datasets which vary in size, complexity and skewness. We use SVM as the classifier and tf-idf weighting for term weighting. We observed that almost in all metrics, local policy outperforms when the number of keywords is low and global policy outperforms as the number of keywords increases. In addition to the evaluation of the existing feature selection metrics, we propose new metrics, which have shown high success rates especially in datasets with a low number of keywords. Moreover, we propose a keyword selection policy called adaptive keyword selection (AKS). It is based on selecting different number of keywords for different classes and it improved the performance significantly in skew datasets.
4717907
Target classification with simple infrared sensors using artificial neural networks###This study investigates the use of low-cost infrared (IR) sensors for the determination of geometry and surface properties of commonly encountered features or targets in indoor environments, such as planes, corners, edges, and cylinders using artificial neural networks (ANNs). The intensity measurements obtained from such sensors are highly dependent on the location, geometry, and surface properties of the reflecting target in a way which cannot be represented by a simple analytical relationship, therefore complicating the localization and classification process. We propose the use of angular intensity scans and feature vectors obtained by modeling of angular intensity scans and present two different neural network based approaches in order to classify the geometry and/or the surface type of the targets. In the first case, where planes, 90deg corners, and 90deg edges covered with aluminum, white cloth, and Styrofoam packaging material are differentiated, an average correct classification rate of 78% of both geometry and surface over all target types is achieved. In the second case, where planes, 90deg edges, and cylinders covered with different surface materials are differentiated, an average correct classification rate of 99.5% is achieved. The method demonstrated shows that ANNs can be used to extract substantially more information than IR sensors are commonly employed for.
4717906
A hierarchical architecture to implement a &#x03B3; synchronizer in wireless sensor networks###A synchronizer provides synchronous execution of asynchronous algorithms in an asynchronous network. We provide an architecture for a gamma synchronizer in a wireless sensor network. The sensor network is partitioned into clusters of connected sub-spanning trees by an algorithm described by Erciyes, K. et al (2008) in the first phase. Once the sub-spanning trees in the clusters are formed, the ring formation algorithm provides bands of rings among the clusterheads of the same levels. This structure provides the necessary architecture for the gamma synchronizer where communication between the clusters is performed using the ring similar to a alpha synchronizer and the intra cluster communication is accomplished using the sub-spanning tree as in beta synchronizers. We discuss the model along with the algorithms and show that this architecture provides scalable operation of a gamma synchronizer in a sensor network of significant size.
4717905
Improving face recognition from videos with preprocessed representative faces###In this work, a face recognition system working on video records is constructed. The aim of this study is to analyze; how -instead of using all video frames and building a complex model-using only a subset of informative frames (named representative frames), automatically selected and preprocessed by a system, affects success rate and processing time.
4717904
An evaluation of divide-and-combine strategies for image categorization by multi-class support vector machines###Categorization of real world images without human intervention is a challenging ongoing research. The nature of this problem requires usage of multiclass classification techniques. In divide-and-combine approach, a multiclass problem is divided into a set of binary classification problems and then the binary classifications are combined to obtain multi-class classification. Our objective in this work is to compare several divide-and-combine multiclass SVM classification strategies for real world image classification. Our results show that One-against-all and One-against-one MaxWins are the most efficient methods.
4717909
Correspondenceless Pose Estimation from a single 2D image using classical mechanics###Estimation of camera pose in world coordinates using a ldquorigidrdquo object model with ldquofeature pointsrdquo and their respective images with ldquoknown correspondencerdquo to the object model is a problem that has been studied extensively in the computer vision literature. We propose a ldquocorrespondencelessrdquo method called gravitational pose estimation (GPE). The word correspondenceless means the information of which image point corresponds to which object point is not needed. GPE requires only a single frame of image plus an object model and is inspired by classical mechanics. GPE creates a gravitational field from the image and lets the object model move and rotate in that force field. Experiments were carried out with both real and synthetic images. Results from real images were compared to measurements directly from our mechanical fixture. Experiments demonstrated GPE is accurate and hence allows efficient pose estimation even when correspondence information is unknown.
4717918
Hydrodynamic based hybrid dynamic load balancing###Balanced load distribution is especially important to attain optimal use of existing computational resources in distributed and parallel applications. In dynamic load balancing (DLB), surplus workload in nodes overwhelmed with work is transferred to relatively free nodes during run time. While in iterative DLB methods, the load reaches to its final execution node through several iteration steps, the execution node is selected directly in one step in direct methods. However, direct methods require immense system state information to perform selection. In this paper, we present two new hybrid dynamic load balancing (HLB) methods that take advantage of both direct and iterative methods. HLB aims to consume minimum possible system resources to balance common workload distributions by using an iterative method, the hydrodynamic approach (HA). Besides, HLB intends to solve the problems derived from exceptional instantaneous load rises by using a direct method which requires only little system knowledge. The excess workload is shared directly with some non-neighboring nodes which are selected randomly or from a fixed distribution list. The experimental results designate that the hybrid methods outrun other iterative methods in terms of performance and whole system utilization.
4717919
Combined color and texture tracking for video post-editing###Tracking a human head in a complicated scene with changing object pose, illumination conditions, and many occluding objects, is the subject of this paper. We present a general tracking algorithm, which uses a combination of object color statistics and object texture features with motion estimation. The object is defined by an ellipse window that is initially selected by the user. Color statistics are obtained by calculating object color histogram in the YCrCb space, with more resolution reserved for chroma components. In addition to the conventional discrete color histogram, a novel method, uniform fuzzy color histogram (UFCH) is proposed. The object texture is represented by lower frequency components of the objectpsilas discrete cosine transform (DCT), and local binary patterns (LBP). By using the tracker, performances of different features and their combinations are tested. The tracking procedure is based on constant velocity motion estimation by condensation particle filter, in which the sample set is obtained by the translation of the object window. Histogram comparison is based on Bhattacharyya coefficient, and DCT comparison is calculated by sum of squared differences (SSD). Similarity measures are joined by combining their independent likelihoods. As the combined tracker follows different features of the same object, it is an improvement over a tracker that makes use of only color statistics or texture information. The algorithm is tested and optimized on the specific application of embedding interactive object information to movies.
4717853
System virtualization method for RFID tag infrastructure network###The use of RFID tag which identifies a thing and an object will be expanded with progress of ubiquitous society, and it is supposed that the various RFID-based services will be provided in the near future. Therefore, it is necessary to study how to construct RFID network system as a social infrastructure like the Internet. This paper proposes the virtualization method of RFID tag network system to enable the same physical RFID network system to be used by multiple different service systems. The system virtualization not only reduces the system cost but also can dramatically reduce the installation space of physical readers and the operation cost. It is proposed that all equipments in the RFID network system except radio tag could be shared with the conventional virtual technologies for servers or networks. It is also proposed to add two new identifiers, in order to improve user convenience in the infra-structured RFID network system. One is dasiasystem IDpsila to keep the association between a virtual reader element and a physical RFID tag, and the other is dasiapolicy numberpsila that specifies the conditional tag ID processing.
4717855
Evaluation of splitting methods in top-down OBB-tree construction###The Oriented Bounding Box trees (OBB-trees) offer efficient means for real-time collision detection between geometric models. The efficiency of the OBB-tree in answering the queries depends mainly on the way the triangles are splitted and distributed to the child nodes. In this paper we examine four different partitioning methods for top-down construction of OBB-trees and compare their performance in three different types of queries: collision detection between two objects, finding the triangle closest to a given point and extremal point queries. In addition to the examined methods we also offer a method for determining and splitting disconnected sets of triangles at an early stage during the hierarchy construction.
4717854
Building detection from aerial images using invariant color features and shadow information###Robust detection of buildings is an important part of the automated aerial image interpretation problem. Automatic detection of buildings enables creation of maps, detecting changes, and monitoring urbanization. Due to the complexity and uncontrolled appearance of the scene, an intelligent fusion of different methods gives better results. In this study, we present a novel approach for building detection using multiple cues. We benefit from segmentation of aerial images using invariant color features. Besides, we use the edge and shadow information for building detection. We also determine the shape of the building by a novel method.
4717857
(2,2)-Secret Sharing scheme with improved share randomness###Visual secret sharing (VSS) is a method to scatter a secret image into shares of random patterns that have to be stacked together to reconstruct the secret. Since each share is a random pattern of black and white pixels, it is not enough to rebuild the secret from just one share. Stacking two shares reconstructs the first secret whereas rotating the first share by 90deg counterclockwise and stacking reconstructs the second secret. A novel algorithm to create both shares from two secrets with improved randomness is proposed in this paper. Share creation algorithm selects extended patterns of random pixels to satisfy contrast requirements of both secrets reconstructed by normal and rotated stacking. Proposed method have desirable security properties because of the improved randomness of shares.
4717856
An efficient heuristic for placement, scheduling and routing in wireless sensor networks###In this paper we consider the differentiated coverage problem for heterogeneous sensor networks over a finite planning horizon consisting of discrete time intervals. The goal is to determine optimal locations of sensors and sinks, activity schedules of the deployed sensors, and sensor-to-sink data routes subject to coverage, flow conservation, energy consumption and budget constraints with the objective of maximizing the network lifetime. We first give a mixed-integer linear programming formulation. Since this formulation is difficult to be solved exactly even for small instances, we propose a heuristic that first finds connected sensor sets with minimum cost satisfying the coverage constraint, and then determines optimal sensor-to-sink data routes with optimal flow quantities maximizing the lifetime of the sensor sets. Computational experiments performed on various test instances indicate that the heuristic is very efficient and quite accurate.
4717859
Web image annotation using word co-occurrence and association analysis###With the popularization of Web and image devices, there are more and more digital images available on the Internet. How to effectively organize and manage these Web images becomes a critical issue. In this paper, a novel approach that employs the relationships between words to achieve Web image annotations is proposed. First, the related textual information associated with Web images is identified as the candidate annotations for Web images. Second, the word co-occurrence is utilized to eliminate irrelevant keywords for improving the annotation accuracy. Then, the keyword-based association analysis is exploited to further discover possible semantics for enriching the final annotations of Web images. Finally, experimental results demonstrate the effectiveness of the proposed method.
4717858
Novel constrained-path computation algorithm for failure recovery and CAC in GMPLS optical networks###GMPLS protocol was designed in order to apply label-switching techniques to different network technologies, such as time division multiplexing (TDM), wavelength routing and packet switching networks. Resource reservations are performed all along label switched paths (LSP). This paper proposes a novel constraint-based routing (CBR) algorithm for GMPLS networks. Our solution includes path computation and resourcespsila assignment for both admission control and failure recovery mechanisms. Unlike other constraint-based routing protocols, our algorithm supports several classes of traffic with different failure recovery schemes. We investigated the performance of our algorithm through simulation works which are performed on a GMPLS optical network configured with the European reference core topology. Numerical results have shown that, compared to constrained shortest path first (CSPF) routing, our solution enhances the success probability of path setup and optimizes the global performance in terms of queuing delay and bit loss ratio.
4717912
A high-precision template localization algorithm using SIFT keypoints###High-precision localization is one of the important applications in the field of computer vision. In this paper a high-precision template localization algorithm based on SIFT (scale invariant feature transform) is presented. The proposed method is composed of three main steps. In the initial step the SIFT features are extracted. With these features the basic matching strategy and clustering method similar distance threshold (SDT) are investigated to match the keypoints between template and test images and eliminate the possibility of mismatch. Then iterative least square method (ILSM) is adopted to locate the template and improve the accuracy. Compared with the traditional template matching methods, the proposed method could enhance the robustness effectively, which ensures to give correct results, no matter the test image changes its scale, rotates itself or is covered partly. The localization accuracy reaches 0.1 pixels.
4717913
Segmentation of coronary vessel structures in X-ray angiogram images by using spatial pattern matching method###In this paper, a model based segmentation method is introduced for extracting blood vessel structures in poor quality coronary angiograms. This method extracts blood vessels in the angiograms by exploiting the spatial coherence existing in the image. Here, a circular sampling method is employed to exploit the coherence. This method uses a collection of 2D patterns to represent the 3D structures of vessels. The segmentation method employs the circular sampling method to produce the 2D slice samples at certain depths on each pixel on a varying background on the image, so several 2D sample slices of the 3D pattern of blood vessels are collected. These 2D slices are compared with certain original patterns in order to check whether a slice is part of a blood vessel. Finally, results from several overlapping 2D slices are evaluated collectively and checked whether they represent a 3D blood vessel histogram. To produce the final segmented image, incorrectly segmented noisy parts and discontinuous parts are eliminated by using circular filtering methods. The performance of the method is examined on various qualities of x-ray angiograms and synthetic images. Results indicate that the proposed method yields a good performance in automatic segmentation of such images.
4717914
Accurate and robust image registration based on radial basis neural networks###Neural network-based image registration using global image features is relatively a new research subject and the schemes devised so far use a feedforward neural network to find the geometrical transformation parameters. In this work, we propose to use a radial basis function neural network instead of feedforward neural network to overcome lengthy pre-registration training stage. This modification has been tested on a typical neural network-based registration method using discrete cosine transformation features in the presence of noise. The proposed scheme does not only speed up the training stage enormously, but also increases the accuracy and robustness against additive white noise owing to the better generalization ability of the radial basis function neural networks.
4717915
A generic system for the classification of marble tiles using Gabor filters###This paper presents a marble tile feature extraction system which can be easily used for any classification system. Our system employs Gabor filtering and other image processing techniques to differentiate between different marble textures. As a final product of the system, the percentages of veins, spots, and swirls on the marble images are calculated. Therefore, the presented system can be considered as the core engine of a very portable marble tile classification system. This paper also presents a new verification method which is based on satisfying the inter-expert variability. Experiments on 40 different marble tiles are performed and we concluded that the presented system can be reliably employed for the measurement of the marble features in real life.
4717916
Face recognition by combining Gabor wavelets and nearest neighbor discriminant analysis###One of the successful approaches in face recognition is the Gabor wavelets-based approach. The importance of the Gabor wavelets lie under the fact that the kernels are similar to the 2-D receptive field profiles of the man visual neurons, offering spatial locality, spatial frequency and orientation selectivity. In this work, we propose a new combination of a Gabor wavelets-based method for illumination and expression invariant face recognition. It applies the Nearest Neighbor Discriminant Analysis to the augmented Gabor feature vectors obtained by the Gabor wavelets representation of facial images. To make use of all the features provided by different Gabor kernels, each kernel output is concatenated to form an augmented Gabor feature vector. The feasibility of the proposed method has been successfully tested on Yale database by giving a comparison with its predecessor, NNDA. The effectiveness of the method is shown by a comparative performance study against standard face recognition methods such as the combination of Gabor and Eigenfaces and the combination of the Gabor and Fisherfaces, using a subset of the FERET database containing a total of 600 facial images of 200 subjects exhibiting both illumination and facial expression variations. The achieved recognition rate of 98 percent in the FERET test shows the efficiency of the proposed method.
4717917
Design of an adaptive Genetic Algorithm for maximizing and minimizing throughput in a computer network###The Genetic Algorithms (GAs) control parameter settings are key factors in the determination of the exploitation versus exploration tradeoff. Adaptive genetic algorithms (AGAs) have been built for inducing exploitation/exploration relationships that improve the final results. One of the most widely studied adaptive approaches are the adaptive parameter setting techniques. Nevertheless, there are no standard rules for choosing appropriate values for these parameters and this decision is usually taken in terms of the most common values or experimental formulas given in literature, or by means of trial an error methods. The paper presents an effective approach based on a meta-level GA combined with an adaptation strategy of the GA control parameters to find and adjust the optimum probabilities to improve the GA performance. In order to validate our approach, an AGA have been designed to drive the generation of a background traffic for maximizing and minimizing throughput in a computer network. Different comparisons are performed, aiming to assess the acceptable optimization power of the proposed system.
4717867
A new way of calculating the recovery line through eliminating useless checkpoints in distributed systems###Uncoordinated checkpointing protocol is a simple protocol used in many distributed systems for fault tolerance. In this paper, we discuss on the size of rollback it has in the presence of failures. In order to determining the recovery line in checkpoint-based recovery, we first study to common approaches: dependency graph and checkpoint graph and provide some algorithms for these approaches. Then we introduce a new approach for calculating the recovery line and making a graph (independent graph). Finally we present a solution for reducing the cost of graph when calculating the recovery line, particularly when the domino effect is occurred.
4717868
Semantic Cross-lingual Information Retrieval###Cross lingual Information Retrieval (CLIR) refers to the information retrieval activities in which the query and/or documents may appear in different languages. Dictionary-based query translation has been a common method in CLIR systems. In these methods we face with the problem of translation ambiguity in which a single word in one language has more than one translation in the other language. In this paper we propose a hybrid approach to retrieve English documents relevant to Persian queries. In this approach we exploit a combination of phrase reorganization, pattern based phrase translation and query expansion before and after translation to improve the dictionary-based query translation. We also propose an improved probabilistic algorithm to choose the best translation of words and phrases. Finally, the documents will be ranked according to statistical language model with some translation steps. Our experimental results show that each of the mentioned methods can bring significant improvement over simple dictionary approaches.
4717869
Matchbook a web based recommendation system for matchmaking###In todaypsilas world, many systems and approaches make it possible for us to be guided by the recommendations they provide. Personalization, which intends to resolve the problems caused by the huge amount of information around, is the main goal to be achieved by these systems. In this paper, we represent the Web based recommendation system that we have developed by using personalization techniques combining machine learning with automated reasoning.
4456831
SIRC, a multiple isolation level protocol for middleware-based data replication###One of the weaknesses of database replication protocols, compared to centralized DBMSs, is that they are unable to manage concurrent execution of transactions at different isolation levels. In the last years, some theoretical works related to this research line have appeared but none of them has proposed and implemented a real replication protocol with support to multiple isolation levels. This paper takes advantage of our MADIS middleware and one of its implemented Snapshot Isolation protocols to design and implement SIRC, a protocol that is able to execute concurrently both generalized snapshot isolation (GSI) and generalized loose read committed (GLRC) transactions. We have also made a performance analysis to show how this kind of protocols can improve the system performance and decrease the transaction abortion rate in applications that do not require the strictest isolation level in every transaction.
4456830
Enhanced semantic operations for web service composition###In service composition, service discovery and combining suitable services through determination of interoperability among different services are important operations. Utilizing semantics improves the quality and facilitates automation of these operations. There are several previous approaches for semantic service discovery and service matching. In this work, we exploit and extend these semantic approaches in order to make Web service composition process more facilitated, less error prone and more automated. This work includes a service discovery and service interoperability checking technique which extends the previous semantic matching approaches. In addition to this, as a guidance system for the user, a new semantic domain model is proposed that captures semantic relations between concepts in various ontologies.
4456833
An enhanced grouping algorithm for vertical partitioning problem in DDBs###Distribution design involves making decisions on the fragmentation and allocation of data across the sites of a computer network. Vertical partitioning is the process of subdividing the attributes of a relation to generate fragments. In this paper, we propose an enhancement to our previous work for vertical partitioning algorithm using grouping approach. This algorithm starts from the attribute affinity matrix and generates initial groups based on the affinity values between attributes. Then, it attempts to merge the initial groups to produce final groups that will represent the fragments.
4456832
Database Fusion Using Atom###This paper describes a database fusion infrastructure that produces a local database consisting of aggregated information that is selected from multiple remote databases and is transformed into a common table format and data representation in the local database. The database fusion infrastructure provides reliable data distribution (RDD), consistent data replication (CDR), and database aggregation (DBA) and is based on the atom syndication technology. RDD ensures that the publisher knows that a consumer has received the data intended for it and, thus, that the publisher can garbage collect the data. CDR uses RDD to provide data replication from a source database to a target database for the purposes of security, high availability and fast local access. DBA uses CDR to aggregate information from remote database sources, possibly deployed on different computing platforms in different enterprises, using different databases from different vendors, different database schemas and different communication protocols.
4456835
Improving quality of code generated from OCL expressions###In this paper, we briefly describe existing principles and stages for generating code from OCL expressions pointing out the drawbacks that cause inefficiencies of the resulting code. The proposed improvement of the transformation is based on extended abstract syntax trees (AST) with context-specific attributes. Principles for defining such attributes on AST trees and an example of transformation is presented.
4456834
A comparison of methods for semantic photo annotation suggestion###In this work we describe a general framework for semi-automated semantic digital photo annotation though the use of suggestions. We compare context-based methods with Latent Semantic Indexing, a linear algebra approach to information retrieval. Through experiments on real data sets containing up to 13,705 semantically annotated photos, we show that a carefully chosen combination of context-based methods can not only be efficient, but also extremely effective as well. Furthermore, we propose a new combination of context-based methods that outperforms previous work by up to 19% higher recall while running up to 21 times faster.
4456837
Solution of large-scale scattering problems with the multilevel fast multipole algorithm parallelized on distributed-memory architectures###We present the solution of large-scale scattering problems involving three-dimensional closed conducting objects with arbitrary shapes. With an efficient parallelization of the multilevel fast multipole algorithm on relatively inexpensive computational platforms using distributed-memory architectures, we perform the iterative solution of integral-equation formulations that are discretized with tens of millions of unknowns. In addition to canonical problems, we also present the solution of real-life problems involving complicated targets with large dimensions.
4456836
An inspection approach for conceptual models in notations derived from UML: A case study###This paper presents an overview of a proposed framework for improving the quality of conceptual models developed in notations derived from UML. To this aim, wellness properties of a UML model are described shortly. Through a case study, we show that an ad hoc inspection process in accordance with this framework can be defined and performed depending on the conceptual modeling notation and typical real models. This study showed us that the application of the defined inspection process, which adopts concepts and properties available for formally defined notations, revealed many non trivial fallacies in models especially in derived activity diagrams.
4456839
Heading-based sectional hierarchy identification for HTML documents###Most of the documents found on the Web are prepared in HTML format which was basically designed for presentation of data. As a result, some limitations are encountered when these documents are accessed automatically for a semantic interpretation of their content. One such inadequacy is in representing the sectional hierarchy (i.e. sections and subsections) of these documents and the headings in this hierarchy. Automatically obtaining this information is a difficult task due to the underlying format and the cluttered structure encountered in most of the Web pages. In this paper, we propose a novel approach to extract heading-based sectional hierarchies of HTML documents. This is the first part of the research, where we aim to use this information in automatic summaries to improve Web search experience of Internet users.
4456838
A framework for the management of distributed systems based on SNMP###The traditional task of managing and monitoring a network has never been a trivial one. With recent changes in computing and networking, the area of distributed systems management faces new challenges and increasing complexity. Research in the relevant field reveals that, while there are many research and commercial solutions available, some of them are based on proprietary standards. Others focus on monitoring, while lacking the ability to actively make modifications and fine- tuning. Some others have a narrow target group. This paper proposes a framework for the management of distributed applications. The managed hosts are treated as integral parts of the deployment and not as stand alone, isolated entities. The framework is based on SNMP and is not limited to monitoring. On the contrary, it is capable of carrying out SNMP-SET commands, actively modifying the run-time parameters of the managed application. Finally, it can perform the management of a variety of distributed systems, ranging from small clusters to larger scale deployments such as computational or data grids.
4456851
Orthographic backface culling using nested normal cone hierarchies###This paper presents an efficient dual-space method for culling back-facing triangles of a triangular mesh under directional projection. The proposed method first performs a binary space partitioning on the points that correspond to the image of the normals under the Gaussian Map. Using this bsp-tree a nested pyramid hierarchy containing the triangle-normals is constructed. In the last step, cones are fitted to each pyramid in the nested hierarchy to improve the robustness and efficiency of the back-face culling algorithm. The results of a preliminary performance analysis of the proposed method is also given.
4456850
Tara: An algorithm for fast searching of multiple patterns on text files###This work introduces a new multi-pattern matching algorithm that performs searching of fixed-length strings on text files very fast by benefiting from bit-parallelism. The algorithm is given name tara. Bounded gaps as well as character classes in keywords are also supported. Although the worst case time complexity is quadratic, it performs very fast in practise. Experiments are conducted to compare the performance of the proposed algorithm with widely used GNU grep file search utility and also with 9 variants of Aho&amp;Corasick and Comentz&amp;Walter algorithms on natural language text. On the average tara is approximately 10% faster then grep, where up to 70% percent speed up is observed. The benchmark with the AC and CW variants results that the speed up obtained by tara is 3,5 times relative to its nearest successor.
4456857
Range queries in natural language dictionaries with recursive lists of clusters###We evaluate the performance of range queries in the Recursive List of Clusters (RLC) metric data structure, when the metric spaces are natural language dictionaries with the Levenshtein distance. The study compares RLC with five data structures (GNAT, H-Dsatl, LAESA, LC, and vp-trees) and comprises six dictionaries. The natural language dictionaries (in English, French, German, Italian, Portuguese, and Spanish), are characterised according to the mean and the variance of the histograms of distances. The experimental results show that RLC has a good performance in all tested cases and, in some of them, it outperforms all the other data structures. In addition, RLC is the only data structure that always keeps its good performance, whether the space dimension is lower or higher, and whether the query radius is smaller or larger.
4456856
The effect of data set characteristics on the choice of clustering validity index type###Clustering techniques are widely used to give insight about the similarities/dissimilarities between data set items. Most algorithms require the user to tune parameters such as number of clusters or threshold for cut-off point in a dendrogram. Such parameters also affect the clustering quality. In a good quality cluster, the intra-cluster similarity should be high, whereas the inter-cluster similarity should be low. To determine the optimal cluster number, several cluster validity methods have been proposed. However, there is no guideline with respect to which clustering validity methods can be used in conjunction with which clustering algorithms. In this paper, Dunn and SD validity indices were applied to Kohonen self organizing maps, k-means and agglomerative clustering algorithms and their limitations were shown empirically.
4456855
Induction of logical relations based on specific generalization of strings###Learning logical relations from examples expressed as first order facts has been studied extensively by the inductive logic programming research. Learning with positive-only data may cause over generalization of examples leading to inconsistent resulting hypotheses. A learning heuristic inferring specific generalization of strings based on unique match sequences is shown to be capable of learning predicates with string arguments. This paper describes an inductive learner based on the idea of specific generalization of strings, and the given clauses are generalized by considering the background knowledge.
4456854
Authorship attribution###Authorship attribution is the process of determining the writer of a document. In literature, there are lots of classification techniques conducted in this process. In this paper we explore information retrieval methods such as tf-Idf structure with support vector machines, parametric and nonparametric methods with supervised and unsupervised (clustering) classification techniques in authorship attribution. We performed various experiments with articles gathered from Turkish newspaper Milliyet. We performed experiments on different features extracted from these texts with different classifiers, and combined these results to improve our success rates. We identified which classifiers give satisfactory results on which feature sets. According to experiments, the success rates dramatically changes with different combinations, however the best among them are support vector classifier with bag of words, and Gaussian with function words.
4456859
Improved post-processing for GMM based adaptive background modeling###In this paper, we propose a new post-processing method for Gaussian mixture model (GMM) based adaptive background modeling which was proposed by Stauffer and Grimson. This is a ubiquitous and successful background modeling method. A drawback of this method is that it assumes independence of pixels and relies solely on the difference between current pixel value and its past values. This causes some errors within the foreground region and results in fragmentation of foreground objects detected. Our method uses relaxed- thresholding and adds foreground edge information in close proximity of detected foreground blobs. The close proximity is obtained as the union of convex hulls of close-by regions which we call the hysteresis region. Our results show that we can achieve increased recall rate with the proposed method without much decreasing the precision of the conventional method.
4456858
Automatic identification of pronominal Anaphora in Turkish texts###Anaphora identification is an important problem especially for its impact on anaphora and coreference resolution systems. In this paper, a system that automatically identifies anaphoric pronouns in Turkish is presented. The proposed system takes a decision tree learning approach, that of Quinlan's C 4.5, where a corpus examination is carried out to determine linguistic features specific to Turkish which are to be used by the decision tree learner. The proposed system is significant especially for its ease of incorporation into any anaphora resolution system for Turkish. The system is evaluated on two different Turkish text samples and its performance on these samples is close to that of human identification.
4456898
Enabling delegation for impromptu collaboration in pervasive computing###One of the most common types of interactions in pervasive computing has been known as impromptu collaboration, which is often characterized as being opportunistic, spontaneous, proximity-based, and transient. Many research efforts have been made to secure this type of communications, addressing issues such as secure device identification, authentication, trust, and access control. However, little focus has been given on the issue of delegation, which could be a key enabler toward secure resource sharing in pervasive computing. In this paper we present a lightweight delegation framework extended from a previous access control solution for impromptu collaboration in pervasive computing. Our solution relies on the use of XML, and is secured by HMAC.
4456863
Software effort estimation using machine learning methods###In software engineering, the main aim is to develop projects that produce the desired results within limited schedule and budget. The most important factor affecting the budget of a project is the effort. Therefore, estimating effort is crucial because hiring people more than needed leads to a loss of income and hiring people less than needed leads to an extension of schedule. The main objective of this research is making an analysis of software effort estimation to overcome problems related to it: budget and schedule extension. To accomplish this, we propose a model that uses machine learning methods. We evaluate these models on public datasets and data gathered from software organizations in Turkey. It is found out in the experiments that the best method for a dataset may change and this proves the point that the usage of one model cannot always produce the best results.
4456893
The effect of euclidean distance in directed scale-free network generation###Krapivsky-Redner model is one of the variations of Barabasi-Albert model a widely known model generating scale- free network. In Krapivsky-Redner model, a directed network whose both in-degree and out-degree distributions follow a power law is created, however; the clustering coefficient is lower than the ones observed in nature. In this paper, we added a new constraint, the Euclidean distance to the attraction formed by popularity. With this new constraint, we preserved the scale-free structure and came up with directed networks with shortened Euclidean edge distance and increased clustering coefficient. We simulated this system within multidimensional spaces ranging from one dimension to five dimensions.
4456862
Ensemble based incremental SVM classifiers for changing environments###For most of the real-world applications, two main challenges are infinite data flow and time changing concepts. Generally data are gathered over a long period of time and the data generation mechanism may change with time. In a dynamic environment, knowledge about the environment is rarely complete due to time-changing concepts. In recent years, a lot of methods have been proposed for effective learning in changing environments. Due to their ability to learn from new data, incremental learning algorithms can be used for learning in changing environments. In this paper we propose an ensemble based incremental learning approach with SVM (support vector machines) classifiers to provide ability to learn new domain knowledge in a non-stationary environment. Experiments on different datasets with simulated concept drift show promising results.
4456853
A physically-based facial skin model to simulate facial expressions on digitally scanned 3D models###In this paper we present a physically-based 3D facial skin model based on biomechanical properties of human facial anatomy. The skin model is mainly formed of two DNURBS surfaces as a two layered membrane structure. The model represents the soft tissue layer which covers the skull and is attached to the skull via muscle insertion points. The model is fitted on digitally scanned 3D facial surfaces and controlled via DNURBS control points, which are spatially close to muscle insertion points. The face (skin model) is modified by displacing control points to mimic muscle contraction in order to simulate facial expressions. The system is solved and the simulations are displayed on a graphics workstation, where near real-time performance is achieved as opposed to many other existing physically- based methods.
4456860
Turkish keyphrase extraction using KEA###Keyphrases provide semantic metadata to summarize and characterize documents. Unfortunately, there are many digital documents especially on the Internet that do not have a list of assigned keyphrases. Assigning keyphrases to these documents manually is a tedious process and requires knowledge of the subject. Automatic keyphrase extraction solves this problem. In this paper, we present implementation of keyphrase extraction algorithm (KEA) for Turkish as well as extending it with new features to improve its performance.
4456861
A classification algorithm for finding the optimal rank aggregation method###In this paper, we develop a classification algorithm for finding the optimal rank aggregation algorithm. The input features for the classification are measures of noise and misinformation in the rankers. The optimal ranking algorithm varies greatly with respect to these two factors. We develop two measures to compute noise and misinformation: cluster quality and rank variance. Further, we develop a cost based decision method to find the least risky aggregator for a new set of ranked lists and show that this decision method outperforms any static rank aggregation method by through rigorous experimentation.
4456866
A nonparametric statistical approach for stereo correspondence###This paper introduces a novel non-parametric statistical metric that can decide if the recovered set of parameters from a computer vision optimization process can actually be considered as a statistically significant solution. The level of significance can be used as a quality metric of the solution which makes it possible (i) to compare the solutions obtained using different optimization methods, and also (ii) to set intuitive thresholds on the acceptance criteria. We chose the stereo correspondence optimization methods as the initial test bed for the new technique. We compare and combine the results of sum of squared differences (SSD) and sum of absolute differences (SAD) methods for stereo correspondence. We validated our claims by running experiments on standard stereo pairs with ground truth. The results showed that the introduced ideas actually work very well and they can be used to improve the optimization results from different sources.
4456867
GPU based real time stereoscopic ray tracing###Over the last couple of years graphics processing units (GPU) found in graphics cards evolved into general purpose parallel stream processors. This evolution allows for using GPUs not only for traditional rasterization based rendering but also for global illumination techniques including ray tracing. Fast generation of stereo images is very important for virtual reality applications. Rendering stereo image pairs for left and right eye separately doubles the frame time. This might be a problem for interactive applications especially if computationally expensive rendering techniques such as ray-tracing are employed. It is possible to reduce the stereo image generation time by ray tracing the scene for one eye and then reprojecting the image into the second eye. In this case, only problematic pixels will be ray traced for the second eye. GPUs are designed to write pixel color values at unique screen locations defined by projected geometry. However reprojection may require writing pixel information to multiple screen locations. This is one-to-many dynamic scattering problem in which GPUs perform relatively badly. In this work we devised efficient stereo reprojection methods running fully on the GPU. We demonstrated the technique in our GPU based interactive ray tracer and showed that reprojection method can reduce the stereo frame time considerably. We also propose a GPU based approach to handle missing object problem if an object is seen only by the reprojected eye. Additionally, reprojections of reflection/refraction rays to create approximate images are discussed.
4456864
Cross comparison of synonym graphs in a multi linguistic context###Language is one of the most important aspects of human cognition; it represents the way we think, act and communicate with each other. Each language has its own history, background, and form. A language represents a lot of important cultural aspects of the nation speaking it. Languages differ and so do cultures. In this paper we analyze cultural differences between East and West in a multi-linguistic context from a complex networks point of view. There has been considerable work on the topic of cultural differences by psychologist and sociologist. Also studies on complex networks that make use of WordNet have been done, but until now there is no previous work that uses WordNets from different Eastern and Western languages as complex lexical networks in order to obtain possible differences or similarities between the cultures using those respective languages. Our work aims to do this.
4456841
Artificial agent society simulations in an encounter-based normative action environment###The purpose of the study is to investigate potential relationship between agents' socialness and society's behavior predictability in an encounter-based normative action environment. For this purpose, we proposed a hypothesis and tested it against different simulation setups in the context of classical single source shortest path problem. By the end of simulations, it is observed that the hypothesis holds for both norm internalization and spreading measures when the agents in society have some degree of autonomy. That means for our setup, we conclude that lower degree of socialness results in lower behavioral predictability of the society when the agents have some degree of autonomy.
4456884
Internet banking payment protocol with fraud prevention###In this paper, we propose a secure Internet banking payment protocol (IBPP), which is suitable for Internet banking payments and facilitates fraud prevention mechanism. The proposed protocol employs a dynamic key generation technique that relies on advanced authentication technologies such as biometrics and smart cards to prevent a fraud. The proposed protocol also satisfies transaction security properties provided by other payment protocols such as SET and iKP. The formal analysis illustrates that our protocol achieves the goals of Internet payment protocols. Moreover, the involved parties' information is not required to be sent during transactions which results in a security enhancement of the system.
4456885
Exploiting potentially dead blocks for improving data cache reliability against soft errors###Soft errors due to energetic particle strikes are a big concern for systems to run in a reliable manner. This reliability concern have been more serious with technology scaling and aggressive leakage control mechanisms. Since cache memories consume the largest fraction of on-chip real estate, they are more vulnerable to soft errors, as compared to many other system components. This paper proposes a solution to the problem of designing a reliable data cache without trading reliability for performance and area, which is a typical characteristic of conventional parity and ECC based protection techniques. Although parity is simple and fast, it can detect only odd numbered errors without correcting any of them. On the other hand, ECC techniques are more complex and time-consuming, and have the capability of correcting some of the errors. Our technique enhances data cache reliability by storing the replica(s) of data items in active use into cache lines which hold data not likely to be reused. The bookkeeping information about replicas is maintained in a small fully associative cache called shadow cache. By exploiting the replicas to correct the soft errors enhances the data reliability. Since we keep the replicas in potentially dead blocks, the performance loss is negligible with a little extra chip area requirement for the shadow cache. Our experimental results indicate that our technique, compared to the previous similar two techniques, is more effective for enhancing the L1 data cache reliability in modern Superscalar machines with only negligible degradation in performance.
4456868
Music recommendation based on adaptive feature and user grouping###We present a music recommendation system that uses different features of audio content for each user based on the user's listening history. The system is based on the idea that different people may give more importance to certain aspects of music. MFCC, MPITCH, BEAT, STFT feature sets are obtained for all the available songs and then different clusterings of the songs based on each possible feature set is obtained. When a user session is observed, the cluster IDs of songs the user listened in each clustering are obtained. The clustering that has been able group the users' songs in the best possible way according to Shannon entropy is selected as the right clustering for that user. Using this content based recommendation scheme, as opposed to a static set of features resulted in up to 60 percent increase in recommendation success. Based on the same clustering performance idea, inclusion of the singer information and the most popular songs at the time of recommendation, in addition of the content is also explored and has resulted in performance increase. We introduce a third algorithm that is based on adaptive groupings of users. This algorithm is the best performer among the three algorithms we consider. Experiments are carried out on user session data consisting of 2000 to 500 sessions of lengths 5 to 15.
4456869
MRI fuzzy segmentation of brain tissue using IFCM algorithm with particle swarm optimization###Medical image segmentation is a complex and challenging task due to the intrinsic nature of the images. Magnetic resonance imaging (MRI) segmentation is of particular importance for further image analysis. Fuzzy c-mean (FCM) is a common clustering algorithm which is used for segmentation of MR images. However in the case of noisy MR images, efficiency of this algorithm considerably reduces. Recently, researchers have introduced two new parameters in order to improve the performance of traditional FCM in the case of noisy images. New parameters are computed using artificial neural networks and through a complex and time consuming optimization problem. In this paper, we present a new method for computation of these two parameters, efficiently. We use a particle swarm optimization (PSO) method and show the capability of PSO to find optimal values of these parameters. The advantage of the new proposed method is its simplified computations. Our simulation results on a set of noisy MR images, demonstrate the effectiveness of proposed optimization method compared with some related recent algorithms.
4456880
Threshold broadcast encryption with reduced complexity###Threshold broadcast encryption (TBE) is a promising extension of threshold cryptography with its advantages over traditional threshold cryptosystems, such as eliminating the need of a trusted party, the ability of setting up the system by individual users independently and the ability of choosing the threshold parameter and the group of privileged receivers at the time of encryption. An ElGamal-based solution for TBE was proposed by Ghodosi et al. In this paper, we propose an improved ElGamal-based TBE scheme with reduced transmission cost.
4456881
Fast service recovery under shared protection at connection level in WDM grooming networks###In this paper, a novel algorithm optimizing the utilization of backup path resources for survivable WDM mesh grooming networks, based on graph vertex-coloring approach, is proposed. This is the first optimization technique, dedicated to protection-at-connection level (PAC) in WDM grooming networks, such that does not increase the backup path length and thus provides fast service recovery. The concept was evaluated for the U.S. Long-Distance Network and European COST 239 Network. The results show that, with only a little degradation of link capacity utilization efficiency (up to 8%), up to 20% shorter average values of service recovery time can be achieved.
4456882
On robustness and self-adaptiveness of a socially inspired P2P network###The growing interest for P2P overlay networks in resource sharing rises new challenges for researchers, mainly because those systems are intrinsically different from well-studied client-server environments. Finding algorithms and mechanisms to make resource sharing efficient is only the first step toward affordable, scalable and secure P2P applications. Despite the low attention put in the last years to robustness of P2P overlay networks, checking and measuring resilience of such structure with respect to faults, is a key point to push their usage in wide- range real applications. The present contribution goes into this direction, presenting an accurate study of robustness and fault- tolerance of a novel socially-inspired P2P overlay network named PROS A. We show how the self-organising structure induced by PROSA has many interesting self-adaptive capabilities, as observed in real social collaboration networks.
4456883
Generalized ID-based ElGamal signatures###ID-based cryptography has been a very active area of research in cryptography since bilinear pairings were introduced as a cryptographic tool, and there have been many proposals for ID-based signatures recently. In this paper, we introduce the concept of generalized ID-based ElGamal signatures and show that most of the proposed ID-based signature schemes in the literature are special instances of this generalized scheme. We also obtain numerous new signatures from this generalized scheme which have not been proposed before.
4456848
Efficient tracking approach of multiple interacting objects using data association###Robust tracking of multiple interacting objects in image sequences is a challenging problem due to the disturbances that occur often in the real environment. In this context, a system of independent particle filters and an adaptive motion model is used which allow the separated handling of moving objects in conflict situations. For solving the problems of the fluctuation detection and dealings with object interactions, a data association step is suggested with data exclusion, data allocation and data administration. Furthermore, this enables us to recognize conflict image situations and to adjust and adapt the particle filters to these situations. The fitness of the approach will be shown for various real image sequences.
4456849
A feature selection algorithm with redundancy reduction for text classification###Document classification involves the act of classifying documents according to their content to predefined categories. One of the main problems of document classification is the large dimensionality of the data. To overcome this problem, feature selection is required which reduces the number of selected features and thus improves the classification accuracy. In this paper, a new algorithm for multi-label document classification is presented. This algorithm focuses on the reduction of redundant features using the concept of minimal redundancy maximal relevance which is based on the mutual information measure. The features selected by the proposed algorithm are then input to one of two classifiers, the multinomial naive Bayes classifier and the linear kernel support vector machines. The experimental results on the Reuters dataset show that the proposed algorithm is superior to some recent algorithms presented in the literature in many respects like the F<sub>1</sub>-measure and the break-even point.
4456899
Dispersion of a swarm of robots based on realistic wireless intensity signals###The problem of dispersion in multi-robot systems could be loosely defined as maximizing the sensor coverage area while preserving the connectivity within the swarm. Dispersion of robotic swarms appears to be applicable and useful in missions such as planetary exploration, hurricane surveillance, or nuclear decontamination, where the robots with maximal coverage collect samples from the unknown surface, detect the victims, or collect nuclear waste, respectively. In this paper, a simple dispersion algorithm based on wireless signal intensities is proposed and tested in a physics based simulator of a robotic platform which is particularly designed to serve as a test-bed for swarm-robotic studies. The signal intensities are realistically modeled using sampling technique, taking both the distance and relative orientations of the wireless sensors into account. The only parameter of the algorithm, a threshold parameter, is optimized in order to maximize the sensor coverage and minimize the number of disconnected robots.
4456828
The Dynamic-BrickR access method for mobile objects###The database storage and organization of data describing the evolution of mobile objects is an open challenge. Data must be managed in efficient structures with respect to both the storage space consumed and the data access through the means of these structures. We introduce in this paper a new indexing method that improves the data access and, consequently, the queries processing efficiency. The dynamic-BrickR access method uses two structures: an underlying permanent R*-Tree structure, and an in-memory dynamic space grid structure, that it used for building the terminal nodes to feed the R*-Tree. Experiments show significant improvements of the dynamic-BrickR method over the R*- Tree index, regarding the dead space and the overlapping area. The dynamic-BrickR can be used in answering spatial, temporal and spatio-temporal queries. Between them, spatial and spatio-temporal queries benefit by the improvements the dynamic-BrickR brings at the space structuring level and are answered by fewer data access operations.
4456829
A new approach to adaptive encoding data using self-organizing data structures###This paper demonstrates how techniques applicable for defining and maintaining a special case of binary search trees (BSTs) can be incorporated into "traditional" compression techniques to yield enhanced superior schemes. We, specifically, demonstrate that the newly introduced data structure, the Fano Binary Search Tree (FBST) can be maintained adaptively and in a self-organizing manner. The correctness and properties of the encoding and decoding procedures that update the FBST are included. We also include the theoretical and empirical analysis, which shows that the number of shift operators is large for small files, and it tends to decrease (asymptotically towards zero) for large files.
4456840
Using learning automata to model the &#x201C;learning process&#x201D; of the teacher in a tutorial-like system###Unlike the field of Tutorial systems, where a real-life student interacts and learns from a software system, our research focuses on a new philosophy in which no entity need be a real-life individual. Such systems are termed as Tutorial-like systems, and research in this field endeavours to model every component of the system using an appropriate learning model (in our case, a Learning Automaton (LA)). While models for the Student, the Domain, the Teacher, etc. have been presented elsewhere, the aim of this paper is to present a new approach to model how the Teacher, in this paradigm, &#x201C;learns&#x201D; and improves his &#x201C;teaching skills&#x201D; while being himself an integral component of the system. We1 propose to model the &#x201C;learning process&#x201D; of the Teacher by using a higher level LA, referred to as the Meta-Teacher, whose task is to assist the Teacher himself. Ultimately, the intention is that the latter can communicate the teaching material to the Student(s) in a manner customized to the particular Student&#x2019;s ability and progress. In short, the Teacher will infer the progress of the Student, and initiate a strategy by which he can &#x201C;customcommunicate&#x201D; the material to each individual Student.
4456827
Research issues in Peer-to-Peer data management###Data management in peer-to-peer (P2P) systems is a complicated and challenging issue due to the scale of the network and highly transient population of peers. In this paper, we identify important research problems in P2P data management, and describe briefly some methods that have appeared in the literature addressing those problems. We also discuss some open research issues and directions regarding data management in P2P systems.
4456842
Off-line signature verification with PSO-NN algorithm###Analysis of signature is a widely used and developed area of research for personal verification. A typical signature verification system generally consists of four components: data acquisition, pre-processing, feature extraction and verification. This paper presents a novel technique for off-line signature verification (SV). The technique is based on a neural network (NN) approach trained with particle swarm optimization (PSO) algorithm. To test the performance of the proposed PSO-NN algorithm three types of forgeries; random, unskilled and skilled are examined and the experimental results are illustrated.
4456843
Restoration of damaged slices in images using matrix pseudo inversion###A matrix pseudo inversion based method is developed for coding of images in real field in a way that at the receiver, one can restore the lost portions of the images. The proposed solution is similar to channel coding techniques, but it is done before source coding at the transmitter and after decoding of the compressed data at the receiver. Experiments show the effectiveness of the proposed solution in recovering missed portions of the images and video frames.
4456844
Steganography using block-based adaptive threshold###Steganography is a technique to hide secret information in some other data (we call it a vessel) without leaving any apparent evidence of data alteration. All of the traditional steganographic techniques have limited information-hiding capacity. In this paper, we proposed a new method of the adaptive steganography using complexity on bit planes of color image. Applying fixing threshold and variable length, if we insert information into all bit planes, all bit planes showed different image quality. Therefore, we investigated the complexity on bit plane and data, similarity insert information into bit planes. As a result, the proposed method increased the insertion capacity and improved the image quality as compared to fixing threshold and variable length method.
4456845
Modeling and simulation of a General Regression Neural Network using hardware description language###This study presents the development of a synthesizable VHDL (very high speed integrated circuit hardware description language) model of a general regression neural network (GRNN). The GRNN has a four-layer structure which is comprised of an input layer, a pattern layer, a summation layer and an output layer. The designed system can be used for pattern classification applications. Iris dataset is used to test the GRNN in this study. Simulation results show that pattern classification by digital implementation of GRNN has successfully achieved.
4456846
Investigation of Zipf&#x2019;s &#x2018;law-of-meaning&#x2019; on Turkish corpora###Zipf's law-of-meaning states that the number of meanings of a word is related to its frequency. Words that are seen more frequently tend to have more meanings than the ones that are seen less frequently. This law, like the other Zipfian laws is consistent with the principal of least effort. In this study we hope to establish a basis for the number of meanings a word can attain in Turkish with respect to its frequency in a document. Zipfian parameters are derived from two Turkish corpora on which the meanings are labeled. It is hoped that the results of this study contributes in resolving ambiguity of word senses in Turkish.
4456847
Zipf&#x2019;s law of burstiness in Turkish: The length of intervals between repetitions###Zipf law of burstiness of content words is being less studied than his laws that describe the relation between the rank and the frequency of words. Zipf counted the number of intervals of the same length between the repetitions of the words belonging to the same frequency class and on a 260,000 word English corpus empirically showed that the interval size, I, between each occurrence of a word is inversely proportional to the number of intervals having that size: F a I<sup>p</sup>, where p varied between 1 and 1.3. In this study we investigated the validity of the law of burstiness on a Turkish corpus of size 55,000 and found p varying between 0.5 and 0.8.
4456865
Clustering and dimensionality reduction to determine important software quality metrics###During the last two decades research on software engineering is concentrated on quality. The best approach to quality evaluation goes through determining well-defined metrics on software properties. One such property is module complexity, which is a view of the software that is related to how easily it can be modified. There has been work on constructing a metrics domain which measures the module complexity. Generally, PCA (Principal Component Analysis) is used for defining principal metrics in the domain. Since there are usually no labels for the software data, an unsupervised dimensionality reduction technique, such as PCA needs to be used for determining the most important metrics. In this study, we use the clustering similarity obtained when a certain subset of metrics and when the whole set of metrics are used, to determine the most important metrics. We measure the relative difference/similarity between clusterings using three different indices, namely Rand, Jaccard and Fowlkes-Mallow. We use both backward feature selection and PCA for dimensionality reduction. On the publicly available NASA data, we find out that instead of the whole set of 42 metrics, using only 15 dimensions, we get almost the same clustering performance. Therefore, instead of the whole set of software metrics, a smaller number of them could be used to evaluate the software quality.
4456894
An accurate evaluation of machine learning algorithms for flow-based P2P traffic detection###Today, peer-to-peer (P2P) traffic consumes the largest fraction of network bandwidth. The files shared by P2P communications are mostly copyright protected, and there are issues related to Quality of Service (QoS) support and billing of P2P traffic. Hence, scalable and accurate detection of peer-to-peer (P2P) traffic is a significant problem for network service providers. Flow-based detection methods employ characteristics of data flows such as the number of packets per flow to classify P2P and non-P2P traffic. Thus, they provide solutions to problems of port-based and signature-based detection such as P2P applications with dynamic ports, updating the signature database and encrypted packets. In this paper, a comparative evaluation of several flow-based P2P traffic detection methods that employ machine learning (ML) techniques is presented. Different from previous work, the effect of network parameters is taken into consideration in our evaluation. Furthermore a new verification approach based on custom-made data is presented which can circumvent the accuracy problems of the previous verification methods that use port-based or signature-based techniques for the accuracy evaluation.
4456886
Defect prediction for embedded software###As ubiquitous computing becomes the reality of our lives, the demand for high quality embedded software in shortened intervals increases. In order to cope with this pressure, software developers seek new approaches to manage the development cycle: to finish on time, within budget and with no defects. Software defect prediction is one area that has to be focused to lower the cost of testing as well as to improve the quality of the end product. Defect prediction has been widely studied for software systems in general, however there are very few studies which specifically target embedded software. This paper examines defect prediction techniques from an embedded software point of view. We present the results of combining several machine learning techniques for defect prediction. We believe that the results of this study will guide us in finding better predictors and models for this purpose.
4456887
Architectural design of an access control system for enterprise networks###Client computers in enterprise networks have the potential to be the source of serious security problems, especially when their hardware and software components are out of physical administrative control. Besides, services in the network may have client configuration requirements. We propose a system composed of a policy management and enforcement server and client agents, which authenticates the client users and checks their computer configurations before allowing their access to services. The information modeling within the design is based on common information model. Web services are used for communication, following the related specifications in Web based enterprise management.
4456852
Network traffic classification with Self Organizing Maps###Anomaly detection in network traffic is one of the most challenging topics in the study of computer science and networking. This paper introduces a classification method for analyzing network traffic behavior. In order to distinguish the normal traffic with well-known anomalies such as port scanning and DOS attacks, Self Organizing Maps (SOMs), one of the well- known artificial neural network architecture, is used. The measurement of traffic is performed by using Simple Network Management Protocol (SNMP). In this work, it is proposed a SOM-based classifier to discriminate three types of network traffic as port scanning, heavy-download and the rests. It is worth to mention that impressively satisfactory results have been obtained. The method has also been enhanced to obtain better results by trying to find trajectories on the map with sliding the input vectors in time and developed an alarm mechanism. Here it is possible to detect whether consecutive trajectories are hit by one of the classes or not. The success rate of the system is approximate to certain.
4456888
Pocketdrive: A system for mobile control of desktop PC and its applications using PDAs###Today, consumer electronic devices and PCs are inevitable parts of our daily life. Controlling those devices remotely is an important aspect of the technology. We have already universal remote control devices for controlling consumer electronic devices. Similarly, we may control our desktop and laptop PCs and their applications remotely via portable and smaller computers like PDAs and Pocket PCs. This paper presents a system and its architecture that enable a wireless-enabled PDA to control a PC and its applications remotely over a 802.11 or Bluetooth link. With such a system running on a PDA, a user can start, run and control PC applications from any location that is reachable via 802.11 link. This enables flexibility, ease of use, and freedom for the user of PC applications.
4456892
Use of entropy to control coverage and energy dissipation in wireless sensor networks###In this study we proposed a scheme to achieve adequate coverage while maximizing network lifetime in a sensor network. The proposed approach is based on controlling the number of active sensors using information theory. It assumes that the area of coverage is divided into grids and uses a threshold probability to decide whether the sensors should be ON or OFF. Based on the number of ON sensors in each grid we define an entropy value which we use to adjust the threshold probability for the next epoch. The control mechanism is devised to maximize entropy while keeping the number of ON sensors at a minimum. This approach improved the coverage and network lifetime.
4456875
The new approach for inter-communication between guest domains on Virtual Machine Monitor###In general, the speed of internal communication on a single-machine is faster than the other communication.(e.g. communication between remote PCs through the local network) Therefore, we expected that the internal communication between guest domains on a single-machine of virtual machine monitor (VMM)[1] was also fast. However, in the experiment, it showed that rather the speed was slower than the other communication because each guest domain was not well considered to be executed on VMM. In other words, the guest domain has some problems in the architecture of communication and data transmission on VMM. In this paper, we find the causes of those problems and suggest a new design for internal communication to improve its performance. It is well considered to be executed on VMM and shows the performance improvement of bandwidth up to 8 times in comparison with existing internal communication. And it also reduced the latency to 25%.
4456874
Optimal placement and activity scheduling to maximize coverage lifetime in wireless sensor networks###In this paper, we consider the differentiated coverage problem for heterogeneous sensor networks over a finite planning horizon consisting of discrete time intervals. We assume that there are different types of sensors, and the characteristics of each sensor type such as unit cost, sensing range and energy consumption level is known. Furthermore, each sensor is capable to operate in either active or standby modes with different energy consumption rates. The goal is to determine optimal types and locations of the sensors as well as their activity schedules subject to coverage and budget constraints with the objective of maximizing the network lifetime. We first give a mixed-integer linear programming formulation which is computationally intractable and can only be solved optimally for small instances. Therefore, we propose a heuristic based on Lagrangian relaxation and subgradient optimization. Computational experiments performed on various test instances indicate that the new heuristic is efficient and accurate.
4456877
On DiffServ traffic forecast and queue management using neural network with logarithmic scaling and generalized quantization###In this paper we propose a new predictor based on feed forward neural network featured by logarithmic scaling and generalized quantization procedure. We analytically prove its optimal behavior. Further, we integrate our predictor into a queuing system framework with control that serves DiffServ traffic. Within that framework we introduce the SCPO logic which is an essential part of the queuing system performance optimization task. Using simulation studies we show efficiency of the proposed SCPO logic as well as of the designed predictor.
4456876
Network security situation awareness model based on heterogeneous multi-sensor data fusion###Network security situation awareness (NSSA) is an emerging technique in the Held of network security and it helps security analysts to be aware of the actual security situation of their networks. In this paper we presented a novel NSSA model based on multi-sensor data fusion and multi-class support vector machines. In our model, we adopted Snort and NetFlow as two sensors to gather data from network traffic. We employed multi-class support vector machines as fusion engine of our model in combination with an efficient feature reduction approach to fuse the gathered data from heterogeneous sensors. Furthermore, we discussed the alert aggregation algorithm and the security situation awareness generation techniques detailedly. Our model is proved to be feasible and effective through a series of experiments.
4456871
Developing growing hierarchical structures for decision making###This study is about developing a hierarchical approach for decision-making problems. The development is done on a representative decision-making problem. A hierarchical decision making approach which enables fusion of decisions of previous and current levels is proposed. The agents that determine the decisions at different hierarchy levels is accomplished by utilizing a genetic algorithm.
4456870
A pattern classification approach for boosting with genetic algorithms###Ensemble learning is a multiple-classifier machine learning approach which produces collections and ensembles statistical classifiers to build up more accurate classifier than the individual classifiers. Bagging, boosting and voting methods are the basic examples of ensemble learning. In this study, a novel boosting technique targeting to solve partial problems of AdaBoost, a well-known boosting algorithm, is proposed. The proposed system finds an elegant way of boosting a bunch of classifiers successively to form a "better classifier" than each ensembled classifiers. AdaBoost algorithm employs a greedy search over hypothesis space to find a ";good"; suboptimal solution. On the hand, the system proposed employs an evolutionary search with genetic algorithms instead of greedy search. Empirical results show that classification with boosted evolutionary computing outperforms the classical AdaBoost in equivalent experimental environments.
4456873
A load balancing model for grid environment###Workload and resource management are two essential functions provided at the service level of the Grid software. To improve the global throughput of these environments, effective and efficient load balancing algorithms are fundamentally important. Although load balancing problem in classical distributed systems has been intensively studied, new challenges in Grid computing still make it an interesting topic, and many research projects are under way. This is due to the Grid characteristics and to the complex nature of the problem. This paper presents a task load balancing model in Grid environment. First we propose a tree-based model to represent Grid architecture in order to manage workload. This model is characterized by three main features: (i) it is hierarchical; (ii) it supports heterogeneity and scalability; and, (iii) it is totally independent from any Grid physical architecture. Second, we develop a hierarchical load balancing strategy to balance tasks among Grid resources. The main characteristics of the proposed strategy are: (i) it uses a task-level load balancing; (ii) it privileges local tasks transfer to reduce communication cost; and, (iii) it is a distributed strategy with local decision making.
4456872
Restructuring software systems using clustering###In this paper we are focusing on the problem of restructuring object oriented software systems using clustering techniques. Refactoring ([1]) is one major issue to improve the design of software systems, increasing the internal software quality. This paper aims at introducing a new k-medoids based clustering algorithm that can be used for improving the design of software systems, by identifying the needed refactorings. The algorithm uses a measure that evaluates a software system design. Clustering ([2]) is used in order to recondition the class structure of a software system. The proposed approach can be useful for assisting software engineers in their daily works of refactoring software systems. We evaluate our approach using the open source case study JHotDraw ([3]), illustrating the advantages of our approach in comparison with existing approaches.
4456897
Fault-tolerance improvement of planar adaptive routing based on detailed traffic analysis###Currently, some coarse measures like global network latency are used to compare routing protocols. These measures do not provide enough insight of traffic distribution among network nodes in the presence of different fault regions. This paper presents a detailed traffic analysis of fault-tolerant planar adaptive routing (FTPAR) algorithm achieved by an especially developed tool. Per-node traffic analysis illustrates the traffic hotspots caused by fault regions and provides a great assistance in developing fault tolerant routing algorithms. Based on such detailed information, a simple yet effective improvement of FTPAR is suggested. Moreover, the effect of a traffic hotspot on the traffic of neighboring nodes and global performance degradation is investigated. To analyze the per-node traffic, some per-node traffic metrics are introduced and one of them is selected for the rest of work. In an effort to gain deep understanding of the issue of traffic analysis of faulty networks, this paper is the first attempt to investigate per-node traffic around fault regions.
4456896
Parallel hermite interpolation on the pyramid###The pyramid network is one of the most important interconnection topologies used as hardware architecture or software data structure. It has a combined tree-mesh structure making it suitable for solving many parallel problems and applications. This paper proposes a parallel algorithm for Hermite Interpolation on the Pyramid network which has at least N nodes. The proposed algorithm has 3 phases: initialization, main, and final. The algorithm is optimal with a time complexity of O(N) for an appoint interpolation.
4456895
Parallel preconditioners for solutions of dense linear systems with tens of millions of unknowns###We propose novel parallel preconditioning schemes for the iterative solution of integral equation methods. In particular, we try to improve convergence rate of the ill-conditioned linear systems formulated by the electric-field integral equation, which is the only integral-equation formulation for targets having open surfaces. For moderate-size problems, iterative solution of the near-field system enables much faster convergence compared to the widely used sparse approximate inverse preconditioner. For larger systems, we propose an approximation strategy to the multilevel fast multipole algorithm (MLFMA) to be used as a preconditioner. Our numerical experiments reveal that this scheme significantly outperforms other preconditioners. With the combined effort of effective preconditioners and an efficiently parallelized MLFMA, we are able to solve targets with tens of millions of unknowns, which are the largest problems ever reported in computational electromagnetics.
4456890
An evaluation of aspect oriented programming for embedded real-time systems###Crosscutting concerns are the issues in object- oriented programming (OOP) that cannot be modularized within a software module. In this paper, an experimental evaluation of the use of aspect-oriented programming (AOP) for the implementation of crosscutting concerns in embedded real-time systems is presented. The crosscutting functionality of the project under study is realized both by employing OOP and AOP techniques to make a comparison between the two programming paradigms. These two implementations are evaluated with respect to software quality and embedded real-time performance metrics. The analysis of the results shows that the AOP technique improves both of these metrics for the software project under study.
4456879
Lifetime optimization using variable battery capacities and nonuniform density deployment in wireless sensor networks###Nodes in a wireless sensor network have nonuniform energy consumption rates. As a result, early death among highly loaded nodes is a common phenomenon which makes it impossible to use the full capacity of the network. In this paper, we propose to deploy sensors either with variable battery capacities or with nonuniform densities in order to counterbalance the nonuniform energy drainage, thus achieving a longer network lifetime. We divide the monitored region into concentric ring areas and deploy nodes in these areas such that the highest battery resources are allocated to the ring where the highest energy drainage takes place. Results show that up to 6 to 7 times longer lifetime values are attained without any increase in costs with this approach.
4456878
Smart card based remote authentication scheme with strengthened security via ECDH###Nowadays with the widespread Internet use, remote user authentication has became important process to provide security of the system resources. The process determines the identity of a person who attempted to access system. Various cryptographic algorithms are used in previously developed remote user authentication schemes using smart card to provide better security. In this work, we have eliminated the some security flaws in Liaw-Lin-Wu scheme. Also we have applied elliptic curve Diffie Hellman key exchange protocol (ECDH) into Liaw-Lin-Wu scheme to enhance its' security features and to improve its' efficiency.
4456891
Promoting web traffic over a DiffServ architecture###In this paper we present some simulation results about web traffic differentiation. Traditionally, the differentiated services architecture (DiffServ) has been under consideration to providing different services and QoS in Internet. The flow size of the web traffic is important when we intend to achieve certain QoS to these kind of flows. We propose a DiffServ model where short flows are managed with high priority and the maximum QoS. We intend to promote some long flows, but this promotion must not affects the short flows performance. For this purpose, firstly we need to detect the gaps in the bandwidth committed to QoS. Second, some long flows can be promoted over high priority queue, instead of low priority queue. Our proposal has special attention for the very long flows which reduce the web traffic performance. We must detect and penalize these extreme long flows in order to avoid the promotion of these flows and permit others aspirants to be promoted. With these actions we get to improve the global performance of the web traffic. We have used ns2 network simulator tool for the simulation with the PackMime-HTTP object for the realistic synthetic Web traffic generation. We have analyzed some simulation results about web transmission latency and packet loss.
4456889
Applicability of eigenvector centrality principle to data replication in MANETs###An efficient data replication service is crucial for improving data accessability and resource utilization as well as providing consistency in mobile ad hoc systems. In this study, we investigate the applicability of eigenvector centrality (EVC) principle as an aid to determine replica nodes for data items in mobile ad hoc networks. There exist several studies for mathematical modeling of networks and defining roles to nodes based on EVC analysis in static networks. For MANETs, utilization of EVC to determine dissemination power of nodes has been also recently explored. In contrast to prior work, we focus on the question of whether EVC analysis can be helpful in locating nodes with replica roles. We present our approaches for connectivity matrix construction that is significant for precise EVC analysis. Comparative simulation results and analysis are described for both data replication and dissemination as a function of system scalability. Simulation results show that connectivity matrix construction techniques do not result in too much disparity for the performance of data replication and identifying one of the replicas to be the eigenvector central node does not lead to an improvement in data accessability for large networks.
